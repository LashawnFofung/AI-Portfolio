{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/AI-Portfolio/blob/main/src/AI_Powered_Document_Intelligence_Automation_Platform_MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf8S474r_kYp"
      },
      "source": [
        "# **Final Project: ü§ñ AI-Powered Document Intelligence Automation Platform MVP**\n",
        "\n",
        "Production-Grade Retrieval-Augmented Generation (RAG) & Document Governance\n",
        "This platform is a high-performance solution for high-volume document environments (Legal, Finance, HR). Unlike generic RAG systems that suffer from **\"Context Contamination\"** (mixing unrelated data), this system uses **Intelligent Boundary Detection** and **Metadata-Rich Chunking** to isolate and retrieve precise document segments.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üõ†Ô∏è MVP Objectives**\n",
        "- **Contextual Fidelity:** Eliminate hallucinations by segregating the vector space based on document classification.\n",
        "\n",
        "- **Intelligent Automation:** Automatically split bulk PDFs (e.g., a 50-page file containing 5 different contracts) into distinct logical units.\n",
        "\n",
        "- **Hardware-Aware Versatility (Multi-Model Versatility):** Toggle between Gemini 2.0 (API), Mistral 7B, and Phi-2 (Local) with automated VRAM Deep Purges for stability on T4 GPUs.\n",
        "\n",
        "- **Audit-Ready Compliance:** Every response is passed through a Quality Audit Gate before being displayed to the user.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üèóÔ∏è Key Technical Architecture**\n",
        "The system follows a modular Six-Stage Execution Cycle:\n",
        "\n",
        "1. **Ingestion Layer:** Hybrid OCR (PyMuPDF + Tesseract) extracts text and images while preserving spatial metadata.\n",
        "2. **Intelligence Layer:** LLMs classify documents into a taxonomy (e.g., \"Invoice,\" \"Legal Agreement\") and detect page boundaries.\n",
        "3. **Storage Layer:** LlamaIndex + FAISS create Segregated Silos using metadata filters, preventing data leakage between files.\n",
        "4. **Orchestration Layer:** A Python-generator-based \"Thinking Loop\" manages asynchronous status updates and hardware state safety.\n",
        "5. **Audit Layer:** Automatic calculation of the **RAG Triad** (Faithfulness, Relevance, and Context Density).\n",
        "6. **Presentation Layer:** An **Obsidian-themed** Gradio UI with real-time PDF previewing and exportable audit reports.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üåü Core MVP Capabilities**\n",
        "- **Multi-Modal Routing:** Detects if a query is about \"Amounts\" vs. \"Legal Terms\" and targets the specific document silo automatically.\n",
        "- **VRAM Management:** A \"Safety Gate\" logic (`deep_purge_gpu`) ensures the system never crashes when switching between heavy local models and light API models.\n",
        "- **Performance Dashboard:** Real-time visualization of **Latency vs. Token Speed** and **Industry Ground Truth** benchmarks.\n",
        "- **Source Attribution:** Every AI response includes clickable citations (e.g., \"Source: Invoice (p. 4)\") to ensure human-in-the-loop verification.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üìñ How to Operate**\n",
        "- **Environment Setup:** Run Section 1 and 2 to install dependencies and configure your Gemini API Key in the Colab Secrets.\n",
        "- **Initialization:** Run the global configuration cells to load the BGE embedding model and initialize the default LLM.\n",
        "- **Select Engine:** Use the switch_llm function or the UI dropdown to select your preferred AI model (e.g., \"Gemini 2.0 Flash\").\n",
        "- **Upload & Process:** Upload your documents via the Gradio interface. The system will automatically classify and index them.\n",
        "- **Query & Audit:** Enter your business questions. Use the \"Audit Log\" tab to view performance metrics and download a professional PDF summary of the session.\n",
        "\n",
        "<br><br>\n",
        "## **üîç Section Logic & Flow Analysis**\n",
        "\n",
        "- **1-2: Core Config**\n",
        "  - Initializes global state, mounts drives, and manages API/Security keys.\n",
        "      - [Section 1: Setup And Installation](#scrollTo=OnjSSFKJmRQc)\n",
        "        - [Global Asyncio & Uvicorn Fix](#scrollTo=luHZTqNM1YzY&line=2&uniqifier=1)\n",
        "      - [Section 2A: Core Imports, Security Keys, And Global Settings Configurations](#scrollTo=1TlPkZLbvExS)\n",
        "      - [Section 2B: LLM Factory & Resource Configuration](#scrollTo=1MYP_DZAw6QJ)\n",
        "\n",
        "- **3\tData Structures (Schema)**\n",
        "  - Defines LogicalDocument and ChunkMetadata dataclasses for pipeline consistency\n",
        "      - [Section 3: Data Structures For Enhanced Document Management](#scrollTo=RLrZv_r30NBc)\n",
        "\n",
        "- **4-5\tIngestion & OCR**\n",
        "  - **Aware Routing:** Uses LLM-based boundary detection to separate scanned images from text PDFs.\n",
        "      - [Section 4: Document Intelligence Functions](#scrollTo=xQNJUFwB2gMZ)\n",
        "      - [Section 5: Advanced PDF Processing Pipeline](#scrollTo=yGrQwXDp4gh_)\n",
        "\n",
        "- **6\tIntelligent Chunking**\n",
        "  - Implements Metadata-Rich Chunking where every fragment knows its DocID and Page#.\n",
        "      - [Section 6: Intelligent Chunking With Metadata Preservation](#scrollTo=qYroZgDG9FxX)\n",
        "\n",
        "- **7-8\tVector DB**\n",
        "  - Segregates FAISS indices to create \"Document Silos\" for cleaner retrieval.\n",
        "      - [Section 7: Query Routing And Intelligent Retrieval](#scrollTo=d_MsgfAaAGsm)\n",
        "      - [Section 8: Enhanced Answer Generation With Source Attribution](#scrollTo=VrLmw0oyEGfq)\n",
        "\n",
        "- **9-10\tThe Brain (Orchestrator)**\n",
        "  - Handles Streaming UX, Context Window Safety, and Hardware Deep Purges.\n",
        "      - [Section 9: Enhanced Document Store](#scrollTo=mJTVS2CcHBq3)\n",
        "      - [Section 10: Backend Chat & Audit Loogic](#scrollTo=txaCA0n9MNYN)\n",
        "\n",
        "- **11-12\tUI & Reporting**\n",
        "  - Gradio Layout: A three-pillar interface (Operations, Configurations, Audit).\n",
        "      - [Section 11:Chatbot Logic & Orchestration](#scrollTo=80rvI0tlTlEu)\n",
        "      - [Section 12: Gradio Interface, Chat Handlers, & Wiring Logic](#scrollTo=2pYWo7wCa47E)\n",
        "\n",
        "- **13 Application Launcher**\n",
        "  - [Application MVP](#scrollTo=GgDAO0pTcury)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnjSSFKJmRQc"
      },
      "source": [
        "# **SECTION 1. SETUP AND INSTALLATION**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section serves as the **Foundation Layer** of the AI-Powered Document Intelligence Platform. The logic follows a linear, non-destructive sequence:\n",
        "- **Dependency Provisioning:** Installs the multi-modal stack required for the MVP. This includes UI components (`Gradio`), document parsing (`PyMuPDF`, `LlamaIndex`), OCR engines (`Tesseract`), and the vector search backend (`FAISS`).\n",
        "\n",
        "- **Global State Initialization:** Sets up persistent tracking variables (`audit_logs` and `current_llm`). This is a critical design choice for the MVP, as it allows for performance metrics to persist across multiple document uploads and ensures the system knows which LLM engine is currently \"warm\" in memory.\n",
        "\n",
        "- **Asynchronous Handling:** Applies `nest_asyncio` to prevent event loop conflicts during RAG pipeline execution.\n",
        "\n",
        "- **Resource Mounting:** Links Google Drive to ensure the UI has access to static assets (logos/branding) and persistent storage for output reports.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luHZTqNM1YzY",
        "outputId": "e2c797a4-b48a-45ef-bb22-7ee65f7652d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GLOBAL ASYNCIO INSTALL & IMPORTS Complete).\n"
          ]
        }
      ],
      "source": [
        "# ------- GLOBAL ASYNCIO & UVICORN FIX -------\n",
        "\n",
        "# --- [ASYNC & ENVIRONMENT PREP] ---\n",
        "# Prevents kernel crashes when switching models or running RAG queries\n",
        "!pip install -q nest_asyncio\n",
        "!pip install uvicorn>=0.34.0\n",
        "\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import sys\n",
        "\n",
        "print(\"‚úÖ GLOBAL ASYNCIO INSTALL & IMPORTS Complete).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GekfvnoW_jaJ",
        "outputId": "49773dea-b1a2-466e-f6b2-1a9bd73af1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.12/dist-packages (0.3.16)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Requirement already satisfied: llama-index-llms-llama-cpp in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: json-repair in /usr/local/lib/python3.12/dist-packages (0.55.1)\n",
            "Requirement already satisfied: llama-cpp-python<0.4,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-llama-cpp) (0.3.16)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-llama-cpp) (0.14.13)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.1.6)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.22.1)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.3.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.13.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.6.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.5.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.12.3)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (80.10.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.0.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.67.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.0.3)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.26.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.6)\n",
            "‚úÖ Global audit_logs list initialized.\n",
            "‚úÖ Global state for LLM variables initialized.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ SECTION 1. SETUP AND INSTALLATION COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 1. SETUP AND INSTALLATION -------\n",
        "\n",
        "# 1.1 UI, PDF Processing & Machine Learning Foundations\n",
        "# Grouping core utilities for document extraction and interface building\n",
        "!pip install -q \\\n",
        "    gradio gradio_pdf \\\n",
        "    pypdf PyPDF2 pymupdf \\\n",
        "    pillow \\\n",
        "    sentence-transformers transformers \\\n",
        "    faiss-cpu \\\n",
        "    numpy pandas jedi\\\n",
        "    json-repair\n",
        "\n",
        "# # 1.2 LlamaIndex Orchestration Stack\n",
        "# Specifically for RAG (Retrieval-Augmented Generation) and metadata management\n",
        "!pip install -q \\\n",
        "    llama-index \\\n",
        "    llama-index-readers-file \\\n",
        "    llama-index-vector-stores-faiss\n",
        "\n",
        "#1.3 LLM Engine Support (Multi-Modal Switching)\n",
        "# Libraries required to swap between API-based (Gemini) and Local (HuggingFace) models\n",
        "!pip install -q \\\n",
        "    llama-index-llms-huggingface \\\n",
        "    llama-index-embeddings-huggingface \\\n",
        "    transformers accelerate bitsandbytes\n",
        "\n",
        "!pip install -U -q google-generativeai llama-index-llms-google-genai\n",
        "\n",
        "# 1.4 OCR & Specialized Reporting Tools\n",
        "# Tesseract for scanned docs; ReportLab for automated PDF performance summaries\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install -q \\\n",
        "    pytesseract \\\n",
        "    reportlab rouge-score \\\n",
        "    matplotlib seaborn\n",
        "\n",
        "# --- MISTRAL MODEL INSTALLATION with COLAB GPU  ---\n",
        "# Install llama-cpp-python with CUDA support for the T4 GPU\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n",
        "# Install the LlamaIndex connector for LlamaCPP and json-repair for Mistral cleaning\n",
        "!pip install llama-index-llms-llama-cpp json-repair\n",
        "\n",
        "\n",
        "# --- [GLOBAL STATE INITIALIZATION] ---\n",
        "\n",
        "# --- 1. Initialize GLOBAL AUDIT LOG for Performance Tracking ---\n",
        "# audit_logs: Stores performance data (latencies, ROUGE scores) for the report generator\n",
        "audit_logs = []\n",
        "print(\"‚úÖ Global audit_logs list initialized.\")\n",
        "\n",
        "# --- 2. Initialize GLOBAL STATE TRACKING FOR LLM CHOICE ---\n",
        "# current_llm/name: Tracks the active engine to prevent unnecessary re-loading of weights\n",
        "current_llm = None\n",
        "current_model_name = \"\"\n",
        "print(\"‚úÖ Global state for LLM variables initialized.\")\n",
        "\n",
        "\n",
        "\n",
        "# --- [EXTERNAL STORAGE LINKING] ---\n",
        "# MOUNT GOOGLE DRIVE (For UI Image) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ SECTION 1. SETUP AND INSTALLATION COMPLETE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTY2U2VytZnG"
      },
      "source": [
        "# **SECTION 2: CORE IMPORTS, SECURITY KEYS, AND GLOBAL SETTINGS CONFIGURATIONS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TlPkZLbvExS"
      },
      "source": [
        "## **SECTION 2A. CORE IMPORTS, SECURITY KEYS, AND GLOBAL SETTINGS CONFIGURATIONS**\n",
        "\n",
        "This section establishes the Intelligence Layer and Security Protocol.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis:**\n",
        "\n",
        "- **Comprehensive Imports:** Consolidates all necessary libraries from data visualization (`Seaborn`) to RAG orchestration (`LlamaIndex`).\n",
        "\n",
        "- **Memory Safeguards:** Implements a 4-bit quantization configuration (`BitsAndBytes`) specifically tuned for the 16GB VRAM limit of the Google Colab T4 GPU.\n",
        "\n",
        "- **Secret Management:** Securely retrieves API keys from Colab's internal `userdata `(Secrets) to prevent accidental exposure in the code.\n",
        "\n",
        "- **Global Singleton Configuration:** Sets the `Settings` object for LlamaIndex, ensuring that every retrieval and generation call throughout the application uses a consistent embedding model and LLM.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMdiYDOz_5oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8aeed1-accd-4102-92fd-253df5147724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Image found at: /content/drive/MyDrive/AI_Powered_Document_Intelligence_Automation_Platform/AI Document Assistant logo v2.png\n",
            "‚úÖ Image found at: /content/drive/MyDrive/AI_Powered_Document_Intelligence_Automation_Platform/Document Filter and RAG.png\n",
            "‚úÖ Hugging Face Token successfully loaded from Colab Secrets.\n",
            "‚úÖ Settings initialized with: Gemini 2.0 Flash\n",
            "‚úÖ SECTION 2A. CORE IMPORTS, SECURITY KEYS LOADED, AND GLOBAL SETTINGS CONFIGURATIONS COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 2A. CORE IMPORTS, SECURITY KEYS, AND GLOBAL SETTINGS CONFIGURATIONS -------\n",
        "\n",
        "# 1. Standard Library & Utilities\n",
        "import os, time, json, re, io, tempfile, hashlib\n",
        "import random # Used for simulating performance audit metrics\n",
        "import gc # Memory Management: Essential for Google Colab T4 GPU\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, field\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "# 2. Data Science & Visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# 3. Document Processing & OCR\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "\n",
        "# 4. AI & Machine Learning (Vector Engine/Backend)\n",
        "# Core Frameworks\n",
        "import torch # Memory Management: Essential for Google Colab T4 GPU\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "import faiss\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# 5. LlamaIndex (RAG Framework)\n",
        "# The Orchestrator\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
        "\n",
        "        # --- LLM & Embedding -----\n",
        "from json_repair import repair_json  # Critical imports for your Mistral/Router logic\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "\n",
        "# 6. UI & Automated PDF Reporting\n",
        "import gradio as gr\n",
        "from gradio_pdf import PDF\n",
        "from reportlab.lib import colors\n",
        "from google.colab import userdata\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.colors import HexColor, black, green\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
        "\n",
        "\n",
        "# --- --- --- [MEMORY MANAGEMENT] --- --- ---\n",
        "# Shared 4-bit configuration for T4 GPU efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# --- --- --- [RESOURCE PATH DEFINITIONS] ---- --- ---\n",
        "# 1A. Define File Path - LOGO\n",
        "PROJECT_FOLDER = '/content/drive/MyDrive/AI_Powered_Document_Intelligence_Automation_Platform'\n",
        "LOGO_PATH = os.path.join(PROJECT_FOLDER, 'AI Document Assistant logo v2.png')\n",
        "\n",
        "# 2A. Define File Path - CONFIG & FILTER IMAGE\n",
        "CONFIG_FILTER_PATH = os.path.join(PROJECT_FOLDER, 'Document Filter and RAG.png')\n",
        "\n",
        "# 1B. Verify the path exists to avoid \"File Not Found\" errors later\n",
        "if os.path.exists(LOGO_PATH):\n",
        "    print(f\"‚úÖ Image found at: {LOGO_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: Image not found. Check path: {LOGO_PATH}\")\n",
        "\n",
        "\n",
        "# 2B. Verify the path exists to avoid \"File Not Found\" errors later\n",
        "if os.path.exists(CONFIG_FILTER_PATH):\n",
        "    print(f\"‚úÖ Image found at: {CONFIG_FILTER_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: Image not found. Check path: {CONFIG_FILTER_PATH}\")\n",
        "\n",
        "\n",
        "# --- --- --- [SECURITY & GLOBAL CONFIGURATION] --- --- ---\n",
        "# 1A. Load Gemini API\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in Colab Secrets.\")\n",
        "\n",
        "# MANDATORY: Set as environment variable so all internal LlamaIndex\n",
        "# calls and the switch_llm factory can find it automatically.\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "\n",
        "\n",
        "\n",
        "# 1B Load Hugginf Face Token\n",
        "# Retrieve the secret from Colab and set it as an environment variable\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "    print(\"‚úÖ Hugging Face Token successfully loaded from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Could not find HF_TOKEN in Colab Secrets. Check the 'Key' icon on the left.\")\n",
        "\n",
        "\n",
        "# 2. Configure Embedding Model\n",
        "llama_embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# 3A. GLOBAL CONFIGURATION (Crucial for Section 9 & 10)\n",
        "Settings.embed_model = llama_embed_model\n",
        "\n",
        "# 3B. Initial default LLM\n",
        "Settings.llm = GoogleGenAI(model=\"models/gemini-2.0-flash\")\n",
        "\n",
        "# 3C. SAFE NAME ASSIGNMENT\n",
        "# We use a custom attribute that Pydantic won't block,\n",
        "# or simply use the existing 'model' attribute.\n",
        "# To satisfy your Section 11 logs, we use this \"monkeypatch\" method:\n",
        "try:\n",
        "    # This bypasses Pydantic's strict check\n",
        "    object.__setattr__(Settings.llm, 'model_name', \"Gemini 2.0 Flash\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(f\"‚úÖ Settings initialized with: {getattr(Settings.llm, 'model_name', Settings.llm.model)}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 2A. CORE IMPORTS, SECURITY KEYS LOADED, AND GLOBAL SETTINGS CONFIGURATIONS COMPLETE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MYP_DZAw6QJ"
      },
      "source": [
        "## **SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION**\n",
        "\n",
        "\n",
        "This section implements the **Model Orchestration Layer** using a Factory Pattern.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis:**\n",
        "\n",
        "\n",
        "- **Memory Purge Mechanism:** Before loading a new model, the system explicitly deletes the old object and triggers `torch.cuda.empty_cache()`. This is essential for the T4 GPU, which only has 16GB of VRAM.\n",
        "\n",
        "- **Dynamic Model Loading:  Gemini 2.0 Flash:** Uses an API-based approach (zero local VRAM impact).\n",
        "  - **Mistral-7B (GGUF):** Uses `LlamaCPP` with 4-bit quantization, offloading all layers to the GPU for maximum speed.\n",
        "  - **Phi-2:** A lightweight alternative for rapid testing and low-latency extraction.\n",
        "\n",
        "- **Global Settings Sync:** Every time a model is switched, the `Settings.llm` singleton is updated so the rest of the RAG pipeline automatically uses the new engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljYtjJ54OLd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e330d4a-5bda-4988-9351-8f196320a500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION -------\n",
        "\n",
        "# --- üß† LLM FACTORY ---\n",
        "\n",
        "def switch_llm(model_name: str):\n",
        "\n",
        "    # Global trackers\n",
        "    global current_llm, current_model_name\n",
        "\n",
        "    # MEMORY PURGE (T4 GPU Optimization) ---\n",
        "    deep_purge_gpu()\n",
        "\n",
        "\n",
        "    print(f\"üöÄ Initializing {model_name}...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "    # ---- üß† Gemini 2.0 (Cloud) -----\n",
        "        if model_name == \"Gemini 2.0 Flash\":\n",
        "\n",
        "            if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "                return \"‚ùå Error: GOOGLE_API_KEY not found in environment.\"\n",
        "\n",
        "            current_llm = GoogleGenAI(model=\"models/gemini-2.0-flash\")\n",
        "\n",
        "        # ---- üß† Mistral-7B (Local GGUF) -----\n",
        "        elif model_name == \"Mistral-7B (Llama-CPP)\":\n",
        "            from llama_index.llms.llama_cpp import LlamaCPP\n",
        "\n",
        "            # Using 4-bit quantization to fit comfortably on T4 GPU\n",
        "            current_llm = LlamaCPP(\n",
        "                model_url=\"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "                temperature=0.1,\n",
        "                max_new_tokens=512,\n",
        "                context_window=4096,\n",
        "                # n_gpu_layers: -1 moves everything to GPU, 30-35 is safer for 16GB VRAM\n",
        "                model_kwargs={\"n_gpu_layers\": 30, \"offload_kqv\": True},\n",
        "                messages_to_prompt=lambda msgs: f\"[INST] {' '.join([m.content for m in msgs])} [/INST]\",\n",
        "            )\n",
        "\n",
        "        # ---- üß† Phi-2 (Local HF) -----\n",
        "        elif model_name == \"Phi-2 (Small & Fast)\":\n",
        "            from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "            # Phi-2 requires 'trust_remote_code' and specific dtypes for T4\n",
        "            current_llm = HuggingFaceLLM(\n",
        "                model_name=\"microsoft/phi-2\",\n",
        "                tokenizer_name=\"microsoft/phi-2\",\n",
        "                context_window=2048,\n",
        "                max_new_tokens=512,\n",
        "                model_kwargs={\"trust_remote_code\": True, \"torch_dtype\": torch.float16},\n",
        "                device_map=\"cuda\"\n",
        "            )\n",
        "\n",
        "        # --- VALIDATION & SETTINGS BINDING ---\n",
        "        from llama_index.core import Settings\n",
        "        Settings.llm = current_llm\n",
        "        current_model_name = model_name\n",
        "\n",
        "        return f\"‚úÖ System Active: {model_name}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Emergency Recovery: Revert to Gemini to keep the tunnel alive\n",
        "        current_llm = GoogleGenAI(model=\"models/gemini-2.0-flash\")\n",
        "        Settings.llm = current_llm\n",
        "        return f\"‚ö†Ô∏è Fallback Active: {str(e)}\"\n",
        "\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION COMPLETE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yELhtACnssFY"
      },
      "source": [
        "## **SECTION 2C. LLM FACTORY & RESOURCE CONFIGURATION**\n",
        "\n",
        "This section establishes the **Taxonomic Framework** for the platform. It moves beyond simple lists by creating a multi-dimensional mapping of document types.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis:**\n",
        "\n",
        "- **Semantic Mapping:** Defines each category with \"Context Keywords.\" This allows the LLM to identify a document even if the explicit title (e.g., \"Invoice\") is missing, by looking for supporting evidence (e.g., \"amounts due,\" \"billing\").\n",
        "\n",
        "- **Sector Clustering:** Groups document types into industry-specific \"Sectors\" (Real Estate, Healthcare, Legal). This enables the UI to eventually offer \"Industry-Specific Extraction Modes.\"\n",
        "\n",
        "- **Prompt Automation:** Dynamically generates the `TAXONOMY_PROMPT_STR`. This ensures that if you add a new category to the dictionary, the LLM‚Äôs \"instructions\" are automatically updated without manual rewriting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev6---7c6tQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8caabbb3-acbf-4b60-b180-e9e5f307fd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 2C. LLM FACTORY & RESOURCE CONFIGURATION COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 2C. LLM FACTORY & RESOURCE CONFIGURATION -------\n",
        "\n",
        "\n",
        "# --- GLOBAL DOCUMENT TAXONOMY CONFIGURATION ---\n",
        "DOCUMENT_TAXONOMY = {\n",
        "    \"categories\": {\n",
        "        \"Resume\": \"CV, professional profile, work history, Career, experience, education, skills, career summary, employment dates, employment history\",\n",
        "        \"Contract\": \"Legal agreement, force majeure, governing law, indemnification, confidentiality, termination clause, 'in witness whereof', terms and conditions, service agreement, obligations, parties, signature page, clauses, 'hereby agree'\",\n",
        "        \"Mortgage_Contract\": \"Home loan agreement, home loan, deed of trust, lien, mortgage terms, property financing, interest rates, principal amount, principal, note, amortization, escrow, prepayment penalty, borrower, lender\",\n",
        "        \"Invoice\": \"Bill, payment request, financial statement, amounts due, billing, charges, invoiced items, itemized list, subtotal, remittance, 'please pay by'\",\n",
        "        \"Pay_Slip\": \"Salary statement, wage slip, wages, earnings statement, deductions, pay period, year-to-date (YTD), gross pay, net pay, pay period, social security withholding, employer tax ID\",\n",
        "        \"Lender_Fee_Sheet\": \"Loan fee, loan estimate, lender charge, closing cost, closing disclosure, origination fee, appraisal fee, title insurance, escrow deposit, settlement charges\",\n",
        "        \"Land_Deed\": \"Property deed, property ownership, title document, title, ownership certificate, property ownership, grantor, grantee, survey, parcel number, county recorder, transfer of ownership, quitclaim, warranty deed, conveyance, parcel ID, legal description, notary acknowledgement\",\n",
        "        \"Bank_Statement\": \"Account statement, opening balance, account balance, transaction history, deposits, withdrawals, checking/savings, available credit\",\n",
        "        \"Tax_Document\": \"W2, 1099, 1099-MISC, Form 1040, IRS, tax return, tax form, federal income tax, social security wages, tax year, withholding, tax amounts (Includes W2s and Tax Returns)\",\n",
        "        \"Insurance\": \"Insurance policy, coverage document, coverage, policy details, premium, claims, policy declaration, coverage, premium, deductible, insured party, policy number, liability, claims, effective date, policy holder\",\n",
        "        \"Report\": \"Analysis, research document, findings, conclusion, research data, whitepaper, executive summary, methodology\",\n",
        "        \"Legal_Letter\": \"Formal correspondence, formal notice, formal request, attorney-client privilege, re:, service of process, demand letter, notice to quit, legal notification, attorney-client communication, correspondence, memo, communication, requests, instructions, notifications\",\n",
        "        \"Health_Form\": \"Application, questionnaire, data entry form, submitted data, form fields\",\n",
        "        \"ID_Document\": \"Driver's license, passport, passport number, identification, ID numbers, birth certificate, visa, state ID, identity verification, expiration date, photo ID, place of birth, issue date, biometric, state seal\",\n",
        "        \"Medical_Report\": \"Medical report, prescription, health record, health information, medical conditions, patient records, clinical notes, diagnosis, lab results, physician statement\",\n",
        "        \"Other\": \"Miscellaneous documents that do not contain keywords for specific financial, legal, or professional categories, or doesn't fith other categories\"\n",
        "    },\n",
        "    \"sectors\": {\n",
        "        \"Real Estate\": [\"Mortgage_Contract\", \"Lender_Fee_Sheet\", \"Land_Deed\", \"Pay_Slip\", \"Tax_Document\", \"Bank_Statement\", \"Report\"],\n",
        "        \"Healthcare\": [\"Medical\", \"Medical_Report\", \"Health_Form\", \"Insurance\"],\n",
        "        \"Legal\": [\"Contract\", \"Land_Deed\", \"Legal_Letter\", \"Health_Form\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Helper function to get sector for a document type\n",
        "def get_sector_for_type(doc_type: str) -> str:\n",
        "    \"\"\"Returns the primary industry sector for a given document type.\"\"\"\n",
        "    for sector, types in DOCUMENT_TAXONOMY[\"sectors\"].items():\n",
        "        if doc_type in types:\n",
        "            return sector\n",
        "    return \"General\"\n",
        "\n",
        "print(\"‚úÖ SECTION 2C. LLM FACTORY & RESOURCE CONFIGURATION COMPLETE.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLrZv_r30NBc"
      },
      "source": [
        "# **SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT**\n",
        "\n",
        "This section defines the **Data Blueprint** for the entire platform. By using Python `dataclasses`, we create a hierarchical representation of documents that prevents \"context drift.\"\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "- **Physical Layer (** `PageInfo` **):** Captures raw text and page numbers to ensure \"ground truth\" citations.\n",
        "\n",
        "- **Business Layer (**`LogicalDocument` **):** Enables boundary detection. It treats a multi-document PDF as a collection of semantic entities (e.g., separating an Invoice from a Contract within the same file).\n",
        "\n",
        "- **Retrieval Layer (** `ChunkMetadata` **):** The unit of search. It stores rich metadata (IDs and types) alongside embeddings, allowing the vector engine to perform \"Siloed Retrieval\" (filtering results by document type).\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvc4XUc1AvTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132a81fe-96c7-4794-c881-90d7f6897d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DATA STRUCTURES INITIALIZED.\n",
            "‚úÖ SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT -------\n",
        "@dataclass\n",
        "class PageInfo:\n",
        "    \"\"\"\n",
        "    PHYSICAL LAYER: Represents one page of the input file.\n",
        "    Used for OCR tracking and initial classification. Stores information about a single page.\n",
        "    \"\"\"\n",
        "    page_num: int\n",
        "    text: str\n",
        "    doc_type: Optional[str] = None\n",
        "    page_in_doc: int = 0   # Position relative to the logical start\n",
        "\n",
        "@dataclass\n",
        "class LogicalDocument:\n",
        "    \"\"\"\n",
        "    BUSINESS LAYER: Groups pages into a single 'semantic' entity.\n",
        "    Represents a logical document within a PDF.\n",
        "    \"\"\"\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    sector: str = \"General\"\n",
        "    chunks: List[Dict] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class ChunkMetadata:\n",
        "    \"\"\"\n",
        "    RETRIEVAL LAYER: The actual object indexed in the Vector Database.\n",
        "    Rich metadata here allows for 'Siloed Retrieval' (filtering by doc_type).\n",
        "    Rich metadata for each chunk.\n",
        "    \"\"\"\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    chunk_index: int\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    sector: str = \"General\"\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"Converts metadata to a dictionary for LlamaIndex node compatibility.\"\"\"\n",
        "        return {\n",
        "            \"chunk_id\": self.chunk_id,\n",
        "            \"doc_id\": self.doc_id,\n",
        "            \"doc_type\": self.doc_type,\n",
        "            \"sector\": self.sector,\n",
        "            \"page_start\": self.page_start,\n",
        "            \"page_end\": self.page_end\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ DATA STRUCTURES INITIALIZED.\")\n",
        "\n",
        "print(\"‚úÖ SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT COMPLETE.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQNJUFwB2gMZ"
      },
      "source": [
        "# **SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS**\n",
        "\n",
        "This section contains the \"brains\" of the ingestion engine. It moves beyond simple text extraction by adding two critical semantic checks.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "- **Semantic Classification (**`classify_document_type` **):** Instead of just indexing text, the system identifies what it is reading (e.g., a \"Tax Return\" vs. a \"Medical Record\"). This allows for metadata-filtered searches later.\n",
        "\n",
        "- **Logical Boundary Detection\n",
        "(** `detect_document_boundary` **):** This logic prevents \"Context Contamination.\" It analyzes the transition between pages to decide if a new document has started. If page 5 looks like a \"Contract\" but page 6 looks like a \"Bank Statement,\" the system creates a hard boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6u6uV1RJab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf72ba40-6b02-468e-e17f-afc1e5357b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS -------\n",
        "\n",
        "# --- GLOBAL TAXONOMY DEFINITIONS ---\n",
        "\n",
        "# VALID_DOC_TYPES to match DOCUMENT_TAXONOMY keys exactly\n",
        "VALID_DOC_TYPES = list(DOCUMENT_TAXONOMY[\"categories\"].keys())\n",
        "\n",
        "TAXONOMY_PROMPT_STR = \", \".join(VALID_DOC_TYPES)\n",
        "\n",
        "\n",
        "\n",
        "def heuristic_classify(text: str) -> str:\n",
        "    \"\"\"Fallback classifier using keywords if the LLM fails. Case-insensitive.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "\n",
        "    # Mapping keys from DOCUMENT_TAXONOMY to their expanded keyword lists (all lowercase)\n",
        "    keywords_map = {\n",
        "        \"Resume\": [\"cv\", \"work history\", \"skills\", \"education\"],\n",
        "        \"Contract\": [\"agreement\", \"indemnification\", \"confidentiality\", \"hereby agree\"],\n",
        "        \"Mortgage_Contract\": [\"mortgage\", \"deed of trust\", \"amortization\", \"interest rate\"],\n",
        "        \"Invoice\": [\"bill to\", \"amount due\", \"subtotal\", \"tax invoice\"],\n",
        "        \"Pay_Slip\": [\"payslip\", \"pay slip\", \"earnings statement\", \"net pay\", \"ytd\", \"income\", \"salary\", \"base pay\", \"payroll\", \"compensation\"],\n",
        "        \"Lender_Fee_Sheet\": [\"closing cost\", \"origination fee\", \"settlement charges\"],\n",
        "        \"Land_Deed\": [\"grantor\", \"grantee\", \"parcel id\", \"quitclaim\", \"conveyance\"],\n",
        "        \"Bank_Statement\": [\"account statement\", \"transaction history\", \"deposits\"],\n",
        "        \"Tax_Document\": [\"w2\", \"w-2\", \"1099\", \"irs\", \"tax return\", \"income tax\"],\n",
        "        \"Insurance\": [\"insurance policy\", \"premium\", \"easement\", \"encumbrance\"],\n",
        "        \"ID_Document\": [\"passport\", \"driver's license\", \"id number\", \"photo id\"]\n",
        "    }\n",
        "\n",
        "\n",
        "    # TEMPLATE CHECK: If it contains many underscores or dollar signs WITHOUT digits\n",
        "    # Prevents the AI from getting confused by blank forms\n",
        "    underscore_count = text_lower.count(\"____\")\n",
        "    dollar_no_digit = (\"$\" in text_lower and not any(char.isdigit() for char in text_lower))\n",
        "\n",
        "    if underscore_count > 5 or dollar_no_digit or \"sample\" in text_lower:\n",
        "        return \"Other\"\n",
        "\n",
        "    for category, keywords in keywords_map.items():\n",
        "        if any(word in text_lower for word in keywords):\n",
        "            return category\n",
        "\n",
        "    return \"Other\"\n",
        "\n",
        "\n",
        "\n",
        "def classify_document_type(text: str, max_length: int = 2000) -> str:\n",
        "    \"\"\"\n",
        "    Identifies document category by providing the LLM with the full taxonomy context.\n",
        "    \"\"\"\n",
        "\n",
        "    # Truncate text if too long to avoid token limits\n",
        "    # Safety Check: Use a sample to stay within LLM context limits and reduce latency\n",
        "    text_sample = text[:max_length] if len(text) > max_length else text\n",
        "\n",
        "\n",
        "    # 1. Build a context string from your actual taxonomy\n",
        "    taxonomy_context = \"\\n\".join([f\"- {cat}: {desc}\" for cat, desc in DOCUMENT_TAXONOMY[\"categories\"].items()])\n",
        "\n",
        "    # 2. Updated Prompt: No more hardcoded \"Financial Statement\" instructions\n",
        "    prompt = f\"\"\"\n",
        "    You are a document expert. Classify the text into exactly ONE of these categories:\n",
        "    {VALID_DOC_TYPES}\n",
        "\n",
        "    Use these definitions for guidance:\n",
        "    {taxonomy_context}\n",
        "\n",
        "    CRITICAL RULES:\n",
        "    1. Respond with ONLY the category name.\n",
        "    2. If the text looks like a BLANK FORM, TEMPLATE, or contains \"____\" placeholders instead of real data, you MUST use 'Other'.\n",
        "    3. Do NOT classify blank appraisal forms as 'Lender_Fee_Sheet' unless they contain filled-in numbers.\n",
        "    4. If unsure, use 'Other'.\n",
        "\n",
        "    Text snippet:\n",
        "    {text_sample}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Check for MockLLM\n",
        "        if \"MockLLM\" in str(type(Settings.llm)):\n",
        "            return heuristic_classify(text_sample)\n",
        "\n",
        "        response = Settings.llm.complete(prompt).text.strip()\n",
        "\n",
        "        # Exact Match Check\n",
        "        for valid_type in VALID_DOC_TYPES:\n",
        "            if valid_type.lower() == response.lower() or valid_type.lower() in response.lower():\n",
        "                return valid_type\n",
        "\n",
        "        # Fallback to heuristic if LLM output is ambiguous\n",
        "        return heuristic_classify(text_sample)\n",
        "    except Exception:\n",
        "        return heuristic_classify(text_sample)\n",
        "\n",
        "\n",
        "def detect_document_boundary(prev_text: str, curr_text: str,\n",
        "                            current_doc_type: str = None) -> bool:\n",
        "    \"\"\"\n",
        "    Detect if two consecutive pages belong to the same document.\n",
        "    Returns True if they're from the same document.\n",
        "    \"\"\"\n",
        "    # Quick heuristic checks first\n",
        "    if not prev_text or not curr_text:\n",
        "        return False\n",
        "\n",
        "    # Sample the texts for L\\LM analysis\n",
        "    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text\n",
        "    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Determine if these two pages are from the SAME document or different documents.\n",
        "\n",
        "    Current document type: {current_doc_type or 'Unknown'}\n",
        "\n",
        "    End of Previous Page:\n",
        "    ...{prev_sample}\n",
        "\n",
        "    Start of Current Page:\n",
        "    {curr_sample}...\n",
        "\n",
        "    Consider:\n",
        "    - Continuity of content\n",
        "    - Formatting consistency\n",
        "    - Topic coherence\n",
        "    - Page numbers or headers\n",
        "\n",
        "    Default to 'Yes' unless you see a clear signal of a different entity\n",
        "    (e.g., a new person's name on a resume, a different bank logo, or a new header 'Exhibit A').\n",
        "    Answer ONLY 'Yes' or 'No'.\n",
        "\n",
        "    Decision Criteria:\n",
        "    1. Does the sentence from the previous page continue?\n",
        "    2. Is the formatting (headers/footers) consistent?\n",
        "    3. Does the subject matter suddenly shift (e.g., from a lease to a utility bill)?\n",
        "\n",
        "    Answer 'Yes' if they are the SAME document.\n",
        "    Answer 'No' if a NEW document has started.\n",
        "    Respond with ONLY 'Yes' or 'No'.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use the global LlamaIndex LLM setting\n",
        "        response = Settings.llm.complete(prompt)\n",
        "\n",
        "        return response.text.strip().lower().startswith('yes')\n",
        "    except Exception as e:\n",
        "        print(f\"Boundary detection error: {e}\")\n",
        "        # Default to keeping pages together if uncertain\n",
        "        return True\n",
        "\n",
        "print(\"‚úÖ SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS COMPLETE.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGrQwXDp4gh_"
      },
      "source": [
        "# **SECTION 5. ADVANCED PDF PROCESSING PIPELINE**\n",
        "\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section implements the **Transformation Layer** of the platform. It is designed to handle \"dirty\" real-world data through three specialized sub-systems:\n",
        "\n",
        "<br>\n",
        "\n",
        "- **Hybrid OCR Router:** Detects if a page is a \"Searchable PDF\" or a \"Scanned Image.\" If no text is found, it automatically triggers Tesseract OCR to \"see\" the content.\n",
        "\n",
        "- **Taxonomy-Aware Segmentation:** Uses the intelligence functions from Section 4 and the Schema from Section 2C to group pages into `LogicalDocuments` while simultaneously tagging them with their business `Sector`.\n",
        "\n",
        "- **High-Fidelity UI Rendering:** Includes a specialized rendering engine that converts PDF pages into high-contrast images (using `fitz.Matrix(3, 3)`) for display within the Gradio interface, ensuring even small-print legal text is legible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPphWV5WK273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbb22f9-14f9-415c-9fef-9a6f26cf4e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 5. ADVANCED PDF PROCESSING PIPELINE COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 5. ADVANCED PDF PROCESSING PIPELINE -------\n",
        "\n",
        "# --- 1. CORE SEGMENTATION LOGIC ---\n",
        "def analyze_pages(pages_info):  # Shared Analysis Logic\n",
        "    \"\"\"\n",
        "   Groups individual pages into logical business units using the 2C Taxonomy.\n",
        "    Flow: Page Ingestion -> Boundary Detection -> Classification -> Sector Mapping.\n",
        "    \"\"\"\n",
        "\n",
        "    logical_docs = []\n",
        "    current_pages = []\n",
        "    doc_counter = 0\n",
        "\n",
        "    for i, page in enumerate(pages_info):\n",
        "        if i == 0:\n",
        "            # Initialize the first document type\n",
        "            doc_type = classify_document_type(page.text)\n",
        "            current_pages = [page]\n",
        "        else:\n",
        "            # Check if current page is a continuation of the previous one\n",
        "            if detect_document_boundary(pages_info[i-1].text, page.text, doc_type):\n",
        "                current_pages.append(page)\n",
        "            else:\n",
        "              # Boundary detected: Finalize the current logical document\n",
        "                sector = get_sector_for_type(doc_type)\n",
        "\n",
        "                logical_docs.append(\n",
        "                    LogicalDocument(\n",
        "                        doc_id=f\"doc_{doc_counter}\",\n",
        "                        doc_type=doc_type,\n",
        "                        page_start=current_pages[0].page_num,\n",
        "                        page_end=current_pages[-1].page_num,\n",
        "                        text=\"\\n\\n\".join(p.text for p in current_pages),\n",
        "                    )\n",
        "                )\n",
        "                doc_counter += 1\n",
        "                # Start new document tracking\n",
        "                doc_type = classify_document_type(page.text)\n",
        "                current_pages = [page]\n",
        "\n",
        "    # Handle the final trailing document in the sequence\n",
        "    if current_pages:\n",
        "        sector = get_sector_for_type(doc_type)\n",
        "        logical_docs.append(\n",
        "            LogicalDocument(\n",
        "                doc_id=f\"doc_{doc_counter}\",\n",
        "                doc_type=doc_type,\n",
        "                sector=sector,\n",
        "                page_start=current_pages[0].page_num,\n",
        "                page_end=current_pages[-1].page_num,\n",
        "                text=\"\\n\\n\".join(p.text for p in current_pages),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return pages_info, logical_docs\n",
        "\n",
        "\n",
        "# --- 2. MULTI-MODAL INGESTION ROUTERS ---\n",
        "def extract_and_analyze_file(file): # Aware Router\n",
        "    \"\"\"Detects file extension and routes to PDF or Image processor.\"\"\"\n",
        "\n",
        "    ext = os.path.splitext(file.name)[1].lower()\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        return extract_and_analyze_pdf(file)\n",
        "    elif ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
        "        return extract_and_analyze_image(file)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
        "\n",
        "\n",
        "def extract_and_analyze_pdf(pdf_file) -> Tuple[List[PageInfo], List[LogicalDocument]]:\n",
        "    \"\"\"\n",
        "    HYBRID OCR PIPELINE: Extracts digital text or triggers OCR for scanned pages.\n",
        "    \"\"\"\n",
        "\n",
        "    # Capture the actual name from the Gradio file object\n",
        "    original_filename = os.path.basename(pdf_file.name)\n",
        "\n",
        "\n",
        "    print(\"üìñ Starting PDF extraction and analysis for: {original_filename}\")\n",
        "\n",
        "    doc = fitz.open(pdf_file.name) # open file\n",
        "\n",
        "    pages_info = []\n",
        "    for i, page in enumerate(doc):\n",
        "        text = page.get_text().strip()\n",
        "\n",
        "        # Hybrid OCR: If no text found, render page to image and use Tesseract\n",
        "        if not text:\n",
        "            pix = page.get_pixmap()\n",
        "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "            text = pytesseract.image_to_string(img)\n",
        "\n",
        "        pages_info.append(PageInfo(page_num=i, text=text))\n",
        "\n",
        "    doc.close()\n",
        "    return analyze_pages(pages_info)\n",
        "\n",
        "\n",
        "def extract_and_analyze_image(image_file): # Image Ingestion\n",
        "    \"\"\"Processes standalone images via OCR and treats them as a single document.\"\"\"\n",
        "\n",
        "    print(\"üñºÔ∏è Processing Image:\", image_file.name)\n",
        "\n",
        "    img = Image.open(image_file.name)\n",
        "    text = pytesseract.image_to_string(img)\n",
        "\n",
        "    pages_info = [PageInfo(page_num=0, text=text)]\n",
        "    return analyze_pages(pages_info)\n",
        "\n",
        "\n",
        "\n",
        " # --- 3. UI RENDERING LOGIC ---\n",
        "\n",
        "# For Document Viewer in UI (Convert Uploaded PDF file into an image to be viewed in Gradio UI)\n",
        "def load_pdf_into_viewer(selected_file):\n",
        "    \"\"\"Renders PDF pages to crisp images for the Gradio viewer.\"\"\"\n",
        "\n",
        "    if not selected_file or not os.path.exists(str(selected_file)):\n",
        "        return None, {\"current_page\": 0, \"images\": []}, \"**Page 0 of 0**\"\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(selected_file)\n",
        "        images = []\n",
        "        # Matrix(3,3) provides 300 DPI equivalent for high readability\n",
        "        for page in doc:\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(3, 3), colorspace=fitz.csRGB)\n",
        "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "            images.append(img)\n",
        "        doc.close()\n",
        "\n",
        "        indicator = f\"<center>**Page 1 of {len(images)}**</center>\"\n",
        "\n",
        "        return images[0], {\"current_page\": 0, \"images\": images}, f\"<center>**Page 1 of {len(images)}**</center>\"\n",
        "    except Exception as e:\n",
        "        print(f\"Viewer Error: {e}\")\n",
        "        return None, {\"current_page\": 0, \"images\": []}, \"**Error loading viewer**\"\n",
        "\n",
        "\n",
        "def flip_page(direction, state):\n",
        "    \"\"\"Handles 'Next' and 'Previous' button clicks in the UI.\"\"\"\n",
        "    images = state.get(\"images\", [])\n",
        "    current = state.get(\"current_page\", 0)\n",
        "\n",
        "    if not images:\n",
        "        return None, state, \"**Page 0 of 0**\"\n",
        "\n",
        "    current = min(current + 1, len(images) - 1) if direction == \"next\" else max(current - 1, 0)\n",
        "    state[\"current_page\"] = current\n",
        "    indicator = f\"<center>**Page {current + 1} of {len(images)}**</center>\"\n",
        "    return images[current], state, indicator\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 5. ADVANCED PDF PROCESSING PIPELINE COMPLETE.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYroZgDG9FxX"
      },
      "source": [
        "# **SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION**\n",
        "\n",
        "This section defines the **Granular Transformation Layer**. After a document has been logically segmented (e.g., separating an Invoice from a Contract), the text must be broken down into \"chunks\" that fit the LLM's context window while ensuring \"provenance\" (data origin) is never lost.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "- **Semantic Sliding Window:** A custom algorithm that ensures no information is lost at chunk boundaries by creating a calibrated \"overlap.\"\n",
        "\n",
        "- **LlamaIndex Orchestration (** `SentenceSplitter` **):** A high-level path that respects paragraph and sentence boundaries, preventing a chunk from being cut off in the middle of a critical legal clause.\n",
        "\n",
        "- **Metadata Injection:** The \"Secret Sauce.\" Every chunk is stamped with its `doc_type`, `sector`, `doc_id` and `page_range`. This ensures 100% precision in the retrieval phase by filtering out irrelevant document \"silos.\"\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvk9h4e1LUCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f04da5-d21c-4b7b-a501-46eaa556a7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION -------\n",
        "\n",
        "# --- 1. CUSTOM SLIDING WINDOW CHUNKING ---\n",
        "def chunk_document_with_metadata(logical_doc: LogicalDocument,\n",
        "                                chunk_size: int = 500,\n",
        "                                overlap: int = 100) -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Splits a logical document into overlapping chunks while preserving business context.\n",
        "    Ensures that context at boundaries is maintained via the 'stride' method.\n",
        "    \"\"\"\n",
        "\n",
        "    chunks_metadata = []\n",
        "    words = logical_doc.text.split()\n",
        "\n",
        "    # Case A: Document is smaller than the threshold\n",
        "    if len(words) <= chunk_size:\n",
        "        # Document is small enough to be a single chunk\n",
        "        chunk_meta = ChunkMetadata(\n",
        "            chunk_id=f\"{logical_doc.doc_id}_chunk_0\",\n",
        "            doc_id=logical_doc.doc_id,\n",
        "            doc_type=logical_doc.doc_type,\n",
        "            sector=logical_doc.sector,\n",
        "            chunk_index=0,\n",
        "            page_start=logical_doc.page_start,\n",
        "            page_end=logical_doc.page_end,\n",
        "            text=logical_doc.text\n",
        "        )\n",
        "        chunks_metadata.append(chunk_meta)\n",
        "\n",
        "    # Case B: Multi-chunk split with sliding window\n",
        "    else:\n",
        "        # Create overlapping chunks\n",
        "        stride = chunk_size - overlap\n",
        "        for i, start_idx in enumerate(range(0, len(words), stride)):\n",
        "            end_idx = min(start_idx + chunk_size, len(words))\n",
        "            chunk_text = ' '.join(words[start_idx:end_idx])\n",
        "\n",
        "            # Calculate which pages this chunk spans\n",
        "            # (simplified - in production, track more precisely)\n",
        "            chunk_position = start_idx / len(words)\n",
        "            page_range = logical_doc.page_end - logical_doc.page_start\n",
        "            relative_page = int(chunk_position * page_range)\n",
        "            chunk_page_start = logical_doc.page_start + relative_page\n",
        "            chunk_page_end = min(chunk_page_start + 1, logical_doc.page_end)\n",
        "\n",
        "            chunk_meta = ChunkMetadata(\n",
        "                chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
        "                doc_id=logical_doc.doc_id,\n",
        "                doc_type=logical_doc.doc_type,\n",
        "                sector=logical_doc.sector,\n",
        "                chunk_index=i,\n",
        "                page_start=chunk_page_start,\n",
        "                page_end=chunk_page_end,\n",
        "                text=chunk_text\n",
        "            )\n",
        "            chunks_metadata.append(chunk_meta)\n",
        "\n",
        "            if end_idx >= len(words):\n",
        "                break\n",
        "\n",
        "    return chunks_metadata\n",
        "\n",
        "\n",
        "# --- 2. LLAMA-INDEX ADVANCED CHUNKING ---\n",
        "def chunk_with_llama_index(logical_doc: LogicalDocument,\n",
        "                           chunk_size: int = 500,\n",
        "                           chunk_overlap: int = 100) -> List[Document]: # Chunk Metadata\n",
        "    \"\"\"\n",
        "    Uses LlamaIndex SentenceSplitter to ensure chunks respect natural language boundaries.\n",
        "    \"\"\"\n",
        "    # Create LlamaIndex document with metadata\n",
        "    doc = Document(\n",
        "        text=logical_doc.text,\n",
        "        metadata={\n",
        "            \"doc_id\": logical_doc.doc_id,\n",
        "            \"doc_type\": logical_doc.doc_type,\n",
        "            \"sector\": logical_doc.sector, # Carrying over from Section 5\n",
        "            \"page_start\": logical_doc.page_start,\n",
        "            \"page_end\": logical_doc.page_end,\n",
        "            \"source\": f\"{logical_doc.doc_type}_document\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Use LlamaIndex's sentence splitter for better chunking\n",
        "    # Sentence-aware splitter prevents cutting mid-sentence\n",
        "    splitter = SentenceSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        paragraph_separator=\"\\n\\n\",\n",
        "    )\n",
        "\n",
        "    # Create nodes (chunks) from document\n",
        "    nodes = splitter.get_nodes_from_documents([doc])\n",
        "\n",
        "    # Convert to our ChunkMetadata format for consistency\n",
        "    chunks_metadata = []\n",
        "    for i, node in enumerate(nodes):\n",
        "        # IMPORTANT: Explicitly pull from node.metadata\n",
        "        # LlamaIndex nodes store metadata in a .metadata dictionary\n",
        "        m = node.metadata\n",
        "\n",
        "        chunk_meta = ChunkMetadata(\n",
        "            chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
        "            doc_id=m.get(\"doc_id\", logical_doc.doc_id),\n",
        "            doc_type=m.get(\"doc_type\", logical_doc.doc_type),\n",
        "            sector=m.get(\"sector\", logical_doc.sector),\n",
        "            chunk_index=i,\n",
        "            # If splitter doesn't track pages, we fallback to logical_doc values\n",
        "            page_start=m.get(\"page_start\", logical_doc.page_start),\n",
        "            page_end=m.get(\"page_end\", logical_doc.page_end),\n",
        "            text=node.get_content()\n",
        "        )\n",
        "        chunks_metadata.append(chunk_meta)\n",
        "\n",
        "    return chunks_metadata\n",
        "\n",
        "\n",
        "# --- 3. BATCH PROCESSOR ---\n",
        "def process_all_documents(logical_docs: List[LogicalDocument],\n",
        "                         use_llama_index: bool = False) -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Orchestrates the conversion of segmented documents into searchable chunks.\n",
        "    \"\"\"\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    print(f\"üß© Processing {len(logical_docs)} logical documents into chunks...\")\n",
        "\n",
        "    for logical_doc in logical_docs:\n",
        "        if use_llama_index:\n",
        "            chunks = chunk_with_llama_index(logical_doc)\n",
        "        else:\n",
        "            chunks = chunk_document_with_metadata(logical_doc)\n",
        "\n",
        "        logical_doc.chunks = chunks  # Store reference, Maintain parent-child relationship\n",
        "        all_chunks.extend(chunks)\n",
        "        print(f\" üìÑ  ‚àü {logical_doc.doc_type} ({logical_doc.doc_id}): {len(chunks)} chunks created.\")\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION COMPLETE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_MsgfAaAGsm"
      },
      "source": [
        "# **SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL**\n",
        "\n",
        " This section introduces the Search Orchestration Layer. It uses a \"Router-First\" architecture to solve the \"Needle in a Haystack\" problem in large document sets:\n",
        "\n",
        " <br>\n",
        "\n",
        " **Logic and Flow Analysis**\n",
        "\n",
        "- **The Intent Router:** Before searching, the LLM analyzes the query to predict which document type (from Section 2C) contains the answer. It uses a robust JSON-repair logic to ensure the system doesn't crash if the LLM's formatting is imperfect.\n",
        "\n",
        "- **Segregated Indices (Silos):** Instead of one massive index, the `IntelligentRetriever` builds specialized \"mini-indices\" for each document type. This prevents \"contextual noise\" (e.g., a Bank Statement's numbers confusing a Legal Contract's query).\n",
        "\n",
        "- **Confidence-Based Logic:** If the AI is >70% confident in its routing, it searches a specific silo. If unsure, it automatically falls back to a global search, ensuring no information is missed.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvWhBx9-L8iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677a556c-da85-4f54-ea48-04baba05db1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL -------\n",
        "\n",
        "\n",
        "# --- 1. THE ROUTER (INTENT ANALYSIS) ---\n",
        "def predict_query_document_type(query: str, llm=None) -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Predicts the target document category based on the Section 2C Taxonomy.\n",
        "    Returns the predicted 'type' and a 'confidence' score.\n",
        "    \"\"\"\n",
        "\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # HARD OVERRIDES (Instant High Confidence)\n",
        "    # Ensures \"Find Amounts\" and \"Income\" queries bypass the LLM for speed/accuracy\n",
        "    all_keywords = [\"monetary amounts\", \"financial figures\", \"all amounts\", \"every amount\"]\n",
        "    if any(kw in query_lower for kw in all_keywords):\n",
        "        return \"All\", 1.0\n",
        "\n",
        "    income_keywords = [\"income\", \"salary\", \"wages\", \"earnings\", \"take-home pay\"]\n",
        "    if any(kw in query_lower for kw in income_keywords):\n",
        "        return \"Pay_Slip\", 0.95\n",
        "\n",
        "\n",
        "    # SETUP LLM & TAXONOMY\n",
        "    # Use the passed LLM, or fall back to the global Settings.llm\n",
        "    active_llm = llm or Settings.llm\n",
        "\n",
        "    # Extract model name for logging (handles LlamaIndex LLM objects)\n",
        "    model_name = getattr(active_llm,\n",
        "                 \"model_name\",\n",
        "                 \"AI Engine\").lower()\n",
        "\n",
        "    # Access global document taxonomy\n",
        "    valid_keys = list(DOCUMENT_TAXONOMY[\"categories\"].keys())\n",
        "    taxonomy_str = \"\\n\".join([f\"- {k}: {v}\" for k, v in DOCUMENT_TAXONOMY[\"categories\"].items()])\n",
        "\n",
        "    print(f\"üß† Routing via {model_name}...\")\n",
        "\n",
        "\n",
        "\n",
        "    # --- LOGGING THE CHOSEN LLM ---\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n",
        "        \"event\": \"ROUTING_ATTEMPT\",\n",
        "        \"model_used\": model_name,\n",
        "        \"query_preview\": query[:30] + \"...\"\n",
        "    }\n",
        "    audit_logs.append(log_entry)\n",
        "    print(f\"üß† Routing via {model_name}...\")\n",
        "\n",
        "\n",
        "    # CONSTRUCT MODEL-AWARE PROMPTS\n",
        "    is_mistral = \"mistral\" in model_name\n",
        "    is_phi = \"phi\" in model_name\n",
        "    # is_gemini = \"gemini\" in model_name\n",
        "\n",
        "    raw_prompt = f\"\"\"\n",
        "    Analyze the user query and pick the best category from the list.\n",
        "    Query: \"{query}\"\n",
        "\n",
        "    Available categories:\n",
        "    {taxonomy_str}\n",
        "    - All: Use this if the user asks for a summary of EVERY document, \"all amounts\", or \"all figures\" across the entire file.\n",
        "    - Pay_Slip: Use for income, salary, wages, earnings, or take-home pay questions.\n",
        "    - Tax_Document: Use for annual income reports, W2s, or IRS filings.\n",
        "    - Mortgage_Contract: Use for loan terms and interest.\n",
        "\n",
        "\n",
        "    Return ONLY a JSON object:\n",
        "    <json>{{\"type\": \"CategoryName or All\", \"confidence\": 0.9}}</json>\"\"\"\n",
        "\n",
        "    # Apply special wrapping for small/specific models\n",
        "    if is_mistral:\n",
        "        final_prompt = f\"[INST] {raw_prompt} [/INST]\"\n",
        "    elif is_phi:\n",
        "        # Phi-2 works best with very direct few-shot or completion style\n",
        "        final_prompt = f\"Instruct: {raw_prompt}\\nOutput: <json>\"\n",
        "    else:\n",
        "        final_prompt = raw_prompt\n",
        "\n",
        "\n",
        "    try:\n",
        "        # 4. EXECUTION & RESPONSE CLEANING\n",
        "        response_text = active_llm.complete(final_prompt).text.strip()\n",
        "\n",
        "        # Phi-2 fix: if we pre-filled <json>, add it back to the text for parsing\n",
        "        if is_phi and not response_text.startswith(\"<json\"):\n",
        "            response_text = \"<json>\" + response_text\n",
        "\n",
        "        # 5. EXTRACTION LOGIC\n",
        "        json_str = None\n",
        "        xml_match = re.search(r'<json>(.*?)</json>', response_text, re.DOTALL)\n",
        "        if xml_match:\n",
        "            json_str = xml_match.group(1)\n",
        "        else:\n",
        "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group()\n",
        "\n",
        "        doc_type, confidence = \"Other\", 0.0\n",
        "\n",
        "        # Try parsing JSON\n",
        "        if json_str:\n",
        "            try:\n",
        "                # Use your repair_json helper for minor syntax fixes\n",
        "                result = json.loads(repair_json(json_str))\n",
        "                doc_type = result.get(\"type\", \"Other\")\n",
        "                confidence = float(result.get(\"confidence\", 0.0))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # 6. KEYWORD FALLBACK (CRITICAL FOR SMALL MODELS)\n",
        "        # If JSON failed or returned an invalid key, sweep the text for any valid category name\n",
        "        if doc_type not in valid_keys or confidence < 0.1:\n",
        "            for key in valid_keys:\n",
        "                if key.lower() in response_text.lower():\n",
        "                    doc_type, confidence = key, 0.7\n",
        "                    break\n",
        "\n",
        "        # Final validation against taxonomy - ALLOW \"All\"\n",
        "        if doc_type not in valid_keys and doc_type != \"All\":\n",
        "            doc_type = \"Other\"\n",
        "\n",
        "        print(f\"‚úÖ Router assigned: {doc_type} ({confidence*100:.1f}%)\")\n",
        "        return doc_type, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üéØ Routing error: {e}\")\n",
        "        return \"Other\", 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. THE INTELLIGENT RETRIEVER (VECTOR ENGINE) ---\n",
        "class IntelligentRetriever:\n",
        "    \"\"\"\n",
        "    Advanced FAISS retrieval system with metadata-driven Silo Filtering.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.chunks_metadata = [] # Master list of all chunks from all files\n",
        "        self.doc_type_indices = {} # Map of indices per document type\n",
        "\n",
        "    def build_indices(self, new_chunks: List[ChunkMetadata]):\n",
        "        \"\"\"\n",
        "        Builds or updates FAISS indices with new document embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"üî® Processing {len(new_chunks)} new chunks for the vector index...\")\n",
        "\n",
        "        # 1. Create embeddings only for the NEW chunks\n",
        "        print(f\"üî® Processing {len(new_chunks)} new chunks for the vector index...\")\n",
        "        texts = [chunk.text for chunk in new_chunks]\n",
        "        embeddings_list = Settings.embed_model.get_text_embedding_batch(texts, show_progress=True)\n",
        "        new_embeddings = np.array(embeddings_list).astype('float32')\n",
        "        dim = new_embeddings.shape[1]\n",
        "\n",
        "        # Store embeddings in metadata for these new chunks\n",
        "        for i, chunk in enumerate(new_chunks):\n",
        "            chunk.embedding = new_embeddings[i]\n",
        "\n",
        "        # --- TIER 1: GLOBAL INDEX (APPEND MODE) ---\n",
        "        if self.index is None:\n",
        "            self.index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "        self.index.add(new_embeddings)\n",
        "\n",
        "        # IMPORTANT: Append new chunks to the master metadata list\n",
        "        # Prevents previous files from disappearing\n",
        "        self.chunks_metadata.extend(new_chunks)\n",
        "\n",
        "        # --- TIER 2: SEGREGATED INDICES (SILOS) ---\n",
        "        # Updates the silos to include the new data\n",
        "        doc_types = set(chunk.doc_type for chunk in new_chunks)\n",
        "\n",
        "        for doc_type in doc_types:\n",
        "            # Find indices of the new chunks that match this type\n",
        "            # Reference the full self.chunks_metadata to rebuild the mapping correctly\n",
        "            all_type_indices = [idx for idx, chunk in enumerate(self.chunks_metadata)\n",
        "                                if chunk.doc_type == doc_type]\n",
        "\n",
        "            if all_type_indices:\n",
        "                # Rebuild the specific silo index for this type\n",
        "                # (FAISS IndexFlatL2 is fast enough to rebuild for specific silos)\n",
        "                type_embeddings = np.array([self.chunks_metadata[i].embedding for i in all_type_indices]).astype('float32')\n",
        "\n",
        "                type_index = faiss.IndexFlatL2(dim)\n",
        "                type_index.add(type_embeddings)\n",
        "\n",
        "                self.doc_type_indices[doc_type] = {\n",
        "                    'index': type_index,\n",
        "                    'mapping': all_type_indices  # Maps back to the updated master list\n",
        "                }\n",
        "\n",
        "        print(f\"‚úÖ Database updated. Total Chunks: {len(self.chunks_metadata)}\")\n",
        "\n",
        "\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 4,\n",
        "                filter_doc_type: Optional[str] = None,\n",
        "                auto_route: bool = True) -> List[Any]:\n",
        "        \"\"\"Performs a routed or global search based on intent confidence.\"\"\"\n",
        "\n",
        "\n",
        "        # 1. GENERATE QUERY EMBEDDING - Use Settings.embed_model.get_query_embedding\n",
        "        # FAISS expects a 2D numpy array (float32)\n",
        "        # Wrap the single embedding in a list\n",
        "        query_vec = Settings.embed_model.get_query_embedding(query)\n",
        "        query_embedding = np.array([query_vec]).astype('float32')\n",
        "\n",
        "        # Variables to store search results\n",
        "        chunk_indices = []\n",
        "        distances = []\n",
        "\n",
        "\n",
        "        # 2. SELECTION (ROUTING) LOGIC (Which index to search?)\n",
        "        # CASE A: User manually selected a specific filter (and it's not \"All\")\n",
        "        if filter_doc_type and filter_doc_type.lower() != \"all\" and filter_doc_type in self.doc_type_indices:\n",
        "            print(f\"üîç Searching specific silo: {filter_doc_type}\")\n",
        "            type_data = self.doc_type_indices[filter_doc_type]\n",
        "            D, I = type_data['index'].search(query_embedding, k)\n",
        "\n",
        "            # Map the silo-specific index back to the master self.chunks_metadata list\n",
        "            chunk_indices = [type_data['mapping'][i] for i in I[0] if i != -1]\n",
        "            distances = D[0][:len(chunk_indices)]\n",
        "\n",
        "        # CASE B: Auto-Route is enabled (AI guesses the document type)\n",
        "        elif auto_route:\n",
        "            predicted_type, confidence = predict_query_document_type(query)\n",
        "\n",
        "            # If AI is confident and the silo exists, search the silo\n",
        "            if confidence > 0.7 and predicted_type in self.doc_type_indices:\n",
        "                print(f\"üéØ Auto-routed to: {predicted_type} ({confidence:.2%})\")\n",
        "                type_data = self.doc_type_indices[predicted_type]\n",
        "                D, I = type_data['index'].search(query_embedding, k)\n",
        "                chunk_indices = [type_data['mapping'][i] for i in I[0] if i != -1]\n",
        "                distances = D[0][:len(chunk_indices)]\n",
        "            else:\n",
        "                # Fallback to global search if AI is unsure\n",
        "                print(f\"üåê Low routing confidence ({confidence:.2%}). Searching all documents...\")\n",
        "                D, I = self.index.search(query_embedding, k)\n",
        "                chunk_indices = [i for i in I[0] if i != -1]\n",
        "                distances = D[0][:len(chunk_indices)]\n",
        "\n",
        "        # CASE C: Search Everything (Filter is \"All\" or no filter provided)\n",
        "        else:\n",
        "            print(\"üåê Searching global index (all files)...\")\n",
        "            D, I = self.index.search(query_embedding, k)\n",
        "            chunk_indices = [i for i in I[0] if i != -1]\n",
        "            distances = D[0][:len(chunk_indices)]\n",
        "\n",
        "        # 3. CONVERT RESULTS TO SCORED CHUNKS\n",
        "        valid_results = []\n",
        "\n",
        "        # Lower the strict threshold to 0.45 for general use\n",
        "        RELAXED_THRESHOLD = 0.40\n",
        "\n",
        "        for idx, i in enumerate(chunk_indices):\n",
        "            dist = distances[idx]\n",
        "            score = 1.0 / (1.0 + dist)\n",
        "            chunk_obj = self.chunks_metadata[i]\n",
        "\n",
        "            if score >= RELAXED_THRESHOLD:\n",
        "                # Store as a simple namespace or dict to avoid scope/class errors\n",
        "                node_data = type('Node', (object,), {\n",
        "                    'text': chunk_obj.text,\n",
        "                    'metadata': {\n",
        "                        \"page_start\": chunk_obj.page_start,\n",
        "                        \"page_end\": chunk_obj.page_end,\n",
        "                        \"doc_type\": chunk_obj.doc_type,\n",
        "                        \"doc_id\": chunk_obj.doc_id\n",
        "                    },\n",
        "                    'get_content': lambda: chunk_obj.text\n",
        "                })\n",
        "                valid_results.append(type('Result', (object,), {'node': node_data, 'score': score}))\n",
        "\n",
        "        # Safety fallback\n",
        "        if not valid_results and len(chunk_indices) > 0:\n",
        "            print(\"‚ö†Ô∏è Threshold too high. Falling back to top result.\")\n",
        "            idx = chunk_indices[0]\n",
        "            chunk_obj = self.chunks_metadata[idx]\n",
        "            node_data = type('Node', (object,), {\n",
        "                'text': chunk_obj.text,\n",
        "                'metadata': {\"page_start\": chunk_obj.page_start, \"page_end\": chunk_obj.page_end, \"doc_type\": chunk_obj.doc_type},\n",
        "                'get_content': lambda: chunk_obj.text\n",
        "            })\n",
        "            valid_results.append(type('Result', (object,), {'node': node_data, 'score': 1.0 / (1.0 + distances[0])}))\n",
        "\n",
        "        return valid_results\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL COMPLETE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrLmw0oyEGfq"
      },
      "source": [
        "# **SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION**\n",
        "\n",
        "\n",
        "This section represents the final stage of the RAG pipeline. Its goal is to provide evidence-based answers while implementing an automated quality gate.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "1. **Context Synthesis (**  `generate_answer_with_sources` **):** Instead of just passing raw text, the system builds a \"Structured Context.\" Every chunk is prefixed with its metadata (Sector, Doc Type, and Page Numbers). This forces the LLM to provide in-text citations, allowing a human reviewer to verify the answer instantly.\n",
        "\n",
        "2. **Strict Constraint Enforcement:** The prompt is engineered with a \"Closed-Domain\" instruction (Answer based ONLY on provided context). This is your primary defense against hallucinations.\n",
        "\n",
        "3. **The RAG Triad Auditor (** `evaluate_rag_performance` **):** This implements a \"Judge LLM\" to evaluate the response on three metrics: Faithfulness (factuality), Context Relevance (retrieval quality), and Answer Relevance (helpfulness).\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4U5Q3aOMjzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df1d3a1-9e71-4036-e9e5-070f12a7fc65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION -------\n",
        "\n",
        "def clean_llm_json(raw_response):\n",
        "    \"\"\"Fixes JSON formatting and strips Mistral/Llama-CPP instruction tags.\"\"\"\n",
        "    # Convert object to string if it's a Gradio list or dict\n",
        "    text = str(raw_response)\n",
        "\n",
        "    # --- MISTRAL REPAIR: Strip Echoed Prompt Artifacts ---\n",
        "    # Removes everything before the last [/INST] tag if present\n",
        "    if \"[/INST]\" in text:\n",
        "        text = text.split(\"[/INST]\")[-1]\n",
        "\n",
        "    # Remove common local LLM artifacts that break JSON parsers\n",
        "    junk_markers = [\"[INST]\", \"Context:\", \"Question:\", \"Answer:\", \"```json\", \"```\"]\n",
        "    for marker in junk_markers:\n",
        "        text = text.replace(marker, \"\")\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    try:\n",
        "        # Use json_repair to handle trailing commas or missing quotes common in 4-bit Mistral\n",
        "        return json_repair.repair_json(text, return_objects=True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è JSON Repair failed: {e}\")\n",
        "        # Return a basic structure so the UI doesn't crash\n",
        "        return {\"answer\": text, \"sources\": []}\n",
        "\n",
        "\n",
        "# --- THE GENERATOR (CONTEXT-AWARE SYNTHESIS) ---\n",
        "def generate_answer_with_sources(query: str,\n",
        "                                retrieved_chunks: list) -> dict:\n",
        "    \"\"\"\n",
        "    Generate answer with detailed source attribution using the active LLM.\n",
        "    \"\"\"\n",
        "    if not retrieved_chunks:\n",
        "        return {\n",
        "            'answer': \"I couldn't find relevant information to answer your question.\",\n",
        "            'sources': [],\n",
        "            'confidence': 0.0,\n",
        "            'context_text': \"\"\n",
        "        }\n",
        "\n",
        "    # Identify the active model name safely\n",
        "    current_model = getattr(Settings.llm, \"model_name\", \"Mistral 7B\").lower()\n",
        "\n",
        "\n",
        "    # 1.1 Context Preparation\n",
        "    # Prefix every chunk with its 'Physical Provenance' (Type + Page)\n",
        "    context_parts = []\n",
        "    sources = []\n",
        "\n",
        "    for item in retrieved_chunks:\n",
        "        node = item.node\n",
        "        score = item.score\n",
        "        meta = node.metadata\n",
        "\n",
        "        doc_type = meta.get('doc_type', 'Document')\n",
        "        p_start = meta.get('page_start', '?')\n",
        "        p_end = meta.get('page_end', '?')\n",
        "        text_content = node.get_content()\n",
        "\n",
        "        header = f\"[Source: {doc_type}, Pages {p_start}-{p_end}]\"\n",
        "        context_parts.append(f\"{header}\\n{text_content}\\n\")\n",
        "        sources.append({'doc_type': doc_type, 'pages': f\"{p_start}-{p_end}\", 'relevance': f\"{score:.2%}\"})\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "\n",
        "    # Model-Specific Prompting\n",
        "    if \"gemini\" in current_model:\n",
        "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer based ONLY on context. Cite document types and page numbers.\"\n",
        "    elif \"phi\" in current_model:\n",
        "        # Phi-2 works best with \"Instruct/Output\" tags or direct completion\n",
        "        safe_context = context[:4000] # Stricter limit for Phi-2 memory\n",
        "        prompt = f\"Instruct: Use the context below to answer the question.\\nContext: {safe_context}\\nQuestion: {query}\\nOutput:\"\n",
        "    else:\n",
        "        # Mistral 7B Instruction format\n",
        "        prompt = f\"[INST] Answer using ONLY the context provided.\\nContext: {context}\\nQuestion: {query} [/INST]\"\n",
        "\n",
        "    try:\n",
        "        raw_response = Settings.llm.complete(prompt).text.strip()\n",
        "\n",
        "        # Post-process to remove model artifacts\n",
        "        response = raw_response.split(\"Output:\")[-1].split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        avg_score = sum(item.score for item in retrieved_chunks) / len(retrieved_chunks)\n",
        "\n",
        "        return {\n",
        "            'answer': response,\n",
        "            'sources': sources,\n",
        "            'context_text': context,\n",
        "            'confidence': avg_score,\n",
        "            'chunks_used': len(retrieved_chunks)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Generation Error ({current_model}): {e}\")\n",
        "        return {'answer': f\"Error: {str(e)}\", 'sources': sources, 'confidence': 0.0, 'context_text': context}\n",
        "\n",
        "\n",
        "# --- THE AUDITOR (PERFORMANCE METRICS) ---\n",
        "def evaluate_rag_performance(query, context, answer):\n",
        "    \"\"\"\n",
        "    The 'Judge LLM' logic: Evaluates the RAG Triad in JSON format.\n",
        "    Ensures high-fidelity output and detects potential hallucinations.\n",
        "\n",
        "    RAG Triad:\n",
        "    Faithfulness, Answer Relevance, and Context Relevance.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Act as an AI Quality Auditor. Rate this RAG response (1-5 scale).\n",
        "\n",
        "    Query: {query}\n",
        "    Context: {context}\n",
        "    Answer: {answer}\n",
        "\n",
        "    Rate the following from 1-5 (5 is best) in JSON format:\n",
        "    1. Faithfulness (Is the answer supported ONLY by the context?)\n",
        "    2. Context Relevance (Is the retrieved context useful for the query?)\n",
        "    3. Answer Relevance (Does the answer actually address the user's question?)\n",
        "\n",
        "    Respond ONLY in JSON: {{\"faithfulness\": 5, \"relevance\": 4, \"answer_relevance\": 5}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the universal LlamaIndex LLM object\n",
        "        response = Settings.llm.complete(prompt).text.strip()\n",
        "\n",
        "        # 1. Clean the response of Markdown code blocks if they exist\n",
        "        # This removes ```json and ``` wrapping\n",
        "        clean_response = re.sub(r'```(?:json)?\\n?|```', '', response).strip()\n",
        "\n",
        "        # 2. JSON extraction (more targeted)\n",
        "        json_match = re.search(r'(\\{.*\\})', clean_response, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            parsed_data = json.loads(json_match.group(1))\n",
        "\n",
        "            # 3. Ensure keys match what run_performance expects\n",
        "            # Your run_performance uses 'Relevance' (capital R) or 'audit_score'\n",
        "            # Let's standardize the keys here:\n",
        "            return {\n",
        "                \"faithfulness\": float(parsed_data.get(\"faithfulness\", 0)),\n",
        "                \"relevance\": float(parsed_data.get(\"relevance\", 0)),\n",
        "                \"answer_relevance\": float(parsed_data.get(\"answer_relevance\", 0))\n",
        "            }\n",
        "        else:\n",
        "            raise ValueError(f\"No valid JSON found. Raw response: {response[:50]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Audit Evaluation Error: {e}\")\n",
        "        return {\"faithfulness\": 0, \"relevance\": 0, \"answer_relevance\": 0}\n",
        "\n",
        "print(\"‚úÖ SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION COMPLETE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJTVS2CcHBq3"
      },
      "source": [
        "# **SECTION 9. ENHANCED DOCUMENT STORE**\n",
        "\n",
        "The EnhancedDocumentStore manages the document lifecycle using a State-Machine Architecture:\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "- **Unified Ingestion (The \"Push\" Pipeline):** When a user uploads a file, `process_file` triggers a sequential flow: **Extraction ‚Üí Logical Segmentation ‚Üí Siloed Chunking ‚Üí Vector Indexing.** It is designed to be additive, meaning you can upload multiple PDFs and the system will merge their intelligence into one searchable brain.\n",
        "\n",
        "- **Semantic Query Routing:** The `query` method serves as the bridge between the UI and the retrieval engine. It is \"self-healing\"‚Äîif a targeted silo search fails to find an answer, it automatically falls back to a global search, ensuring the user is never met with a \"No results\" error due to a misclassification.\n",
        "\n",
        "- **UI Serialization:** Methods like `get_document_structure` translate complex internal Python objects (Dataclasses) into human-readable strings and labels for the Gradio dashboard.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKXiXfaVNOIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221e38e0-24a4-4060-abe1-a7c947cf72b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 9. ENHANCED DOCUMENT STORE COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 9. ENHANCED DOCUMENT STORE -------\n",
        "class EnhancedDocumentStore:\n",
        "    \"\"\"\n",
        "    The central hub orchestrating the end-to-end RAG lifecycle.\n",
        "    Maintains state for pages, logical documents, and vector indices.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Master State Variables\n",
        "        self.pages_info = []       # List of RawPage objects (from Section 4)\n",
        "        self.logical_docs = []     # List of LogicalDocument objects (from Section 5)\n",
        "        self.chunks_metadata = []  # List of ChunkMetadata objects (from Section 6)\n",
        "\n",
        "        # Core Engines\n",
        "        self.retriever = IntelligentRetriever() # Section 7\n",
        "\n",
        "        # System Metadata\n",
        "        self.is_ready = False\n",
        "        self.processing_stats = {}\n",
        "        self.active_filenames = []\n",
        "\n",
        "\n",
        "    # --- PRIMARY INGESTION PIPELINE ---\n",
        "    def process_pdf(self, pdf_file, filename: str = \"document.pdf\"):\n",
        "        \"\"\"\n",
        "        Executes the full pipeline: Extract -> Segment -> Chunk -> Index.\n",
        "        Supports additive processing (adding new files to existing index).\n",
        "        \"\"\"\n",
        "\n",
        "        self.filename = filename\n",
        "        self.is_ready = False\n",
        "        start_time = datetime.now()\n",
        "\n",
        "\n",
        "        # Step 1: File Type Routing. Get file extension\n",
        "        ext = filename.split('.')[-1].lower()\n",
        "\n",
        "        try:\n",
        "          # --- THE ROUTER LOGIC (Decision Gate) ---\n",
        "          # Step 1: Append instead of Overwrite ---\n",
        "          # Extract new info but add it to our existing lists\n",
        "          new_pages, new_logical_docs = extract_and_analyze_pdf(pdf_file)\n",
        "\n",
        "          self.pages_info.extend(new_pages)\n",
        "          self.logical_docs.extend(new_logical_docs)\n",
        "\n",
        "          # Step 2: Chunking\n",
        "          new_chunks = process_all_documents(new_logical_docs)\n",
        "          self.chunks_metadata.extend(new_chunks) # Add new chunks to the master list\n",
        "\n",
        "          # --- Update Index - Not recreate) ---\n",
        "          # Ensure build_indices function is capable of adding nodes\n",
        "          # or if using VectorStoreIndex: self.vector_index.insert_nodes(new_nodes)\n",
        "          self.retriever.build_indices(new_chunks)\n",
        "\n",
        "          # Step 4: Telemetry (Update stats for total database)\n",
        "          process_time = (datetime.now() - start_time).total_seconds()\n",
        "          self.processing_stats = {\n",
        "              'filename': filename,\n",
        "              'total_pages': len(self.pages_info), # Total pages in entire system\n",
        "              'documents_found': len(self.logical_docs),\n",
        "              'total_chunks': len(self.chunks_metadata),\n",
        "              'document_types': list(set(doc.doc_type for doc in self.logical_docs)),\n",
        "              'processing_time': f\"{process_time:.1f}s\"\n",
        "          }\n",
        "\n",
        "          self.is_ready = True\n",
        "          return True, self.processing_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, {'error': str(e)}\n",
        "\n",
        "\n",
        "\n",
        "    # --- 1. THE INGESTION ENGINE ---\n",
        "    def process_file(self, file):\n",
        "        \"\"\"\n",
        "        Executes the full pipeline: Extract -> Segment -> Chunk -> Index.\n",
        "        Ensures a hard reset to maintain data privacy between uploads.\n",
        "        \"\"\"\n",
        "\n",
        "        self.filename = os.path.basename(file.name)\n",
        "        start = time.time()\n",
        "\n",
        "        print(f\"‚öôÔ∏è Orchestrator: Starting pipeline for {self.filename}\")\n",
        "\n",
        "        # Step 1: Extract NEW content\n",
        "        new_pages, new_docs = extract_and_analyze_file(file)\n",
        "\n",
        "        # Step 2: Chunk and Append\n",
        "        new_chunks = process_all_documents(new_docs)\n",
        "\n",
        "        self.pages_info.extend(new_pages)\n",
        "        self.logical_docs.extend(new_docs)\n",
        "        self.chunks_metadata.extend(new_chunks)\n",
        "\n",
        "\n",
        "        # Step 3: Index (Append Mode)\n",
        "        self.retriever.build_indices(new_chunks)\n",
        "\n",
        "        # Step 4: Stats\n",
        "        self.processing_stats = {\n",
        "            \"filename\": self.filename,\n",
        "            \"total_pages\": len(self.pages_info),\n",
        "            \"total_chunks\": len(self.chunks_metadata),\n",
        "            \"document_types\": list(set(doc.doc_type for doc in self.logical_docs)),\n",
        "            \"processing_time\": f\"{time.time() - start:.2f}s\",\n",
        "        }\n",
        "\n",
        "        self.is_ready = True\n",
        "        return True, self.processing_stats\n",
        "\n",
        "    # --- 2. THE QUERY ENGINE ---\n",
        "    def query(self, question: str, filter_type: Optional[str] = None,\n",
        "             auto_route: bool = True, k: int = 4) -> Dict:\n",
        "        \"\"\"\n",
        "        Handles user queries with automated intent routing and silo fallbacks.\n",
        "        \"\"\"\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"Please upload and process a PDF first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        # Sanitize the filter: Convert \"All\" strings to None for global search\n",
        "        search_filter = None\n",
        "        if filter_type and str(filter_type).strip().lower() != \"all\":\n",
        "            search_filter = filter_type\n",
        "\n",
        "        # FIRST ATTEMPT: Targeted Retrieval. Retrieve relevant chunks (Section 7 - Segregated Retrieval)\n",
        "        retrieved = self.retriever.retrieve(\n",
        "            question, k=k,\n",
        "            filter_doc_type=search_filter,\n",
        "            auto_route=auto_route\n",
        "        )\n",
        "\n",
        "        # FALLBACK: If 0 results found and we used a filter, try searching EVERYTHING\n",
        "        if not retrieved and search_filter is not None:\n",
        "            print(f\"‚ö†Ô∏è No results found for silo '{search_filter}'. Falling back to Global Search...\")\n",
        "            retrieved = self.retriever.retrieve(\n",
        "                question, k=k,\n",
        "                filter_doc_type=None, # Remove the filter\n",
        "                auto_route=False      # Disable routing for the fallback\n",
        "            )\n",
        "            filter_type = \"All (Fallback)\"\n",
        "\n",
        "        # GENERATE RESPONSE - (Section 8 - Evidence-based Response)\n",
        "        # This function should take the list of nodes and return a dict with 'answer' and 'context_text'\n",
        "        result = generate_answer_with_sources(question, retrieved)\n",
        "\n",
        "        # Ensure 'retrieved_chunks' is in the dictionary so chat_with_status can see it\n",
        "        result['retrieved_chunks'] = retrieved\n",
        "        result['filter_used'] = filter_type or ('auto' if auto_route else 'none')\n",
        "\n",
        "        # Calculate a simple confidence score for the logs\n",
        "        result['confidence'] = sum([n.score for n in retrieved]) / len(retrieved) if retrieved else 0.0\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    # --- 3. UI HELPER METHODS ---\n",
        "    def get_document_structure(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get the document structure for UI display.\n",
        "        \"\"\"\n",
        "        if not self.logical_docs:\n",
        "            return []\n",
        "\n",
        "        structure = []\n",
        "        for doc in self.logical_docs:\n",
        "            structure.append({\n",
        "                'id': doc.doc_id,\n",
        "                'type': doc.doc_type,\n",
        "                'pages': f\"{doc.page_start + 1}-{doc.page_end + 1}\",  # 1-indexed for UI\n",
        "                'chunks': len(doc.chunks) if doc.chunks else 0,\n",
        "                'preview': doc.text[:200] + \"...\" if len(doc.text) > 200 else doc.text\n",
        "            })\n",
        "\n",
        "        return structure\n",
        "\n",
        "\n",
        "# --- UI FILTERING LOGIC ---\n",
        "\n",
        "def get_filtered_structure(selected_filters):\n",
        "    \"\"\"\n",
        "    selected_filters: List of strings from the Multiselect (e.g., [\"Type: Report\", \"File: my_doc.pdf\"])\n",
        "    \"\"\"\n",
        "    # 1. Get all logical documents from your store\n",
        "    # (Using the LogicalDocument dataclass from your Section 3)\n",
        "    all_docs = doc_store.logical_documents\n",
        "\n",
        "    if not selected_filters or \"All\" in selected_filters:\n",
        "        filtered = all_docs\n",
        "    else:\n",
        "        # Extract the actual values from the labels\n",
        "        type_filters = [f.replace(\"Type: \", \"\") for f in selected_filters if f.startswith(\"Type: \")]\n",
        "        file_filters = [f.replace(\"File: \", \"\") for f in selected_filters if f.startswith(\"File: \")]\n",
        "\n",
        "        filtered = [\n",
        "            d for d in all_docs\n",
        "            if d.doc_type in type_filters or os.path.basename(d.source) in file_filters\n",
        "        ]\n",
        "\n",
        "    # 2. Build the display string\n",
        "    structure_lines = [\"üß¨ FILTERED DOCUMENT STRUCTURE:\"]\n",
        "    current_file = \"\"\n",
        "    for doc in filtered:\n",
        "        fname = os.path.basename(doc.source)\n",
        "        if fname != current_file:\n",
        "            structure_lines.append(f\"\\nüìÇ FILE: {fname}\")\n",
        "            current_file = fname\n",
        "        structure_lines.append(f\"   ‚îî‚îÄ üè∑Ô∏è {doc.doc_type.upper()} | üìë Pgs: {doc.page_start + 1}-{doc.page_end + 1}\")\n",
        "\n",
        "    return \"\\n\".join(structure_lines)\n",
        "\n",
        "print(\"‚úÖ SECTION 9. ENHANCED DOCUMENT STORE COMPLETE.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txaCA0n9MNYN"
      },
      "source": [
        "# **SECTION 10. BACKEND CHAT  & AUDIT  LOGIC**\n",
        "\n",
        "This section introduces the RAG Triad evaluation and the Data Serialization engine.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "- **Golden Datasets:** By defining ground-truth Q&A pairs for Healthcare, Legal, and Real Estate, you move from \"guessing\" if the AI is right to \"measuring\" it.\n",
        "\n",
        "- **The AI Auditor:** The `evaluate_response_audit` function uses a \"Judge LLM\" pattern. It asks the model to critique its own work or another model's work, returning structured JSON to calculate faithfulness and relevance.\n",
        "\n",
        "- **Performance Telemetry:** The `run_performance_audit` function acts as a data scientist. It calculates latency, tokens-per-second, and success rates, then generates a visual bottleneck report using `Seaborn`.\n",
        "\n",
        "- **Document Intelligence Bridge:** `process_pdf_handler` is the crucial \"middleman\" that connects the Gradio UI upload button to the `EnhancedDocumentStore` .\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAtFrYkcvP_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d763ed9c-b9d1-46ba-9cec-5b404e9dbe40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 10. BACKEND CHAT  & AUDIT  LOGIC COMPLETE.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 10. BACKEND CHAT  & AUDIT  LOGIC -------\n",
        "\n",
        "\n",
        "# 1. GLOBAL STORE INSTANCE (Initialize The Engine)\n",
        "doc_store = EnhancedDocumentStore()\n",
        "audit_logs = [] # Global registry for session analytics\n",
        "\n",
        "# 2. - GOLDEN DATASETS (Define Ground-Truth) -\n",
        "# GOLDEN DATASETS to test RAG Pipleine responses with source of truth\n",
        "GOLDEN_DATASETS = {\n",
        "    \"Healthcare\": [\n",
        "        {\"question\": \"What is the primary diagnosis?\", \"golden_answer\": \"Diagnosis of Type 2 Diabetes with neuropathy.\"},\n",
        "        {\"question\": \"What are the latest lab results for Glucose?\", \"golden_answer\": \"Fasting glucose was 145 mg/dL.\"\n",
        "}\n",
        "    ],\n",
        "    \"Legal\": [\n",
        "        {\"question\": \"What is the termination notice period?\", \"golden_answer\": \"The agreement requires a 30-day written notice for termination.\"},\n",
        "        {\"question\": \"Who are the parties involved?\", \"golden_answer\": \"Between Acme Corp and John Smith.\"}\n",
        "    ],\n",
        "    \"Real Estate\": [\n",
        "        {\"question\": \"What is the total cash to close?\", \"golden_answer\": \"The estimated cash to close is $95,802.\"},\n",
        "        {\"question\": \"What is the loan amount and the interest rate?\", \"golden_answer\": \"The loan amount is $380,000 and the interest rate is 4.25%.\"},\n",
        "        {\"question\": \"Who are the applicants and what is the property address?\", \"golden_answer\": \"The applicants are John Q. Smith and Mary A. Smith. The property is 1254 Main Street, San Diego, CA 92110.\"},\n",
        "        {\"question\": \"What is the estimated cash to close?\", \"golden_answer\": \"The estimated cash to close is $95,802.\"},\n",
        "        {\"question\": \"Is there a prepayment penalty or balloon payment?\", \"golden_answer\": \"No, the loan does not have a prepayment penalty or a balloon payment.\"},\n",
        "        {\"question\": \"What are the total origination charges in Section A?\", \"golden_answer\": \"The total origination charges are $1,070, including an Underwriting Fee of $550, Wire Transfer Fee of $75, and Administration Fee of $445.\"}\n",
        "\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. - AUDIT EVALUATOR FOR MISTRAL LLM -\n",
        "def evaluate_response_audit(query: str, response: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Uses a Judge LLM to score the RAG response based on:\n",
        "    1. Faithfulness (Is it derived from context?)\n",
        "    2. Relevance (Does it answer the user?)\n",
        "    \"\"\"\n",
        "    active_llm = Settings.llm\n",
        "    model_name = getattr(active_llm, \"model_name\", \"AI Engine\")\n",
        "    is_mistral = \"Mistral\" in model_name\n",
        "\n",
        "    raw_audit_prompt = f\"\"\"\n",
        "    Evaluate this Q&A pair:\n",
        "    Query: {query}\n",
        "    AI Response: {response}\n",
        "\n",
        "    Return ONLY JSON:\n",
        "    {{\"score\": 0.9, \"reasoning\": \"1-sentence explanation\"}}\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply Mistral tags if needed\n",
        "    final_audit_prompt = f\"[INST] {raw_audit_prompt} [/INST]\" if is_mistral else raw_audit_prompt\n",
        "\n",
        "    try:\n",
        "        raw_output = active_llm.complete(final_audit_prompt).text.strip()\n",
        "\n",
        "        # Robust JSON search\n",
        "        json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            repaired = repair_json(json_match.group())\n",
        "            result = json.loads(repaired)\n",
        "            return result\n",
        "        else:\n",
        "            raise ValueError(\"Auditor output was not structured JSON\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Audit Error: {e}\")\n",
        "        return {\"score\": 0.0, \"reasoning\": \"Audit engine parsing failed.\"}\n",
        "\n",
        "\n",
        " #-- 4. PERFORMANCE AUDIT & VISUALIZATION LOGIC ---\n",
        " # Generates The Accuracy Metrics & Plots\n",
        "def run_performance_audit(doc_filter, audit_num_chunks):\n",
        "    \"\"\"\n",
        "    Calculates Speed, Chunk Metrics, and RAG Triad scores for the Audit Dashboard.\n",
        "    \"\"\"\n",
        "    if not audit_logs:\n",
        "        return \"**Avg Latency:** N/A\", {}, \"N/A\", None, [[\"No Data\", \"-\", \"-\", \"-\"]]\n",
        "\n",
        "    # Convert logs to DataFrame for filtering. Filter logs based on active UI selection\n",
        "    full_df = pd.DataFrame(audit_logs).copy()\n",
        "\n",
        "    # --- Ensure required columns exist ---\n",
        "    required_cols = ['latency', 'audit_score', 'Relevance', 'Filter_Used']\n",
        "    for col in required_cols:\n",
        "        if col not in full_df.columns:\n",
        "            full_df[col] = 0 if col != 'Filter_Used' else 'Unknown'\n",
        "\n",
        "    # 1. DEFINE SECTOR MAPPINGS\n",
        "    # This ties the UI selection to the AI's classification types\n",
        "    sector_map = {\n",
        "        \"Real Estate\": [\n",
        "            \"Mortgage_Contract\", \"Lender_Fee_Sheet\", \"Land_Deed\",\n",
        "            \"Pay_Slip\", \"Tax_Document\", \"Bank_Statement\", \"Report\",\n",
        "            \"Other\"\n",
        "        ],\n",
        "        \"Healthcare\": [\n",
        "            \"Medical\", \"Medical_Report\", \"Insurance\", \"Health_Form\",\"Other\"\n",
        "        ],\n",
        "        \"Legal\": [\n",
        "            \"Contract\", \"Land_Deed\", \"Legal_Letter\", \"Form\", \"Other\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 3. APPLY FILTERING\n",
        "    if doc_filter == \"All\":\n",
        "        filtered_df = full_df.copy()\n",
        "    elif doc_filter in sector_map:\n",
        "        relevant_types = sector_map[doc_filter]\n",
        "        filtered_df = full_df[full_df['Filter_Used'].isin(relevant_types)].copy()\n",
        "    else:\n",
        "        filtered_df = full_df[full_df['Filter_Used'] == doc_filter].copy()\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        return f\"**No audit data for: {doc_filter}**\", {}, \"0%\", \"0%\", None, [[\"No Data\", \"-\", \"-\", \"-\"]]\n",
        "\n",
        "\n",
        "    # 4. DATA CLEANING\n",
        "    for col in ['audit_score', 'Relevance', 'latency']:\n",
        "        if col in filtered_df.columns:\n",
        "            # This converts \"4\" or 4.0 to float, and handles errors\n",
        "            filtered_df[col] = pd.to_numeric(filtered_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # 5. CALCULATE METRICS\n",
        "    avg_latency = filtered_df['latency'].mean()\n",
        "    avg_tokens = 150\n",
        "    tokens_per_sec = avg_tokens / avg_latency if avg_latency > 0 else 0\n",
        "\n",
        "    # Success Rate (Faithfulness/Audit Score > 0.7)\n",
        "    success_rate = (filtered_df['audit_score'] > 0.7).mean() * 100\n",
        "\n",
        "    # Context Density Calculation\n",
        "    # Make sure we aren't dividing by zero\n",
        "    avg_relevance = filtered_df['Relevance'].mean() if not filtered_df.empty else 0\n",
        "    context_density = (avg_relevance / 5) * 100 # Assuming a 0-5 scale\n",
        "\n",
        "    # 6. VISUALIZATION\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.barplot(x=['Retriever', 'LLM Speed'],\n",
        "                y=[avg_latency * 0.3, tokens_per_sec / 10],\n",
        "                hue=['Retriever', 'LLM Speed'],\n",
        "                palette=\"viridis\",\n",
        "                legend=False)\n",
        "    plt.title(f\"Efficiency Metrics | Sector: {doc_filter}\")\n",
        "    plt.savefig(\"bottlenecks.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 7. UI TABLE DATA\n",
        "    audit_table_data = [\n",
        "        [\"Generation Speed\", f\"{tokens_per_sec:.1f} t/s\", \"12.5 t/s\", \"Industrial\"],\n",
        "        [\"Context Precision\", f\"{context_density:.0f}%\", \"85%\", \"Target\"],\n",
        "        [\"Avg Latency\", f\"{avg_latency:.1f}s\", \"3.0s\", \"Target\"],\n",
        "        [\"Processed Silos\", f\"{len(filtered_df['Filter_Used'].unique())}\", \"N/A\", \"Count\"]\n",
        "    ]\n",
        "\n",
        "    return (\n",
        "        f\"**Avg Latency:** {avg_latency:.2f}s | **Speed:** {tokens_per_sec:.1f} tokens/sec\",\n",
        "        {\"Faithfulness\": avg_relevance/5, \"Context Density\": context_density/100},\n",
        "        f\"{success_rate:.1f}%\",\n",
        "        f\"{context_density:.0f}%\",\n",
        "        \"bottlenecks.png\",\n",
        "        audit_table_data\n",
        "    )\n",
        "\n",
        "# --- 5. BATCH UPLOAD & UI STATE HANDLER  TO UPLOAD & PROCESS PDF\n",
        "def process_pdf_handler(file_list):\n",
        "    \"\"\"\n",
        "    Orchestrates the ingestion of multiple files and updates UI components.\n",
        "    Returns: (Status Message, Structure JSON, Structure Display, Filter Update, View Selector)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "      if not file_list:\n",
        "\n",
        "              # Return empty defaults for all 6 outputs\n",
        "              return (\n",
        "                \"‚ö†Ô∏è No files uploaded.\",\n",
        "                \"[]\",\n",
        "                \"\",\n",
        "                gr.update(choices=[\"All\"], value=[\"All\"]),\n",
        "                gr.update(choices=[], value=None),\n",
        "                \"No file uploaded\"\n",
        "              )\n",
        "\n",
        "      file_reports = []\n",
        "      all_doc_types = set()\n",
        "      all_filenames = []\n",
        "      view_selector_choices = []\n",
        "\n",
        "      total_pages = 0\n",
        "      total_chunks = 0\n",
        "      start_batch_time = datetime.now()\n",
        "\n",
        "      for file in file_list:\n",
        "          # full_path is the /tmp/gradio/... path needed for the PDF viewer\n",
        "          full_path = file.name\n",
        "          # fname is the clean name for the UI\n",
        "          fname = os.path.basename(full_path)\n",
        "\n",
        "          # 2. PROCESS FILE - Call the Orchestrator from Section 9\n",
        "          # Pass full_path to the engine so it can actually read the bits\n",
        "          success, stats = doc_store.process_pdf(file, filename=fname)\n",
        "\n",
        "          if success:\n",
        "              all_doc_types.update(stats.get('document_types', []))\n",
        "              all_filenames.append(fname)\n",
        "\n",
        "              # Create the (Label, Value) tuple for the dropdown\n",
        "              view_selector_choices.append((fname, full_path))\n",
        "\n",
        "              total_pages += stats.get('total_pages', 0)\n",
        "              total_chunks += stats.get('total_chunks', 0)\n",
        "\n",
        "              # Build a clean plain-text report for this specific file\n",
        "              report = (\n",
        "                f\"üíæ  {fname}\\n\"\n",
        "                f\"   ‚îî‚îÄ üìÑ Pages: {stats['total_pages']} | üß© Chunks: {stats['total_chunks']}\\n\"\n",
        "                f\"   ‚îî‚îÄ üè∑Ô∏è Types: {', '.join(stats['document_types'])}\\n\"\n",
        "                f\"   ‚îî‚îÄ ‚è±Ô∏è Time: {stats['processing_time']}\"\n",
        "              )\n",
        "\n",
        "              # Build individual file report line\n",
        "              file_reports.append(report)\n",
        "\n",
        "          else:\n",
        "            file_reports.append(f\"‚ùå {fname} | FAILED: {stats.get('error', 'Unknown Error')}\")\n",
        "\n",
        "\n",
        "      # --- DATA AGGREGATION for STRUCTURE VIEW LOGIC---\n",
        "      # 3. STRUCTURE DATA AGGREGATION - Generate Structure Visuals\n",
        "      structure_json = doc_store.get_document_structure()\n",
        "      structure_lines = [\"üß¨ GLOBAL DOCUMENT STRUCTURE:\"]\n",
        "      current_file = \"\"\n",
        "\n",
        "      for doc in structure_json:\n",
        "          doc_source = doc.get('source') or doc.get('filename') or doc.get('file_name') or \"Unknown File\"\n",
        "\n",
        "          if doc_source != current_file:\n",
        "              # Clean up path if it's a full /tmp/ path\n",
        "            display_name = os.path.basename(doc_source)\n",
        "            structure_lines.append(f\"\\nüìÇ FILE: {display_name}\")\n",
        "            current_file = doc_source\n",
        "\n",
        "          structure_lines.append(f\"   ‚îî‚îÄ üè∑Ô∏è {doc['type'].upper()} | üìë Pgs: {doc['pages']} | üß© {doc['chunks']} chunks\")\n",
        "\n",
        "      structure_display = \"\\n\".join(structure_lines)\n",
        "\n",
        "      # CONSTRUCT THE MAIN STATUS LOG\n",
        "      batch_time = (datetime.now() - start_batch_time).total_seconds()\n",
        "      joined_reports = \"\\n\\n\".join(file_reports)\n",
        "\n",
        "      status_msg = f\"\"\"\n",
        "  ================================================================\n",
        "  üìÇ BATCH PROCESSING COMPLETE ({batch_time:.1f}s)\n",
        "\n",
        "  {joined_reports}\n",
        "\n",
        "  -----------------------------------------------------------------\n",
        "  üìä TOTAL BATCH STATS:\n",
        "  Files: {len(all_filenames)} | Pages: {total_pages} | Chunks: {total_chunks}\n",
        "  =================================================================\"\"\"\n",
        "\n",
        "\n",
        "      # JSON String for the Code box\n",
        "      # Convert the list (structure_json) to a JSON string\n",
        "      # indent=2 makes it look like a pretty-printed JSON object in the UI\n",
        "      structure_json_string = json.dumps(structure_json, indent=2)\n",
        "\n",
        "      # 4. PREPARE SMART FILTERS (Types + Files)\n",
        "      # Create labels that distinguish between Document Types and Specific Files\n",
        "      unique_types = sorted(list(all_doc_types))\n",
        "      type_options = [f\"Type: {t}\" for t in unique_types]\n",
        "      file_options = [f\"File: {f}\" for f in sorted(all_filenames)]\n",
        "\n",
        "      # Dynamic UI Filter logic. Combine them into one list for the multiselect dropdown\n",
        "      smart_filter_choices = [\"All\"] + type_options + file_options\n",
        "\n",
        "\n",
        "      # Update the the Search Document Filter Dropdown\n",
        "      doc_type_filter_update = gr.update(choices=smart_filter_choices, value=[\"All\"])\n",
        "\n",
        "\n",
        "      # Update the \"Select File to View\" Dropdown\n",
        "      # choices = paths, value = the first path in the list\n",
        "      view_selector_update = gr.update(\n",
        "          choices=view_selector_choices,\n",
        "          value=view_selector_choices[0][1] if view_selector_choices else None\n",
        "      )\n",
        "\n",
        "      # 5. WIRING THE RETURN\n",
        "      # Ensure outputs match the click event:\n",
        "      # (status, json_code, textbox_display, doc_filter, view_selector, status_bar)\n",
        "      return (\n",
        "          \"\\n\\n\".join(file_reports),                         # 1. status_msg (Textbox)\n",
        "          json.dumps(structure_json, indent=2),               # 2. structure_json (Code)\n",
        "          \"\\n\".join(structure_lines),                         # 3. structure_display (Textbox)\n",
        "          gr.update(choices=smart_filter_choices, value=[\"All\"]), # 4. doc_type_filter (Multiselect)\n",
        "          gr.update(                                          # 5. view_selector (Dropdown)\n",
        "              choices=view_selector_choices,\n",
        "              value=view_selector_choices[0][1] if view_selector_choices else None\n",
        "          ),\n",
        "          f\"‚úÖ Successfully indexed {len(all_filenames)} files.\" # 6. op_status_bar (Status Label)\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Process Error: {e}\")\n",
        "        return f\"Error: {str(e)}\", \"[]\", \"‚ùå Failed\", gr.update(), gr.update(), \"Error\"\n",
        "\n",
        "\n",
        "# ------- 6. EXPORT LOGIC (ReportLab) - PERFORMANCE AUDIT REPORT EXPORT (Logic for File Generation) ------- #\n",
        "def handle_audit_export(audit_data):\n",
        "    \"\"\"\n",
        "    Logic to convert your dataframe/audit results into a PDF.\n",
        "    This is similar to your chat export but for the audit tab.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure audit_data is a DataFrame and not empty\n",
        "    if audit_data is None or (isinstance(audit_data, pd.DataFrame) and audit_data.empty):\n",
        "        return gr.update(visible=False, value=None), \"‚ö†Ô∏è No audit data available to export.\"\n",
        "\n",
        "    # Create a temporary file\n",
        "    fd, path = tempfile.mkstemp(suffix=\".pdf\")\n",
        "    os.close(fd) # Close immediately to allow ReportLab to write to it\n",
        "\n",
        "    try:\n",
        "        doc = SimpleDocTemplate(path, pagesize=letter)\n",
        "        styles = getSampleStyleSheet()\n",
        "        elements = []\n",
        "\n",
        "        # 2. Add Header\n",
        "        title_style = ParagraphStyle(\n",
        "            'Title',\n",
        "            parent=styles['Heading1'],\n",
        "            fontSize=16,\n",
        "            spaceAfter=20,\n",
        "            alignment=1  # Center alignment\n",
        "        )\n",
        "        elements.append(Paragraph(\"AI-Powered Document Intelligence - Performance Audit Report\", title_style))\n",
        "\n",
        "        # Date and Time\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "        elements.append(Paragraph(f\"Generated on: {current_time}\", styles['Normal']))\n",
        "        elements.append(Spacer(1, 20))\n",
        "\n",
        "        # 3. Process Table Data\n",
        "        # Convert DataFrame to list of lists (Header + Rows)\n",
        "        # Ensure all values are strings for ReportLab\n",
        "        data = [audit_data.columns.to_list()] + audit_data.values.tolist()\n",
        "\n",
        "        # Create the Table object\n",
        "        audit_table = Table(data, hAlign='CENTER')\n",
        "\n",
        "        # Apply Industry-Standard Styling\n",
        "        audit_table.setStyle(TableStyle([\n",
        "            ('BACKGROUND', (0, 0), (-1, 0), colors.darkslategray), # Header Background\n",
        "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),     # Header Text\n",
        "            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
        "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
        "            ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
        "            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
        "            ('BACKGROUND', (0, 1), (-1, -1), colors.whitesmoke),   # Body Background\n",
        "            ('GRID', (0, 0), (-1, -1), 1, colors.black),           # Table Grid\n",
        "            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]) # Striped rows\n",
        "        ]))\n",
        "\n",
        "        elements.append(audit_table)\n",
        "        elements.append(Spacer(1, 30))\n",
        "        elements.append(Paragraph(\"<b>End of Performance Audit Report</b>\", styles['Italic']))\n",
        "\n",
        "        # 4. Build PDF\n",
        "        doc.build(elements)\n",
        "\n",
        "        # 5. Final check and return\n",
        "        if os.path.exists(path):\n",
        "            # We return two things: the file update and the status message\n",
        "            return gr.update(value=path, visible=True, label=\"üì• Download Performance Audit Report\"), \"‚úÖ Audit report generated successfully!\"\n",
        "        else:\n",
        "            return gr.update(visible=False), \"‚ùå Error: PDF file was not created.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Process Error: {e}\")\n",
        "        # Return 6 items to match the expected Gradio outputs\n",
        "        return (\n",
        "            f\"‚ùå Error: {str(e)}\", # 1. Status Message\n",
        "            \"[]\",                  # 2. JSON Code\n",
        "            \"‚ùå Processing Failed\", # 3. Display Text\n",
        "            gr.update(),           # 4. Filter Update\n",
        "            gr.update(),           # 5. View Selector\n",
        "            \"‚ö†Ô∏è System Error\"      # 6. Status Bar\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# --- 7. REPORT GENERATION UTILITIES ---\n",
        "\n",
        "# Wrapper to combine to generate PDF, export, and download Performance Audit. Works with 'handle_audit_export' function\n",
        "def handle_audit_export_ui(audit_data):\n",
        "    \"\"\"\n",
        "    UI Wrapper: Connects the Audit Dashboard state to the PDF downloader.\n",
        "    Categorized as: UI-Backend Bridge.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Call your existing PDF generator\n",
        "    # handle_audit_export usually returns (gr.update(value=path), status_msg)\n",
        "    file_update, status_msg = handle_audit_export(audit_data)\n",
        "\n",
        "    # 2. Extract the actual string path from the dictionary\n",
        "    file_path = file_update.get(\"value\") if isinstance(file_update, dict) else file_update\n",
        "\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        # We return the STRING path for the DownloadButton\n",
        "        # and the status message for the status bar\n",
        "        return file_path, status_msg\n",
        "\n",
        "    return None, \"‚ùå Export failed: No data found.\"\n",
        "\n",
        "\n",
        "# CHAT HISTORY EXPORT (Logic for PDF File Generation) & DOWNLOAD CHAT HISTORY\n",
        "def export_chat_history_to_pdf(history):\n",
        "    \"\"\"\n",
        "    Transforms the live chat session into a formatted PDF document.\n",
        "    Categorized as: Data Serialization logic.\n",
        "    \"\"\"\n",
        "\n",
        "    if not history or len(history) == 0:\n",
        "        return None  # No file to download\n",
        "\n",
        "    try:\n",
        "        # 1. Create temporary file path\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        file_path = f\"/content/AI-Powered_Document_Intelligence_Platform_Chat_History_{timestamp}.pdf\"\n",
        "\n",
        "        # 2. Setup ReportLab PDF\n",
        "        doc = SimpleDocTemplate(file_path, pagesize=letter)\n",
        "        styles = getSampleStyleSheet()\n",
        "        elements = []\n",
        "\n",
        "        # --- CUSTOM STYLES ---\n",
        "        user_header_style = ParagraphStyle('UserHeader', parent=styles['Normal'], fontSize=10,\n",
        "                                          textColor=colors.white, backColor=HexColor(\"#2E4053\"),\n",
        "                                          borderPadding=5, borderRadius=3, spaceAfter=5)\n",
        "\n",
        "        ai_header_style = ParagraphStyle('AIHeader', parent=styles['Normal'], fontSize=10,\n",
        "                                        textColor=colors.white, backColor=HexColor(\"#1A5276\"),\n",
        "                                        borderPadding=5, borderRadius=3, spaceAfter=5)\n",
        "\n",
        "        metadata_style = ParagraphStyle('Metadata', parent=styles['Normal'], fontSize=8,\n",
        "                                       textColor=colors.grey, fontName=\"Courier-Oblique\",\n",
        "                                       leftIndent=20, spaceBefore=5)\n",
        "\n",
        "\n",
        "        # --- HEADER & TITLE TO ELEMENTS ---\n",
        "        # timestamp/header info at the top right\n",
        "        gen_time = datetime.now().strftime(\"%B %d, %Y | %H:%M\")\n",
        "        elements.append(Paragraph(f\"Generated on: {gen_time}\", styles['Heading1']))\n",
        "        elements.append(Paragraph(f\"AI Document Intelligence Automation Platform\", styles['Heading1']))\n",
        "        elements.append(Spacer(1, 10))\n",
        "\n",
        "        # Main Title\n",
        "        elements.append(Paragraph(\"AI-Powered Document Intelligence Automation Platform Chat History\", styles['Normal']))\n",
        "\n",
        "        # --- ADD 3 LINES OF EXTRA SPACE AFTER MAIN TITLE ---\n",
        "        elements.append(Spacer(1, 45)) # Approximately 3 lines of space (15 units per line)\n",
        "\n",
        "\n",
        "        # ------ BUILD CHAT CONTENT ------\n",
        "\n",
        "        # HANDLE DICTIONARY FORMAT ---\n",
        "        for entry in history:\n",
        "            # 1. Extract role and content safely\n",
        "            if not isinstance(entry, dict): continue\n",
        "\n",
        "            role = entry.get(\"role\", \"user\")\n",
        "            content_data = entry.get(\"content\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "            # --- CRITICAL FIX FOR LIST ERROR ---\n",
        "            # Extract text from Gradio's complex message format if it's a list\n",
        "            if isinstance(content_data, list):\n",
        "                raw_text = \" \".join([item.get(\"text\", \"\") for item in content_data if isinstance(item, dict)])\n",
        "            else:\n",
        "                raw_text = str(content_data)\n",
        "\n",
        "            # --- CLEANING (DO NOT PURGE \\n YET) ---\n",
        "            # Remove raw structural code artifacts\n",
        "            clean_text = re.sub(r\"\\[\\{'text':\\s*'\", \"\", raw_text)\n",
        "            clean_text = re.sub(r\"',\\s*'type':\\s*'text'\\}\\]\", \"\", clean_text)\n",
        "\n",
        "\n",
        "            # Purge symbols but LEAVE \\n and \\r for the splitting logic below\n",
        "            purge_list = [\"‚ñ†\", \"ü§ñ\", \"üë§\", \"‚úÖ\", \"‚è≥\", \"üß†\", \"üéØ\", \"****\", \"**\", \"##\"]\n",
        "            for sym in purge_list:\n",
        "                clean_text = clean_text.replace(sym, \"\")\n",
        "\n",
        "            clean_text = clean_text.replace(\"AI Document Assistant:\", \"\").replace(\"You:\", \"\").strip()\n",
        "\n",
        "            # Handle Metadata Split (Keep text before '---')\n",
        "            parts = clean_text.split('---')\n",
        "            main_answer = parts[0].strip()\n",
        "            metadata = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "            # Add Role Header\n",
        "            header_text = \"<b>USER QUERY</b>\" if role == \"user\" else \"<b>AI ASSISTANT VERIFIED RESPONSE</b>\"\n",
        "            elements.append(Paragraph(header_text, user_header_style if role == \"user\" else ai_header_style))\n",
        "\n",
        "            # --- BULLET POINT & NEW LINE LOGIC ---\n",
        "            # We split by actual newlines to create distinct Paragraph blocks\n",
        "            # This makes reading much easier than one big block of text\n",
        "            text_lines = main_answer.replace(\"\\\\n\", \"\\n\").split('\\n')\n",
        "            for line in text_lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    elements.append(Spacer(1, 6)) # Adds space between paragraphs\n",
        "                    continue\n",
        "\n",
        "                # Detect bullet points (starts with *, -, or :)\n",
        "                if line.startswith('*') or line.startswith('-') or line.startswith(':'):\n",
        "                    # Clean the prefix and add a professional bullet\n",
        "                    bullet_text = f\"&bull; {line.lstrip('*-: ').strip()}\"\n",
        "                    elements.append(Paragraph(bullet_text, styles['Normal']))\n",
        "                else:\n",
        "                    elements.append(Paragraph(line, styles['Normal']))\n",
        "\n",
        "            # 6. Add Metadata (Audit Trail) in a separate gray box\n",
        "            if role != \"user\" and metadata:\n",
        "                elements.append(Spacer(1, 5))\n",
        "                # Filter metadata to remove the ugly \\n‚ñ†‚ñ† prefixes\n",
        "                clean_meta = metadata.replace(\"\\\\n\", \" \").replace(\"n‚ñ†‚ñ†\", \"\").replace(\"‚ñ†\", \"\").strip()\n",
        "                elements.append(Paragraph(f\"<i>Audit Trail:</i> {clean_meta}\", metadata_style))\n",
        "\n",
        "            elements.append(Spacer(1, 15)) # Space after each Q&A turn\n",
        "\n",
        "        doc.build(elements)\n",
        "        return file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"PDF Export Error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ SECTION 10. BACKEND CHAT  & AUDIT  LOGIC COMPLETE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80rvI0tlTlEu"
      },
      "source": [
        "# **SECTION 11. CHATBOT LOGIC & ORCHESTRATION**\n",
        "\n",
        "This section is characterized by its use of **Python Generators (** `yield` **).** This is a sophisticated architectural choice that allows the UI to stay \"alive\" while the heavy lifting happens in the background. Instead of the user waiting 5 seconds for a blank screen to update, they see a step-by-step progress report (Routing -> Searching -> Analyzing -> Scoring).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "- **Safety First:** It performs \"Context Window Safety,\" capping the number of retrieved chunks (k) based on the model's specific limits (e.g., lower for Phi-2, higher for Mistral).\n",
        "\n",
        "- **The Fallback Loop:** If a filtered search (e.g., looking only in \"Legal\") fails, it automatically expands to a \"Global\" search so the user never hits a dead end.\n",
        "\n",
        "- **The Quality Gate:** It doesn't just show an answer; it calculates a **Faithfulness Score** (0.0‚àí5.0) before displaying the result, giving the user a \"Confidence Meter.\"\n",
        "\n",
        "- **Hardware Management:** The `deep_purge_gpu` logic is vital for the Free Tier (T4 GPU), ensuring that VRAM is zeroed out before a new model is loaded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFK3Gn4dNgOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e386bb1-dee7-4ac4-ec87-26045c66c959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 11. CHATBOT LOGIC & ORCHESTRATION Complete\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 11. CHATBOT LOGIC & ORCHESTRATION -------\n",
        "\n",
        "\n",
        "# Chat handler with status bar update. Define how the AI thinks and responds.\n",
        "def chat_with_status(message, history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "        \"\"\"\n",
        "        Primary UI Controller.\n",
        "        Manages the 'Thinking Loop' and streams status updates via yield.\n",
        "        \"\"\"\n",
        "\n",
        "        global current_model_name\n",
        "\n",
        "        # HISTORY INITIALIZATION\n",
        "        # Handles both list-of-dicts and list-of-lists (Gradio formats)\n",
        "        if history is None:\n",
        "          history = []\n",
        "\n",
        "\n",
        "        # DYNAMIC ENGINE DETECTION\n",
        "        active_engine_name = (\n",
        "            globals().get('current_model_name') or\n",
        "            getattr(Settings.llm, \"model_name\", \"AI Engine\")\n",
        "        )\n",
        "\n",
        "\n",
        "        # FILTER SANITIZATION\n",
        "        # Ensure filter is a string (hashable) for the Vector Store\n",
        "        # Gradio dropdowns often pass a list like ['Contract']. LlamaIndex filters need the string 'Contract'.\n",
        "        if isinstance(doc_type_filter, list) and len(doc_type_filter) > 0:\n",
        "            clean_filter = str(doc_type_filter[0])\n",
        "        elif doc_type_filter and doc_type_filter != \"none\":\n",
        "            clean_filter = str(doc_type_filter)\n",
        "        else:\n",
        "            clean_filter = \"All\"\n",
        "\n",
        "        filter_label = clean_filter\n",
        "\n",
        "        # READINESS CHECK - Check if documents exist\n",
        "        if not doc_store.is_ready:\n",
        "            response = \"üìö Please upload and process a PDF document first.\"\n",
        "            history.append({\"role\": \"user\", \"content\": f\"**üë§ You:** {message}\"})\n",
        "            history.append({\"role\": \"assistant\", \"content\": f\"**ü§ñ AI Docuement Assistant:** {response}\"})\n",
        "            yield history, \"‚ö†Ô∏è System Not Ready\"\n",
        "            return\n",
        "\n",
        "\n",
        "        # START PROCESSING TELEMETRY\n",
        "        # Responsive UI Start. Immediate UI Feedback (Streaming Yield)\n",
        "        # Immediately tell the user the AI is working so the app feels responsive.\n",
        "        log_entry = None\n",
        "        start_total = time.time()\n",
        "        routed_type = clean_filter\n",
        "        routing_confidence = 1.0  # Default for manual selection\n",
        "\n",
        "\n",
        "        # AGENTIC ROUTING (UI FEEDBACK LAYER)\n",
        "        if auto_route:\n",
        "            yield history, f\"üéØ AI ({active_engine_name}) is routing...\"\n",
        "            # Pass Settings '.llm' to ensure it uses the engine selected in the UI\n",
        "            routed_type, routing_confidence = predict_query_document_type(message, Settings.llm)\n",
        "            clean_filter = routed_type\n",
        "            filter_label = f\"{routed_type} ({routing_confidence:.1%})\"\n",
        "            print(f\"‚úÖ Router assigned category: {routed_type} ({routing_confidence:.2%})\")\n",
        "\n",
        "\n",
        "        # STREAMING ANALYTICS (Update UI to show the 'Silo' being searched)\n",
        "        # Responsive UI Start\n",
        "        history.append({\"role\": \"user\", \"content\": f\"**üë§ You:** {message}\"})\n",
        "        history.append({\"role\": \"assistant\", \"content\": f\"**ü§ñ AI Document Assistant is üß† Analyzing {filter_label} documents...**\"})\n",
        "        yield history, f\"‚è≥ Searching (Filter: {filter_label})...\"\n",
        "\n",
        "        # --- CONTEXT WINDOW SAFETY LOGIC ---\n",
        "        # Map safety limits based on the current global model name\n",
        "        active_k = int(audit_num_chunks) if audit_num_chunks else 5\n",
        "\n",
        "        # Check if the global variable 'current_model_name' exists (from Section 2B)\n",
        "        # Conservative if model name contains \"Phi\" or \"Mistral\"\n",
        "        # Only reduce if the slider is HIGHER than the model's physical limit\n",
        "        if \"Phi-2\" in active_engine_name:\n",
        "            if active_k > 3: # Phi-2 can usually handle 3 chunks safely\n",
        "                print(f\"‚ö†Ô∏è Phi-2 context safety. Capping slider from {active_k} to 3.\")\n",
        "                active_k = 3\n",
        "        elif \"Mistral\" in active_engine_name:\n",
        "            if active_k > 6: # Mistral can handle about 6 chunks on a T4\n",
        "                print(f\"‚ö†Ô∏è Mistral context safety. Capping slider from {active_k} to 6.\")\n",
        "                active_k = 6\n",
        "        # --------------------------------------------------\n",
        "\n",
        "        # DEBUG PRINT: Verify what we are asking the database\n",
        "        print(f\"DEBUG: Querying for '{message}' with filter '{clean_filter}'\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"üîç DEBUG: Sending to Database...\")\n",
        "        print(f\"   > Query: '{message}'\")\n",
        "        print(f\"   > Applied Filter: '{clean_filter}'\")\n",
        "        print(f\"   > Search Depth (k): {active_k} (Safety Adjusted)\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "\n",
        "        try:\n",
        "            # ----- RETRIEVAL -----\n",
        "            # Ensure search_filter is None if \"All\" is selected to bypass metadata silos\n",
        "            search_filter = None if clean_filter.strip().lower() == \"all\" else clean_filter\n",
        "\n",
        "            # Limit the search depth 'k' based on the user's Audit Slider\n",
        "            result = doc_store.query(\n",
        "                message,\n",
        "                filter_type=search_filter,\n",
        "                auto_route=False,\n",
        "                k=active_k\n",
        "            )\n",
        "\n",
        "            # Extract the chunks correctly for the next step\n",
        "            # Note: your doc_store.query likely returns chunks in a key called 'retrieved_chunks'\n",
        "            chunks_to_process = result.get('retrieved_chunks', [])\n",
        "\n",
        "\n",
        "            # --- SMART FALLBACK ---\n",
        "            if len(chunks_to_process) == 0 and search_filter is not None:\n",
        "                print(f\"‚ö†Ô∏è Zero results in '{search_filter}'. Retrying with Global Search...\")\n",
        "                result = doc_store.query(message, filter_type=None, k=active_k)\n",
        "                chunks_to_process = result.get('retrieved_chunks', [])\n",
        "                applied_filter = \"Global Fallback\" # Update this for the footer\n",
        "            else:\n",
        "                applied_filter = result.get('filter_used', clean_filter)\n",
        "\n",
        "\n",
        "            # --- NEW SYSTEM WRAPPING LOGIC ---\n",
        "            if \"dollar amounts\" in message.lower() or \"financial figures\" in message.lower():\n",
        "                # Financial Extraction System Instruction\n",
        "                system_instruction = (\n",
        "                    \"You are a financial auditor. Extract only digits and dollar values. \"\n",
        "                    \"Ignore blank form fields, underscores (___), and empty placeholders. \"\n",
        "                    \"If a field is empty, do not mention it.\"\n",
        "                )\n",
        "            elif \"summary\" in message.lower():\n",
        "                # Summary System Instruction\n",
        "                system_instruction = \"You are an executive assistant. Provide a structured, concise overview of the text.\"\n",
        "            else:\n",
        "                # General Instruction\n",
        "                system_instruction = \"You are a helpful document assistant. Answer based strictly on the context provided.\"\n",
        "\n",
        "            # Combine for the LLM\n",
        "            effective_query = f\"[SYSTEM: {system_instruction}]\\n\\n[USER REQUEST: {message}]\"\n",
        "\n",
        "            # 7. GENERATE\n",
        "            generation_result = generate_answer_with_sources(effective_query, chunks_to_process)\n",
        "            answer = generation_result.get('answer', \"I'm sorry, I couldn't generate an answer for that.\")\n",
        "\n",
        "            # POST-GENERATION CLEANUP (The \"Safety Net\")\n",
        "            # Strips out if model echoes rules\n",
        "            if \"IMPORTANT RULES\" in answer:\n",
        "                answer = answer.split(\"completely.\")[-1].strip()\n",
        "            if \"professional executive summary\" in answer:\n",
        "                answer = answer.split(\"points.\")[-1].strip()\n",
        "\n",
        "            # EVALUATION & AUDIT\n",
        "            context_text = generation_result.get('context_text', \"\")\n",
        "\n",
        "            # POST-GENERATION AUDIT (The Quality Gate)\n",
        "            # Evaluate the 'RAG Triad' immediately after generation\n",
        "            scores = evaluate_rag_performance(message, context_text, answer)\n",
        "            latency = time.time() - start_total\n",
        "\n",
        "\n",
        "            # LOGGING - Inside chat_with_status when creating log_entry:\n",
        "            log_entry = {\n",
        "                \"timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n",
        "                \"model\": active_engine_name, # Dynamic model name,\n",
        "                \"query\": message[:50],\n",
        "                \"latency\": round(latency, 3),\n",
        "                \"routed_category\": routed_type,\n",
        "                \"audit_score\": float(scores.get(\"faithfulness\", 0)),\n",
        "                \"Relevance\": float(scores.get(\"relevance\", 0)),\n",
        "                \"Filter_Used\": str(clean_filter), # Ensure this is a string, not a list\n",
        "            }\n",
        "\n",
        "\n",
        "            audit_logs.append(log_entry) # Ensure audit_logs = [] is defined in Section 1\n",
        "\n",
        "            # --------- FINAL UI RESPONSE CONSTRUCTION --------------\n",
        "\n",
        "            # RETRIEVED DATA PROCESSING\n",
        "            retrieved_chunks = result.get('retrieved_chunks', [])\n",
        "            chunk_count = len(retrieved_chunks)\n",
        "\n",
        "            # SOURCE LOGIC\n",
        "            unique_sources_list = []  # Initialize a list first\n",
        "\n",
        "            if retrieved_chunks:\n",
        "                for c in retrieved_chunks:\n",
        "                    # LlamaIndex returns a NodeWithScore object; metadata is in .node.metadata\n",
        "                    node = c.node if hasattr(c, 'node') else (c[0] if isinstance(c, (tuple, list)) else c)\n",
        "                    meta = getattr(node, 'metadata', {})\n",
        "\n",
        "                    # Fallback cascade: Meta dict -> Object attribute -> Default '?'\n",
        "                    d_type = meta.get('doc_type', getattr(node, 'doc_type', 'Other'))\n",
        "                    p_start = meta.get('page_start', getattr(node, 'page_start', '?'))\n",
        "                    p_end = meta.get('page_end', getattr(node, 'page_end', '?'))\n",
        "\n",
        "                    label = f\"{d_type} (p.{p_start}-{p_end})\"\n",
        "                    if label not in unique_sources_list:\n",
        "                        unique_sources_list.append(label)\n",
        "\n",
        "            # FORMAT SOURCES TEXT\n",
        "            sources_text = f\"\\n\\nüîç **Sources:** {', '.join(unique_sources_list)}\" if unique_sources_list else \"\"\n",
        "\n",
        "            conf_suffix = f\" | üéØ Routing Confidence: {routing_confidence:.1%}\" if auto_route else \"\"\n",
        "\n",
        "\n",
        "            # 4. METADATA FOOTER LOGIC\n",
        "            applied_filter = result.get('filter_used', 'Global')\n",
        "            stats_text = (\n",
        "                f\"\\n\\n---\\n\"\n",
        "                f\"*‚è±Ô∏è {latency:.2f}s | ü§ñ **Engine: {active_engine_name}**{conf_suffix} *\\n\"\n",
        "                f\"|*‚úÖ Faithfulness: {scores.get('faithfulness', 0)}/5 | üß© Retrieved from {chunk_count} document chunks*\"\n",
        "                f\"\\n*üìÇ Search Scope: {applied_filter}*\"\n",
        "            )\n",
        "\n",
        "            # 5. FINAL ASSEMBLY\n",
        "            full_response = (\n",
        "                f\"ü§ñ **AI Document Assistant:**\\n\"\n",
        "                f\"{answer.strip()}\\n\\n\"\n",
        "                f\"(Answer derived from {chunk_count} document chunks.)\"\n",
        "                f\"{sources_text}\"\n",
        "                f\"{stats_text}\"\n",
        "            )\n",
        "\n",
        "            history[-1] = {\"role\": \"assistant\", \"content\": full_response}\n",
        "\n",
        "            yield history, \"‚úÖ Response Generated\"\n",
        "\n",
        "        except Exception as e:\n",
        "            # THIS IS THE MISSING BLOCK\n",
        "            error_msg = f\"**ü§ñ AI Document Assistant:** ‚ö†Ô∏è Error: {str(e)}\"\n",
        "            history[-1] = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield history, \"‚ùå Search Failed\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. CENTRAL SWITCHING & VRAM MANAGEMENT ---\n",
        "def deep_purge_gpu():\n",
        "\n",
        "    global current_llm\n",
        "\n",
        "    current_llm = None\n",
        "\n",
        "    Settings.llm = None\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Comprehensive CUDA purge\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect() # Clears inter-process memory\n",
        "        torch.cuda.synchronize()\n",
        "    print(\"üßπ VRAM Deep Purged.\")\n",
        "\n",
        "\n",
        "def handle_model_transition(model_name, history, clear_history):\n",
        "    \"\"\"\n",
        "    The UI manager that talks to Gradio. Backend connector for the UI dropdown.\n",
        "    Switches the LLM and manages the Chatbot state.\n",
        "    \"\"\"\n",
        "   # 1. Clear history if the checkbox is checked\n",
        "    if clear_history:\n",
        "        history = []\n",
        "\n",
        "    # Get the result string from the backend\n",
        "    switch_result = switch_llm(model_name)\n",
        "\n",
        "    # 2. Call the backend switch function (Section 2B)\n",
        "    # Force switch_result to a string to prevent \"NoneType\" iteration errors\n",
        "    result_text = str(switch_result) if switch_result else \"‚ùå Error: Unknown Failure\"\n",
        "\n",
        "    if \"‚ùå\" in switch_result:\n",
        "        new_status = f\"### ‚ö†Ô∏è Error: {result_text}\"\n",
        "\n",
        "    else:\n",
        "        # 3. Dynamic Status Construction\n",
        "        status_type = \"History Cleared\" if clear_history else \"Context Retained\"\n",
        "        new_status = f\"### üß† Engine: {model_name} | ‚úÖ {status_type}\"\n",
        "\n",
        "    return history, new_status\n",
        "print(\"‚úÖ SECTION 11. CHATBOT LOGIC & ORCHESTRATION Complete\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pYWo7wCa47E"
      },
      "source": [
        "# **SECTION 12. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v71VnYSsMJGu"
      },
      "source": [
        "## **SECTION 12A. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC**\n",
        "\n",
        "This section serves as the **Environmental Patch Layer**. In complex environments like Google Colab or Python 3.12+, asynchronous event loops (which Gradio uses to handle multiple users or streaming text) can often \"clash\" or crash.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "- **Event Loop Patching:**\n",
        "By using `nest_asyncio`, the system allows the UI to run inside an already-active notebook loop without hanging.\n",
        "\n",
        "- **Uvicorn Override:** The patch for `loop_factory` is a critical stability fix. It prevents a known \"TypeError\" in newer Python versions by stripping out incompatible arguments before the server starts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqNUZCvxUbuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad762b81-2128-4d4b-b928-3d47c1709427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 12A. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC  Complete\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 12A. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC -------\n",
        "\n",
        "\n",
        "# 1. Force Standard Policy (Insurance against uvloop hijacking)\n",
        "if sys.platform != \"win32\":\n",
        "    asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n",
        "\n",
        "# 2. THE MONKEY PATCH: Intercepts the Gradio-Uvicorn handshake\n",
        "original_run = uvicorn.run\n",
        "\n",
        "def patched_uvicorn_run(*args, **kwargs):\n",
        "    # Fix 1: Strip the 'loop_factory' which crashes Python 3.12/Colab\n",
        "    if \"loop_factory\" in kwargs:\n",
        "        kwargs.pop(\"loop_factory\")\n",
        "\n",
        "    # Fix 2: Force standard asyncio and limit concurrency for T4 stability\n",
        "    kwargs[\"loop\"] = \"asyncio\"\n",
        "\n",
        "    # Fix 3: Ensure uvloop isn't used even if requested\n",
        "    if \"http\" in kwargs and kwargs[\"http\"] == \"httptools\":\n",
        "        kwargs[\"http\"] = \"h11\"\n",
        "\n",
        "    return original_run(*args, **kwargs)\n",
        "\n",
        "# Inject the patch\n",
        "uvicorn.run = patched_uvicorn_run\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 12A. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC  Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkOvoN4kNCeK"
      },
      "source": [
        "## **SECTION 12B. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC**\n",
        "\n",
        "This is the Layout & Event Wiring Layer. It follows a \"Pillar Architecture\" designed for professional document workflows.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis:**\n",
        "\n",
        "1. **Behavioral Scripting (JS):** Uses a JavaScript `MutationObserver` to ensure the chatbot automatically scrolls to the bottom as the AI \"types.\"\n",
        "\n",
        "2. **The Three-Pillar Layout:**\n",
        "  - **Tab 1 (Operations):** The primary workspace. It features a \"Dual-Column\" design where the left side handles the \"Physical\" (files, page viewing, engine selection) and the right side handles the \"Digital\" (the chat conversation).\n",
        "  \n",
        "  - **Tab 2 (Transparency):** Dedicated to explainability. It shows the **JSON Structure** of how the AI \"sees\" the document, which is vital for developer debugging.\n",
        "\n",
        "  - **Tab 3 (Governance):** The Audit dashboard. It visualizes metrics like **Latency vs. Token Speed** and **Context Density**, translating technical data into business-ready status reports.\n",
        "\n",
        "3. **Event Wiring:** This uses the `.click()`, .`change()`, and `.submit()` methods to create a \"Reactive\" interface. Specifically, the use of `.then()` allows for \"Chain Reactions\"‚Äîfor example, sending a message and then automatically clearing the input box for the next question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryrTe7SQOG0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b4bcd9-c2b5-4b6e-cf79-6a1d437be970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 12B. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC Complete.\n"
          ]
        }
      ],
      "source": [
        "# ------- SECTION 12B. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC -------\n",
        "\n",
        "\n",
        "# --- JavaScript is for behavior (Auto-Scroll) ---\n",
        "scroll_script = \"\"\"\n",
        "function() {\n",
        "    const targetNode = document.querySelector('#chatbot-box');\n",
        "    if (!targetNode) {\n",
        "        console.log(\"Chatbot box not found yet...\");\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    const observer = new MutationObserver(() => {\n",
        "        // In newer Gradio, the scrollable area is usually a 'div' inside the chatbot\n",
        "        const scrollContainer = targetNode.querySelector('.scrollable-auto') || targetNode.querySelector('.wrapper') || targetNode;\n",
        "        if (scrollContainer) {\n",
        "            scrollContainer.scrollTo({\n",
        "                top: scrollContainer.scrollHeight,\n",
        "                behavior: 'smooth'\n",
        "            });\n",
        "        }\n",
        "    });\n",
        "\n",
        "    observer.observe(targetNode, { childList: true, subtree: true });\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------- UI LAYOUT  ------------------------------------------------------------------------------------- #\n",
        "\n",
        "# CSS: We target only the 'header-container' for centering\n",
        "# CSS: Targets only the tab navigation bar to make it look like a black menu\n",
        "# Targets Download & Export buttons for Chat History & Performance Audit Report\n",
        "custom_css = \"\"\"\n",
        "    /* Center the header text */\n",
        "    .welcome-text-header-container {\n",
        "        text-align: center;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "\n",
        "    /* 1. FORCE ALL BUTTONS TO BLACK */\n",
        "    /* This targets primary buttons, secondary buttons, and specific IDs */\n",
        "    button.primary, button.secondary, #dark-btn, #chat-export-btn, #ingest_btn, #send_btn {\n",
        "        background-color: black !important;\n",
        "        background: black !important;\n",
        "        color: white !important;\n",
        "        border: 1px solid #444 !important;\n",
        "        box-shadow: none !important;\n",
        "    }\n",
        "\n",
        "    /* Button Hover Effect */\n",
        "    button.primary:hover, button.secondary:hover {\n",
        "        background-color: #222 !important;\n",
        "        border-color: #00d1b2 !important;\n",
        "    }\n",
        "\n",
        "    /* 2. NAVIGATION BAR (TABS) STYLING */\n",
        "    /* The horizontal strip background */\n",
        "    .tabs > .tab-nav {\n",
        "        background-color: black !important;\n",
        "        border-bottom: 2px solid #333 !important;\n",
        "        padding: 8px 10px 0px 10px !important;\n",
        "        display: flex !important;\n",
        "        gap: 5px !important;\n",
        "        border-radius: 8px 8px 0 0 !important;\n",
        "    }\n",
        "\n",
        "    /* Individual Tab Labels (Inactive) */\n",
        "    .tabs > .tab-nav > button {\n",
        "        background-color: #111 !important; /* Very dark grey for inactive */\n",
        "        color: white !important;           /* White font */\n",
        "        border: none !important;\n",
        "        border-radius: 5px 5px 0 0 !important;\n",
        "        padding: 10px 25px !important;\n",
        "        font-weight: bold !important;\n",
        "    }\n",
        "\n",
        "    /* The Active (Selected) Tab */\n",
        "    .tabs > .tab-nav > button.selected {\n",
        "        background-color: black !important; /* Pure black for active */\n",
        "        color: #00d1b2 !important;           /* Highlight font color */\n",
        "        border-bottom: 3px solid #00d1b2 !important;\n",
        "    }\n",
        "\n",
        "    /* LABELS (The specific fix you requested) */\n",
        "    .gradio-container .label {\n",
        "        background-color: black !important;\n",
        "        color: white !important;\n",
        "        padding: 4px 10px !important;\n",
        "        border-radius: 5px 5px 0 0 !important;\n",
        "        border: 1px solid #444 !important;\n",
        "        box-shadow: none !important;\n",
        "        font-weight: bold !important;\n",
        "    }\n",
        "\n",
        "    .control-frame {\n",
        "    border: 1px solid #e0e0e0;\n",
        "    border-radius: 12px;\n",
        "    padding: 20px;\n",
        "    background-color: #fcfcfc;\n",
        "    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .section-divider {\n",
        "        border-top: 2px solid #3b82f6;\n",
        "        margin: 15px 0;\n",
        "        opacity: 0.5;\n",
        "    }\n",
        "\n",
        "    .vertical-divider {\n",
        "    border-right: 2px solid #e0e0e0;\n",
        "    height: 90vh; /* Fills most of the vertical screen */\n",
        "    margin: 0 20px;\n",
        "    align-self: center;\n",
        "}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def create_interface():\n",
        "    # Load custom CSS for the 'Obsidian' black-and-teal theme\n",
        "    with gr.Blocks(title=\"AI-Powered Document Intelligence Automation Platform\") as demo:\n",
        "      # --- 1. HEADER SECTION(CENTERED) ---\n",
        "      with gr.Column(elem_classes=\"welcome-text-header-container\"):\n",
        "          gr.Markdown(\"# ü§ñ AI-Powered Document Intelligence Automation Platform\")\n",
        "          gr.Markdown(\"### Providing assistance with document search. ‚ú®\")\n",
        "          gr.Markdown(\"üìÇ Upload & Process Multi-page PDF or Scanned Image and then enter search request in chatbot\")\n",
        "          gr.Markdown(\"Accepted fiile formats: .pdf, .png, .jpg, .jpeg\")\n",
        "          gr.HTML(\"<hr style='border: 1px solid #e0e0e0;'>\")\n",
        "\n",
        "      # --- 2. THREE-PILLAR NAVIGATION TABS ---\n",
        "      with gr.Tabs() as tabs:\n",
        "# -------- --- TAB 1: OPERATIONS (CHATTING & VIEWING) ---\n",
        "          with gr.TabItem(\"üí¨ Chat Operations\", id=\"chat_tab\"):\n",
        "             with gr.Row():\n",
        " # -----------     --------   # TAB 1: OPERATIONAL CORE (LEFT TOP COLUMN: UI IMAGE)\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Image( # AI-Powered Document Assistant logo v2.png\n",
        "                        value=LOGO_PATH,\n",
        "                        width=100,\n",
        "                        show_label=False,\n",
        "                        container=False,\n",
        "                        scale=1)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        " # -----------     --------    # TAB 1 - LEFT COLUMN: LARGE LANGUAGE MODEL (LLM) SELECTION\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"# üß† Large Language Models (LLM)\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      # Status indicator\n",
        "                      engine_status = gr.Markdown(\"*Status: Ready*\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      # Create the choice button\n",
        "                      llm_selector = gr.Dropdown(\n",
        "                          choices=[\"Gemini 2.0\", \"Mistral 7B\", \"Phi-2\"],\n",
        "                          label=\"Select LLM Engine\",\n",
        "                          value=\"Gemini 2.0\",\n",
        "                          scale=1,\n",
        "                          container=False)\n",
        "                      clear_on_switch_checkbox = gr.Checkbox(label=\"Clear History on Switch\")\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "\n",
        "                    # DOCUMENT PROCESSING CENTRAL\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"## üìÇ Document Processing Central\")\n",
        "\n",
        "                    # FILE_UPLOAD, INGEST_BTN (PROCESS DOCUMENT), CLEAR_ALL_BTN, DOC_TYPE_FILTER, STATUS_OUTPUT\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"Upload file(s) and press Process Document button\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      file_upload = gr.File(\n",
        "                        label=\"Upload Multi-page PDF or Scanned Image\",\n",
        "                        file_count=\"multiple\", #Enable muliple\n",
        "                        file_types=[\".pdf\", \".png\", \".jpg\", \".jpeg\"],\n",
        "                        interactive=True,\n",
        "                        type=\"filepath\")\n",
        "\n",
        "\n",
        "                    with gr.Row():\n",
        "                      ingest_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\", interactive=True, scale=1 )\n",
        "                      clear_all_btn = gr.Button(\"üóëÔ∏è Clear All\", variant=\"primary\", interactive=True, scale=1)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # PROCESSING STATUS & METADATA\n",
        "                    with gr.Row():\n",
        "                      # This shows the bullet points of document pages created in 'structure_display' string in def 'process_pdf_handler'.\n",
        "                      status_output = gr.Textbox(\n",
        "                          label=\"Processing Status & Metadata\",\n",
        "                          lines=15, # Increased height\n",
        "                          elem_classes=\"status-window\",\n",
        "                          interactive=False,\n",
        "                          placeholder=\"Technical details will appear here after upload...\",\n",
        "                          visible=True,\n",
        "                          elem_id=\"status-box\")   # ID for custom styling\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # VIEW DOCUMENT\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"# üìÑ Document Preview\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      # View the PDF pages as images in the UI\n",
        "                      # Fixed height viewer prevents layout shifts\n",
        "                      doc_viewer = gr.Image(\n",
        "                              label=\"Page Viewer\",\n",
        "                              type=\"pil\",\n",
        "                              interactive=False,\n",
        "                              height=550)\n",
        "\n",
        "                    with gr.Row():\n",
        "                      prev_btn = gr.Button(\"‚¨ÖÔ∏è Previous\", scale=1)\n",
        "                      # Indicator shows: Page 1 of 10\n",
        "                      next_btn = gr.Button(\"Next ‚û°Ô∏è\", scale=1)\n",
        "\n",
        "                    with gr.Row():\n",
        "                      page_indicator = gr.Markdown(\"## <center>Page 0 of 0</center>\")\n",
        "\n",
        "                      op_status_bar = gr.Markdown(\"**Status:** Ready\")\n",
        "                      # Hidden State to store the PDF pages and current index\n",
        "                      viewer_state = gr.State({\"current_page\": 0, \"images\": []})\n",
        "                      filename_debug_output = gr.Textbox(label=\"Uploaded Filename (Debug)\", visible=False, lines=1, interactive=False) # ADDED DEBUG TEXTBOX\n",
        "\n",
        "\n",
        "\n",
        "# -----------     --------   # TAB 1 - RIGHT COLUMN: AI-POWERED DOCUMENT INTELLIGENCE CHATBOT INTERFACE\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Markdown(\"## AI-Powered Document Intelligence Chatbot\")\n",
        "\n",
        "                    # Chatbot Design\n",
        "                    chatbot = gr.Chatbot(\n",
        "                      label=\"AI Document Assistant\",\n",
        "                      height=1000,\n",
        "                      show_label=True,\n",
        "                      value=[{\"role\": \"assistant\", \"content\": \"**ü§ñ AI Document Assistant:** üëã Welcome! Upload files in the üìÇ Upload & Process Documents tab to begin. üöÄ\"}],\n",
        "                      elem_id=\"chatbot-box\",\n",
        "                      autoscroll=True,\n",
        "                      render_markdown=True) # Processes the **bold** text\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        msg_input = gr.Textbox(show_label=False, placeholder=\"Ask a question about your docs...\", scale=8, container=True)\n",
        "\n",
        "                    with gr.Row():\n",
        "                        send_btn = gr.Button(\"üöÄSend\", scale=1, variant=\"primary\", interactive=True)\n",
        "                        chat_download_btn = gr.DownloadButton(\n",
        "                               \"üì§ Download Chat History (PDF)\", # Button the user clicks to start the export\n",
        "                               visible=True,\n",
        "                               interactive=True,\n",
        "                               elem_id=\"chat-export-btn\",\n",
        "                               variant=\"primary\",\n",
        "                               scale=1)\n",
        "\n",
        "                        # This component holds the actual file once generated\n",
        "                        # Visible=False until the file is ready\n",
        "                        chat_download_file = gr.File(label=\"Download Ready\", visible=False, scale=1)\n",
        "\n",
        "                    with gr.Row():\n",
        "                        example_btn1 = gr.Button(\"üìù Summary\", variant=\"primary\", interactive=True, scale=1)\n",
        "                        example_btn2 = gr.Button(\"üí∞ Find Amounts\", variant=\"primary\", interactive=True, scale=1)\n",
        "                        clear_chat_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"primary\", interactive=True, scale=1)\n",
        "\n",
        "# -----------  # --- TAB 2: RAG CONFIGURATIONS & DOCUMENT(s) FILTERS ---\n",
        "          with gr.TabItem(\"‚öôÔ∏è Configurations & üìÇ Filters\", id=\"Config_filters_tab\"):\n",
        "             with gr.Row():\n",
        "\n",
        "\n",
        " # -----------     --------   # TAB 2 - LEFT COLUMN: DOCUMENT STRUCTURE\n",
        "                with gr.Column(scale=2):\n",
        "                    # DOCUMENT STRUCTURE\n",
        "                    gr.Markdown(\"# üß¨ Processed Document Breakdown\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                          gr.Markdown(\"\"\"\n",
        "                            These view displays the breakdown of your file. Our system identifies\n",
        "                            identifies distinct sub-documents types (e.g., an Invoice followed by a Lease)\n",
        "                            within a single upload, mapping the specific page ranges and initial content previews\n",
        "                            to ensure the retriever (search) knows exactly where each piece of information originated.\n",
        "                            \"\"\")\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # Human-Readable Text output\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"### Document Structure\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"Identifies distinct sub-documents and page ranges within your file.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        structure_output_textbox = gr.Textbox(label=\"Text Output\", visible=True, scale=3, lines=8)\n",
        "\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "                    # Developer JSON output\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"### Developer Document Structure\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"Machine-ready schema for debugging and database integration.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                         # This shows the actual raw JSON data \"ADDED code\" in def 'process_pdf_handler'.\n",
        "                        structure_output_code = gr.Code(label=\"JSON Output\", language=\"json\", lines=25, interactive=False, elem_id=\"structure-json-box\")\n",
        "\n",
        "\n",
        " # -----------     --------   # TAB 2 - RIGHT COLUMN: FILTERS  & RAG CONFIGURATIONS\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Image( # Document Filter and RAG.png\n",
        "                        value=CONFIG_FILTER_PATH,\n",
        "                        show_label=False,\n",
        "                        container=False,\n",
        "                        scale=1)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # FILTER DOCUMENTS\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"# Filters\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"Filter AI Document Assistant responses or document preview by File or Document Type.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        view_selector = gr.Dropdown(label=\"Select File to View\", choices=[],scale=2)\n",
        "                    with gr.Row():\n",
        "                        doc_type_filter = gr.Dropdown(\n",
        "                            choices=[\"All\"]+ VALID_DOC_TYPES, # Automatically populates from DOCUMENT_TAXONOMY\n",
        "                            label=\"Filter By Document (File) & Type:\",\n",
        "                            value=\"All\",\n",
        "                            interactive=True,\n",
        "                            multiselect=True,\n",
        "                            scale=2)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    with gr.Row():\n",
        "                        # RIGHT COLUMN: RAG CONFIGURATION ROW\n",
        "                        gr.Markdown(\"# ‚öôÔ∏è RAG Configuration\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"To refine AI Document Assistant response, adjust Recall Chunks.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        auto_route = gr.Checkbox(value=True, label=\"üéØ Auto-Route Queries\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        audit_num_chunks = gr.Slider(\n",
        "                                  minimum=1,\n",
        "                                  maximum=10,\n",
        "                                  value=4,\n",
        "                                  step=1,\n",
        "                                  label=\"üìä Recall Chunks\",\n",
        "                                  info=\"Determines how many chunks are analyzed for precision.\")\n",
        "\n",
        "\n",
        "# -----------    # --- TAB 3: AUDIT & GROUND TRUTH ---\n",
        "          with gr.TabItem(\"‚öñÔ∏è Performance Audit\", id=\"audit_tab\"):\n",
        "\n",
        "              #Blank row for Spacing\n",
        "              with gr.Row():\n",
        "\n",
        "\n",
        "                  # LEFT COLUMN: SECTOR FILTER PERFORMANCE AUDIT\n",
        "                  with gr.Column(scale=1):\n",
        "                      gr.Markdown(\"### ‚öôÔ∏è PERFORMANCE AUDIT CONFIGURATIONS\")\n",
        "\n",
        "                      with gr.Row():\n",
        "                        sector_dropdown = gr.Dropdown(\n",
        "                        choices=[\"Real Estate\", \"Healthcare\", \"Legal\", \"All\"], # Can be optimized in future enhancements for other domains\n",
        "                        label=\"Select Performance Audit Sector\",\n",
        "                        value=\"All\",\n",
        "                        interactive=True,\n",
        "                        visible=True\n",
        "                      )\n",
        "\n",
        "                      # Left-Middle-Top: GRAPHS\n",
        "                      with gr.Row():\n",
        "                        run_audit_btn = gr.Button(\"üèÅ Run Performance Audit\", variant=\"primary\", scale=1)\n",
        "                        audit_download_btn = gr.DownloadButton(\"üìÑ Download Performance Audit Report (PDF)\", variant=\"primary\", scale=1)\n",
        "                        audit_download_file = gr.File(label=\"üìÑ Download Performance Audit Report (PDF)\", visible=False, interactive=False, container=True)\n",
        "\n",
        "                  # RIGHT COLUMN: VISUALIZATION PERFORMANCE AUDIT METRICS\n",
        "                  with gr.Column(scale=6):\n",
        "                        gr.Markdown(\"# ‚öñÔ∏è Performance Audit\")\n",
        "\n",
        "                        # HEADING\n",
        "                        with gr.Column(scale=6):\n",
        "                            gr.Markdown(\"### ‚öôÔ∏è MONITORING & PERFORMANCE DASHBOARD\")\n",
        "                            gr.Markdown(\"### üõ†Ô∏è Industry Ground Truth Evaluation\")\n",
        "                            gr.Markdown(\"This dashboard translates raw AI-Judge scores into Permonace Audit status.\")\n",
        "                            gr.Markdown(\"--------------------------------------------------------------------------------\")\n",
        "                            gr.Markdown(\"### üìä Live Performance\")\n",
        "                            gr.Markdown(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "                        # RIGHT COLUMN: VISUALIZATION PERFORMANCE AUDIT METRICS\n",
        "                        with gr.Column(scale=6):\n",
        "\n",
        "                          # Right-Top: LIVE PERFORMANCE\n",
        "                          with gr.Row():\n",
        "                            latency_stat = gr.Markdown(\"**Avg Latency:** -- | **Speed:** --\")\n",
        "                            density_perc = gr.Label(label=\"Context Density\")\n",
        "                          with gr.Row():\n",
        "                            audit_accuracy_gauge = gr.Label(label=\"RAG Triad & Context Metrics\")\n",
        "                            accuracy_gauge = gr.Label(label=\"Context Density Score\")\n",
        "                            bottleneck_plot = gr.Image(label=\"Latency vs. Token Speed\")\n",
        "\n",
        "                          # Right-Middle-Top: AUDIT TABLE\n",
        "                          with gr.Row():\n",
        "                            audit_table = gr.Dataframe(\n",
        "                              headers=[\"Metric\", \"Current Audit\", \"Industry Benchmark\", \"Status\"],\n",
        "                              value=[]\n",
        "                          )\n",
        "\n",
        "\n",
        "          # Bottom: GLOBAL STATUS BAR (Visible across all tabs)\n",
        "          op_status_bar = gr.Markdown(\n",
        "                value=\"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\",\n",
        "                elem_id=\"op_status_bar\"\n",
        "          )\n",
        "\n",
        "# ----------------------------------------------- COMPONENTS WIRING (Defined with chat_interface) ------------------------------------------------------------------------------------- #\n",
        "\n",
        "      # Chat Event handlers\n",
        "      def update_status_bar():\n",
        "            \"\"\"Update the status bar with current statistics.\"\"\"\n",
        "            if doc_store.is_ready:\n",
        "                stats = doc_store.processing_stats\n",
        "                cache_rate = 0\n",
        "                if hasattr(doc_store.retriever, 'total_queries') and doc_store.retriever.total_queries > 0:\n",
        "                    cache_rate = (doc_store.retriever.cache_hits / doc_store.retriever.total_queries) * 100\n",
        "\n",
        "                return f\"**Status:** ‚úÖ Ready | **Documents:** {stats.get('documents_found', 0)} | **Chunks:** {stats.get('total_chunks', 0)} | **Cache Rate:** {cache_rate:.0f}%\"\n",
        "            return \"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\"\n",
        "\n",
        "\n",
        "\n",
        "      def clear_all():\n",
        "          \"\"\"Clear everything and reset the interface.\"\"\"\n",
        "          global doc_store, audit_logs\n",
        "          doc_store = EnhancedDocumentStore()\n",
        "          audit_logs = []\n",
        "\n",
        "          # Return 14 values to match your specific UI layout\n",
        "          return (\n",
        "              [],                                 # 1. chatbot\n",
        "              None,                               # 2. file_upload\n",
        "              \"\",                                 # 3. chat_input\n",
        "              None,                               # 4. doc_viewer (Now gr.Image, so return None)\n",
        "              \"\",                                 # 5. structure_output_textbox\n",
        "              \"\",                                 # 6. structure_output_code\n",
        "              \"\",                                 # 7. extra status\n",
        "              gr.update(choices=[], value=None),  # 8. doc_type_filter\n",
        "              gr.update(choices=[], value=None),  # 9. view_selector\n",
        "              pd.DataFrame(),                     # 10. audit_table\n",
        "              None,                               # 11. audit_download_btn\n",
        "              \"üîÑ System Reset\",                  # 12. op_status_bar\n",
        "              {\"current_page\": 0, \"images\": []}, # 13. viewer_state (Reset State)\n",
        "              \"**Page 0 of 0**\"                  # 14. page_indicator (Reset Markdown)\n",
        "          )\n",
        "\n",
        "\n",
        "      def process_pdf_with_status(file_list):\n",
        "            \"\"\"Processes uploaded file and ensures UI doesn't hang on error.\"\"\"\n",
        "            try:\n",
        "                # Calls your existing handler from Section 11;\n",
        "                status, structure_json_string, structure_display, doc_type_filter, view_selector, filename_summary = process_pdf_handler(file_list)\n",
        "\n",
        "                # UI Gloabl Status Bar\n",
        "                status_bar_text = update_status_bar()\n",
        "\n",
        "                return status, structure_json_string, structure_display, view_selector, doc_type_filter, status_bar_text, filename_summary\n",
        "\n",
        "                return (\n",
        "                    status,                     # -> status_output\n",
        "                    structure_json_string,      # -> structure_output_code\n",
        "                    structure_display,          # -> structure_output_textbox\n",
        "                    gr.update(choices=filter_choices, value=\"All\"), # For search filter\n",
        "                    gr.update(choices=file_paths, value=file_paths[0] if file_paths else None), # For viewer\n",
        "                    view_selector,\n",
        "                    f\"‚úÖ {len(all_filenames)} Files Ready\"\n",
        "                )\n",
        "\n",
        "            except Exception as e:\n",
        "                # Debugging print to see exactly what happened in Colab logs\n",
        "                print(f\"Error in wrapper: {str(e)}\")\n",
        "                return f\"‚ùå System Error: {str(e)}\",\"[]\", \"‚ö†Ô∏è Error\", gr.update(choices=[\"All\"]), \"Error\", gr.update(choices=[])\n",
        "\n",
        "\n",
        "      # UI Buttons: SUMMARY, FIND AMOUNTS,CLEAR CHAT\n",
        "      # Define Example question handlers\n",
        "      def ask_summary(history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "          \"\"\"Specific wrapper for the Summary button with deep retrieval.\"\"\"\n",
        "\n",
        "          # 1. Cleaner, Goal-Oriented Prompt\n",
        "          msg = (\n",
        "              \"Provide a high-level executive summary of the document. \"\n",
        "              \"Highlight the primary purpose, key stakeholders, and critical deadlines. \"\n",
        "              \"Do not repeat these instructions in your response.\"\n",
        "          )\n",
        "\n",
        "          if history is None: history = []\n",
        "          final_history = history\n",
        "\n",
        "          # 2. FORCE HIGHER K-VALUE: Summaries need more context than a single question.\n",
        "          # We use max(10, audit_num_chunks) to ensure it's at least 10 chunks.\n",
        "          summary_k = max(10, int(audit_num_chunks) if audit_num_chunks else 10)\n",
        "\n",
        "          for updated_history, status in chat_with_status(\n",
        "              msg, history, doc_type_filter, auto_route, summary_k\n",
        "          ):\n",
        "              final_history = updated_history\n",
        "\n",
        "          return final_history\n",
        "\n",
        "\n",
        "      def ask_amounts(history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "          \"\"\"Specific wrapper for finding financial data.\"\"\"\n",
        "\n",
        "          # 1. Simplified Message (Easier for Gemini to process)\n",
        "          msg = \"Identify and list all numerical dollar amounts, fees, and financial figures found in the text.\"\n",
        "\n",
        "          if history is None: history = []\n",
        "          final_history = history\n",
        "\n",
        "          # 2. DEEP SEARCH: Financials are often buried in late-page exhibits.\n",
        "          amounts_k = max(12, int(audit_num_chunks) if audit_num_chunks else 12)\n",
        "\n",
        "          for updated_history, status in chat_with_status(\n",
        "              msg, history, doc_type_filter, auto_route, amounts_k\n",
        "          ):\n",
        "              final_history = updated_history\n",
        "\n",
        "          return final_history\n",
        "\n",
        "\n",
        "\n",
        "      # --- EVENT WIRING ---\n",
        "\n",
        "      # üîó 1. LLM Selector\n",
        "      # Connect the selector to your handle_model_Transition (UI Manager) function\n",
        "      llm_selector.change(\n",
        "          fn=handle_model_transition,\n",
        "          inputs=[llm_selector, chatbot, clear_on_switch_checkbox],\n",
        "          outputs=[chatbot, engine_status]\n",
        "      )\n",
        "\n",
        "\n",
        "      # üîó 2. Processing Events\n",
        "\n",
        "      # A. File Upload. Ensures the loading state for uploading a file is properly cleared\n",
        "      #    File Preview (2 Outputs)\n",
        "      # When files are picked, show the first one in the viewer and names in the debug box\n",
        "      file_upload.change(\n",
        "            fn=lambda x: (x[0].name if x else None, f\"üìë {len(x)} files selected\" if x else \"No files\"),\n",
        "            inputs=[file_upload],\n",
        "            outputs=[doc_viewer, filename_debug_output]\n",
        "        )\n",
        "\n",
        "      # B. \"View File\" dropdown to switch which PDF is showing in the doc_viewer\n",
        "      view_selector.change(\n",
        "         fn=load_pdf_into_viewer,\n",
        "        inputs=[view_selector],\n",
        "        outputs=[doc_viewer, viewer_state, page_indicator]\n",
        "      )\n",
        "\n",
        "\n",
        "      # C. Document Processing (Backend Ingestion)\n",
        "      ingest_btn.click(\n",
        "            fn=process_pdf_handler,\n",
        "            inputs=[file_upload], # Pull from the actual uploaded file\n",
        "            outputs=[\n",
        "                status_output,              # Receives status_msg\n",
        "                structure_output_code,      # Receives structure_json_string\n",
        "                structure_output_textbox,   # Receives structure_display (Bulleted String)\n",
        "                doc_type_filter,               # Receives filter update - doc_type_filter (Dropdown - Filter Document Type)\n",
        "                view_selector,\n",
        "                op_status_bar]              # Receives status bar update - status_bar_text (String - Global Status Indicator)\n",
        "        )\n",
        "\n",
        "\n",
        "      # D. Document Viewer Navigation: Previous Button\n",
        "      prev_btn.click(\n",
        "          fn=flip_page,\n",
        "          inputs=[gr.State(\"prev\"), viewer_state],\n",
        "          outputs=[doc_viewer, viewer_state, page_indicator]\n",
        "      )\n",
        "\n",
        "      # E. Document Viewer Navigation: Next Button\n",
        "      next_btn.click(\n",
        "          fn=flip_page,\n",
        "          inputs=[gr.State(\"next\"), viewer_state],\n",
        "           outputs=[doc_viewer, viewer_state, page_indicator]\n",
        "      )\n",
        "\n",
        "\n",
        "      # üîó 3. Chat Texbox (Message) Input & Send\n",
        "\n",
        "      # A. Chat Functionality (The \"Conversation\" bridge)\n",
        "      # Use .then() to clear the input box after sending\n",
        "      msg_input.submit(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "            outputs=[chatbot, op_status_bar]\n",
        "      ).then(lambda: \"\", None, [msg_input]) # Only clear the input\n",
        "\n",
        "\n",
        "      # B. Chat Functionality (The \"Conversation\" bridge)\n",
        "      # Use .then() to clear the input box after sending\n",
        "      send_btn.click(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "            outputs=[chatbot, op_status_bar]\n",
        "      ).then(lambda: \"\", None, [msg_input])\n",
        "\n",
        "\n",
        "\n",
        "      # üîó 4. Download Chat History & Performance Audit Events\n",
        "\n",
        "      # A. When the button is clicked:\n",
        "      # -----Take 'chatbot' as input, run 'export_chat_history_to_pdf',\n",
        "      # -----and send the result to 'chat_download_file'\n",
        "      chat_download_btn.click(\n",
        "          fn=export_chat_history_to_pdf,\n",
        "          inputs=[chatbot],\n",
        "          outputs=[chat_download_btn]\n",
        "      )\n",
        "\n",
        "      # B. Run Performance Audit\n",
        "      run_audit_btn.click(\n",
        "          fn=run_performance_audit,\n",
        "          inputs=[sector_dropdown, audit_num_chunks],\n",
        "          outputs=[\n",
        "              latency_stat,          # 1. (Markdown) -> f\"**Avg Latency:**...\"\n",
        "              audit_accuracy_gauge,  # 2. (Label/Plot) -> {\"Faithfulness\": ...}\n",
        "              accuracy_gauge,        # 3. (Label) -> f\"{success_rate}%\"\n",
        "              density_perc,          # 4. (Label) -> f\"{context_density}%\"\n",
        "              bottleneck_plot,       # 5. (Image) -> \"bottlenecks.png\"\n",
        "              audit_table            # 6. (Dataframe) -> audit_table_data\n",
        "          ]\n",
        "      )\n",
        "\n",
        "\n",
        "      # B. Export & Download Performance Audit\n",
        "      audit_download_btn.click(\n",
        "          fn=handle_audit_export_ui,\n",
        "          inputs=[audit_table],\n",
        "          outputs=[\n",
        "              audit_download_btn,  # Receives the file update\n",
        "              op_status_bar         # Receives the status message (the \"‚úÖ Audit report...\" text)\n",
        "          ]\n",
        "      )\n",
        "\n",
        "\n",
        "      # üîó 5. UI Utility Events\n",
        "\n",
        "      # A. Utility/Reset Buttons: Clear ALL (Start new)\n",
        "      # Clear the entire platform\n",
        "      clear_all_btn.click(\n",
        "            fn=clear_all,\n",
        "            inputs=[],\n",
        "            outputs=[\n",
        "                chatbot,                  # 1\n",
        "                file_upload,               # 2\n",
        "                status_output,            # 3\n",
        "                doc_viewer,               # 4\n",
        "                filename_debug_output,     # 5\n",
        "                structure_output_code,    # 6\n",
        "                structure_output_textbox, # 7\n",
        "                doc_type_filter,           # 8\n",
        "                view_selector,            # 9\n",
        "                audit_table,              # 10\n",
        "                audit_download_file,       # 11\n",
        "                op_status_bar,            # 12\n",
        "                viewer_state,             # 13\n",
        "                page_indicator            # 14\n",
        "            ]\n",
        "      )\n",
        "\n",
        "      # B. Utility/Reset Buttons: Clear Chat History\n",
        "      clear_chat_btn.click(\n",
        "          fn=lambda: (\n",
        "              [{\"role\": \"assistant\", \"content\": \"**ü§ñ AI Document Assistant:** üëã Chat cleared. How can I help you with your documents? üöÄ\"}],\n",
        "              gr.update(visible=False)\n",
        "          ),\n",
        "          inputs=None,\n",
        "          outputs=[chatbot, chat_download_file]\n",
        "      )\n",
        "\n",
        "      # C. Summary Button Wiring\n",
        "      example_btn1.click(\n",
        "          fn=ask_summary,\n",
        "          inputs=[chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "          outputs=[chatbot]\n",
        "      )\n",
        "\n",
        "      # D. Find Amounts Button Wiring\n",
        "      example_btn2.click(\n",
        "          fn=ask_amounts,\n",
        "          inputs=[chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "          outputs=[chatbot]\n",
        "      )\n",
        "\n",
        "      return demo\n",
        "\n",
        "      # üîó 6. ADDED - Initialize JavaScript for Auto-Scroll\n",
        "      demo.load(js=scroll_script)\n",
        "\n",
        "print(\"‚úÖ SECTION 12B. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC Complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgDAO0pTcury"
      },
      "source": [
        "# **SECTION 13. APPLICATION LAUNCHER**\n",
        "\n",
        "This section is the **Runtime Orchestrator**. Its primary responsibility is to manage the transition between development and production environments.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "1. **Port Management:** `gr.close_all()` is a critical \"defensive\" coding practice for notebook environments (like Google Colab). It prevents the common \"Address already in use\" error by forcefully closing previous sessions before starting a new one.\n",
        "\n",
        "2. **Theme Merging::** IIt combines the `gr.themes.Soft()` base (which provides modern typography and spacing) with your `custom_css` (the \"Obsidian\" black-and-teal skin). This ensures the UI is both structurally sound and aesthetically branded.\n",
        "\n",
        "3. **TThe Connectivity Bridge:**\n",
        "  - `debug=True:` This allows you to see real-time Python errors in the notebook console while the app is running, which is essential for troubleshooting RAG retrieval issues.\n",
        "\n",
        "  - `share=True:` This is the most powerful feature of the launcher; it creates a temporary **Gradio Live Link** (e.g.,` https://xyz123.gradio.live`). This allows stakeholders to test the platform on their own devices without needing to install Python or the local models.\n",
        "\n",
        "  <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7AtFI83Ouno"
      },
      "outputs": [],
      "source": [
        "# ------- SECTION 13. APPLICATION LAUNCHER -------\n",
        "\n",
        "# 1. Cleanup: Close any existing Gradio servers to free up ports\n",
        "gr.close_all()\n",
        "\n",
        "print(\"üöÄ Initializing Platform Components...\")\n",
        "print(\"üìÇ Loading Vector Store...\")\n",
        "print(\"üß† Connecting LLM Engine...\")\n",
        "\n",
        "\n",
        "# 2. Build the Interface\n",
        "# Calls the create_interface() function defined in Section 12\n",
        "demo = create_interface()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ AI-Powered Document Intelligence Automation Platform Launching...\")\n",
        "\n",
        "    demo.launch(\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=custom_css,\n",
        "        debug=True,\n",
        "        share=True,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNtIBO9geNu18Mkr2Sq+YI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}