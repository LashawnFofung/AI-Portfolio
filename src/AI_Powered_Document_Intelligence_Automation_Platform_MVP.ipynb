{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOF+DJquylZ8U4Z++E19I7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LashawnFofung/AI-Portfolio/blob/main/src/AI_Powered_Document_Intelligence_Automation_Platform_MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI-Powered Document Intelligence Automation Platform MVP**\n",
        "\n",
        "This notebook serves as a production-ready MVP for a Multi-Modal Document Intelligence system. It is designed to solve complex business problems associated with high-volume document reviews, such as mortgage processing, legal discovery, and HR automation. Unlike standard RAG (Retrieval-Augmented Generation) systems that often suffer from \"Context Contamination,\" this platform utilizes Intelligent Document Boundary Detection and Metadata-Rich Chunking to ensure that queries are routed to the precise document context required.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üõ†Ô∏è MVP Objectives**\n",
        "- **Contextual Fidelity:** Eliminate \"hallucinations\" caused by mixing data from different documents within the same vector space.\n",
        "\n",
        "- **Intelligent Automation:** Automate the classification and segmentation of bulk-uploaded files (e.g., a single PDF containing a resume, an ID, and a contract).\n",
        "\n",
        "- **Multi-Model Versatility:** Allow users to switch between different LLM engines (Gemini, Mistral, Phi-2) based on performance and latency needs.\n",
        "\n",
        "- **Production Readiness:** Integrate OCR (Tesseract), vector search (FAISS), and professional PDF report generation into a unified workflow.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üèóÔ∏è Key Technical Architecture**\n",
        "The platform is built on a modular pipeline that ensures data integrity from ingestion to output:\n",
        "\n",
        "- **Ingestion Layer:** Supports PDF and Image files with Hybrid OCR (PyMuPDF + Tesseract) to handle scanned documents.\n",
        "- **Intelligence Layer:** Uses LLM-driven classification to identify document types and detect logical boundaries between pages.\n",
        "- **Storage Layer:** Implements Metadata-Rich Chunking using LlamaIndex and FAISS, creating segregated vector indices for each document type to prevent cross-contamination.\n",
        "- **Orchestration Layer:** A central router predicts the most likely document type for a given query and routes the request to the specific vector index.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üåü Core MVP Capabilities**\n",
        "- **Multi-Modal Routing:** Automatically detects if a query is about an \"Invoice\" vs. a \"Mortgage Contract\" and searches only the relevant documents.\n",
        "- **Logical Document Segmentation:** Breaks down large, combined PDF files into individual logical units (e.g., separating Page 1-3 as a \"Resume\" and Page 4 as an \"ID\").\n",
        "- **Performance Auditing:** Tracks ROUGE scores and response times to evaluate the quality of the AI's answers.\n",
        "- **Interactive UI:** A Gradio-based interface featuring a PDF viewer, audit logs, and a model-switching dashboard.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **üìñ How to Operate**\n",
        "- **Environment Setup:** Run Section 1 and 2 to install dependencies and configure your Gemini API Key in the Colab Secrets.\n",
        "- **Initialization:** Run the global configuration cells to load the BGE embedding model and initialize the default LLM.\n",
        "- **Select Engine:** Use the switch_llm function or the UI dropdown to select your preferred AI model (e.g., \"Gemini 2.0 Flash\").\n",
        "- **Upload & Process:** Upload your documents via the Gradio interface. The system will automatically classify and index them.\n",
        "- **Query & Audit:** Enter your business questions. Use the \"Audit Log\" tab to view performance metrics and download a professional PDF summary of the session.\n",
        "\n",
        "<br><br>\n",
        "## **üîç Section Logic & Flow Analysis**\n",
        "\n",
        "- **1-2: Setup & Config**\n",
        "  - Establishes the environment, mounts Google Drive, and initializes global state variables (audit_logs, current_llm).\n",
        "      - [Section 1: Setup And Installation](#scrollTo=OnjSSFKJmRQc)\n",
        "      - [Section 2A: Core Imports, Security Keys, And Global Settings Configurations](#scrollTo=1TlPkZLbvExS)\n",
        "      - [Section 2B: LLM Factory & Resource Configuration](#scrollTo=1MYP_DZAw6QJ)\n",
        "\n",
        "- **3\tData Structures**\n",
        "  - Defines PageInfo, LogicalDocument, and ChunkMetadata dataclasses to maintain data schema consistency across the pipeline.\n",
        "      - [Section 3: Data Structures For Enhanced Document Management](#scrollTo=RLrZv_r30NBc)\n",
        "\n",
        "- **4-5\tIngestion & OCR**\n",
        "  - The \"Aware Router\" (extract_and_analyze_file) directs files to PDF or Image processors. Logic includes LLM-based boundary detection.\n",
        "      - [Section 4: Document Intelligence Functions](#scrollTo=xQNJUFwB2gMZ)\n",
        "      - [Section 5: Advanced PDF Processing Pipeline](#scrollTo=yGrQwXDp4gh_)\n",
        "\n",
        "- **6\tIntelligent Chunking**\n",
        "  - Converts logical documents into overlapping segments while embedding rich metadata (page numbers, doc IDs) into every chunk.\n",
        "      - [Section 6: Intelligent Chunking With Metadata Preservation](#scrollTo=qYroZgDG9FxX)\n",
        "\n",
        "- **7-8\tVector Search**\n",
        "  - Builds segregated FAISS indices. The IntelligentRetriever applies query routing to search only the relevant document \"silo\".\n",
        "      - [Section 7: Query Routing And Intelligent Retrieval](#scrollTo=d_MsgfAaAGsm)\n",
        "      - [Section 8: Enhanced Answer Generation With Source Attribution](#scrollTo=VrLmw0oyEGfq)\n",
        "\n",
        "- **9-10\tOrchestrator**\n",
        "  - The \"Brain\" of the platform. It handles the full RAG cycle: Query -> Route -> Retrieve -> Generate -> Audit.\n",
        "      - [Section 9: Enhanced Document Store](#scrollTo=mJTVS2CcHBq3)\n",
        "      - [Section 10: Backend Chat & Audit Loogic](#scrollTo=txaCA0n9MNYN)\n",
        "\n",
        "- **11-12\tUI & Reporting**\n",
        "  - The Gradio interface layer and the ReportLab logic for generating production-ready audit reports.\n",
        "      - [Section 11:Chatbot Logic & Orchestration](#scrollTo=80rvI0tlTlEu)\n",
        "      - [Section 12: Gradio Interface, Chat Handlers, & Wiring Logic](#scrollTo=2pYWo7wCa47E)\n",
        "\n",
        "- **13 [Application Launcher](#scrollTo=GgDAO0pTcury)**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uf8S474r_kYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 1. SETUP AND INSTALLATION**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "This section serves as the Foundation Layer of the AI-Powered Document Intelligence Platform. The logic follows a linear, non-destructive sequence:\n",
        "1. **Dependency Provisioning:** Installs the multi-modal stack required for the MVP. This includes UI components (`Gradio`), document parsing (`PyMuPDF`, `LlamaIndex`), OCR engines (`Tesseract`), and the vector search backend (FAISS).\n",
        "2. **Global State Initialization:** Sets up persistent tracking variables (`audit_logs` and `current_llm`). This is a critical design choice for the MVP, as it allows for performance metrics to persist across multiple document uploads and ensures the system knows which LLM engine is currently \"warm\" in memory.\n",
        "3. **Asynchronous Handling:** Inspects the event loop to prevent \"loop already running\" errors common in Jupyter environments when initializing asynchronous RAG pipelines.\n",
        "4. **Resource Mounting:** Links Google Drive to ensure the UI has access to static assets (logos/branding) and persistent storage for output reports.\n"
      ],
      "metadata": {
        "id": "OnjSSFKJmRQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 1. SETUP AND INSTALLATION -------\n",
        "\n",
        "\n",
        "# 1.1 UI, PDF Processing & Machine Learning Foundations\n",
        "# Grouping core utilities for document extraction and interface building\n",
        "!pip install -q \\\n",
        "    gradio gradio_pdf \\\n",
        "    pypdf PyPDF2 pymupdf \\\n",
        "    pillow \\\n",
        "    sentence-transformers transformers \\\n",
        "    faiss-cpu \\\n",
        "    google-generativeai \\\n",
        "    numpy pandas jedi\\\n",
        "    json-repair\n",
        "\n",
        "# # 1.2 LlamaIndex Orchestration Stack\n",
        "# Specifically for RAG (Retrieval-Augmented Generation) and metadata management\n",
        "!pip install -q \\\n",
        "    llama-index \\\n",
        "    llama-index-readers-file \\\n",
        "    llama-index-vector-stores-faiss\n",
        "\n",
        "#1.3 LLM Engine Support (Multi-Modal Switching)\n",
        "# Libraries required to swap between API-based (Gemini) and Local (HuggingFace) models\n",
        "!pip install -q \\\n",
        "    llama-index-llms-google-genai \\\n",
        "    llama-index-llms-huggingface \\\n",
        "    llama-index-embeddings-huggingface \\\n",
        "    transformers accelerate bitsandbytes\n",
        "\n",
        "# 1.4 OCR & Specialized Reporting Tools\n",
        "# Tesseract for scanned docs; ReportLab for automated PDF performance summaries\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install -q \\\n",
        "    pytesseract \\\n",
        "    reportlab rouge-score \\\n",
        "    matplotlib seaborn\n",
        "\n",
        "# --- MISTRAL MDEL INSTALLATION with COLAB GPU  ---\n",
        "# Install llama-cpp-python with CUDA support for the T4 GPU\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n",
        "# Install the LlamaIndex connector for LlamaCPP and json-repair for Mistral cleaning\n",
        "!pip install llama-index-llms-llama-cpp json-repair\n",
        "\n",
        "\n",
        "# --- [GLOBAL STATE INITIALIZATION] ---\n",
        "\n",
        "# --- 1. Initialize GLOBAL AUDIT LOG for Performance Tracking ---\n",
        "# audit_logs: Stores performance data (latencies, ROUGE scores) for the report generator\n",
        "audit_logs = []\n",
        "print(\"‚úÖ Global audit_logs list initialized.\")\n",
        "\n",
        "# --- 2. Initialize GLOBAL STATE TRACKING FOR LLM CHOICE ---\n",
        "# current_llm/name: Tracks the active engine to prevent unnecessary re-loading of weights\n",
        "current_llm = None\n",
        "current_model_name = \"\"\n",
        "print(\"‚úÖ Global state for LLM variables initialized.\")\n",
        "\n",
        "\n",
        "# --- [ASYNC & ENVIRONMENT PREP] ---\n",
        "try:\n",
        "    loop = asyncio.get_event_loop()\n",
        "    if loop.is_running():\n",
        "        print(\"üí° Event loop is active; preparing for asynchronous RAG operations.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# --- [EXTERNAL STORAGE LINKING] ---\n",
        "# MOUNT GOOGLE DRIVE (For UI Image) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ SECTION 1. SETUP AND INSTALLATION COMPLETE.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GekfvnoW_jaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a8a56d-8be7-40b1-c4e0-0a3f477d144e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.12/dist-packages (0.3.16)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Requirement already satisfied: llama-index-llms-llama-cpp in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: json-repair in /usr/local/lib/python3.12/dist-packages (0.55.0)\n",
            "Requirement already satisfied: llama-cpp-python<0.4,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-llama-cpp) (0.3.16)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-llama-cpp) (0.14.12)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.1.6)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.22.1)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.11.7)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.6.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.5.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.12.3)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.0.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.12.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.67.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.0.3)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.26.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.6)\n",
            "‚úÖ Global audit_logs list initialized.\n",
            "‚úÖ Global state for LLM variables initialized.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ SECTION 1. SETUP AND INSTALLATION COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 2A. CORE IMPORTS, SECURITY KEYS, AND GLOBAL SETTINGS CONFIGURATIONS**\n",
        "\n",
        "This section serves as the Operational Command Center for the platform. It transitions the environment from a collection of installed packages to a configured, functional system.\n",
        "\n",
        "The logic flow is as follows:\n",
        "\n",
        "1. **Library Orchestration:** Imports are logically categorized. By grouping them, we separate the \"Engine\" (AI/LLMs) from the \"Interface\" (Gradio/Reporting) and the \"Mechanics\" (Document Parsing/OCR).\n",
        "\n",
        "2. **Resource Verification:** Instead of waiting for the UI to fail, the code proactively validates file paths for essential brand assets (Logos) and architectural diagrams. This is a best practice for production MVPs to ensure the user experience is consistent.\n",
        "\n",
        "3. **Security & Authentication:** Retrieves the Gemini API key from Colab‚Äôs secure userdata (Secrets). This ensures that sensitive keys are never hard-coded into the notebook.\n",
        "\n",
        "4. **Global RAG Settings:** This is the most critical logic step. By assigning Settings.embed_model and Settings.llm, you establish a \"Global Context\" within LlamaIndex. This allows every subsequent component (Indexers, Retrievers, and Query Engines) to automatically inherit these configurations without redundant code.\n"
      ],
      "metadata": {
        "id": "1TlPkZLbvExS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 2A. CORE IMPORTS, SECURITY KEYS, AND GLOBAL SETTINGS CONFIGURATIONS -------\n",
        "\n",
        "# 1. Standard Library & Utilities\n",
        "import os, time, json, re, io, tempfile, hashlib, asyncio\n",
        "import random # Used for simulating performance audit metrics\n",
        "import gc # Memory Management: Essential for Google Colab T4 GPU\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# 2. Data Science & Visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# 3. Document Processing & OCR\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "\n",
        "# 4. AI & Machine Learning (Vector Engine/Backend)\n",
        "# Core Frameworks\n",
        "import torch # Memory Management: Essential for Google Colab T4 GPU\n",
        "import faiss\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# 5. LlamaIndex (RAG Framework)\n",
        "# The Orchestrator\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
        "\n",
        "        # --- LLM & Embedding -----\n",
        "from json_repair import repair_json  # Critical imports for your Mistral/Router logic\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "\n",
        "# 6. UI & Automated PDF Reporting\n",
        "import gradio as gr\n",
        "from gradio_pdf import PDF\n",
        "from reportlab.lib import colors\n",
        "from google.colab import userdata\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.colors import HexColor, black, green\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
        "\n",
        "\n",
        "# --- --- --- [MEMORY MANAGEMENT] --- --- ---\n",
        "# Shared 4-bit configuration for T4 GPU efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# --- --- --- [RESOURCE PATH DEFINITIONS] ---- --- ---\n",
        "# 1A. Define File Path - LOGO\n",
        "PROJECT_FOLDER = '/content/drive/MyDrive/AI_Powered_Document_Intelligence_Automation_Platform'\n",
        "LOGO_PATH = os.path.join(PROJECT_FOLDER, 'AI Document Assistant logo v2.png')\n",
        "\n",
        "# 2A. Define File Path - CONFIG & FILTER IMAGE\n",
        "CONFIG_FILTER_PATH = os.path.join(PROJECT_FOLDER, 'Document Filter and RAG.png')\n",
        "\n",
        "# 1B. Verify the path exists to avoid \"File Not Found\" errors later\n",
        "if os.path.exists(LOGO_PATH):\n",
        "    print(f\"‚úÖ Image found at: {LOGO_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: Image not found. Check path: {LOGO_PATH}\")\n",
        "\n",
        "\n",
        "# 2B. Verify the path exists to avoid \"File Not Found\" errors later\n",
        "if os.path.exists(CONFIG_FILTER_PATH):\n",
        "    print(f\"‚úÖ Image found at: {CONFIG_FILTER_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: Image not found. Check path: {CONFIG_FILTER_PATH}\")\n",
        "\n",
        "\n",
        "# --- --- --- [SECURITY & GLOBAL CONFIGURATION] --- --- ---\n",
        "# 1A. Load Gemini API\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in Colab Secrets.\")\n",
        "\n",
        "\n",
        "# 1B Load Hugginf Face Token\n",
        "# Retrieve the secret from Colab and set it as an environment variable\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "    print(\"‚úÖ Hugging Face Token successfully loaded from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Could not find HF_TOKEN in Colab Secrets. Check the 'Key' icon on the left.\")\n",
        "\n",
        "\n",
        "# 2. Configure Embedding Model\n",
        "llama_embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# 3A. GLOBAL CONFIGURATION (Crucial for Section 9 & 10)\n",
        "Settings.embed_model = llama_embed_model\n",
        "# 3B. Initial default LLM\n",
        "Settings.llm = GoogleGenAI(\n",
        "    model=\"models/gemini-2.0-flash\",\n",
        "    api_key=API_KEY\n",
        ")\n",
        "\n",
        "# 3C. SAFE NAME ASSIGNMENT\n",
        "# We use a custom attribute that Pydantic won't block,\n",
        "# or simply use the existing 'model' attribute.\n",
        "# To satisfy your Section 11 logs, we use this \"monkeypatch\" method:\n",
        "try:\n",
        "    # This bypasses Pydantic's strict check\n",
        "    object.__setattr__(Settings.llm, 'model_name', \"Gemini 2.0 Flash\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(f\"‚úÖ Settings initialized with: {getattr(Settings.llm, 'model_name', Settings.llm.model)}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 2A. CORE IMPORTS, SECURITY KEYS LOADED, AND GLOBAL SETTINGS CONFIGURATIONS COMPLETE.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uMdiYDOz_5oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f623b4b0-042a-410c-d183-f1aa94a14a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Image found at: /content/drive/MyDrive/AI_Powered_Document_Intelligence_Automation_Platform/AI Document Assistant logo v2.png\n",
            "‚úÖ Image found at: /content/drive/MyDrive/AI_Powered_Document_Intelligence_Automation_Platform/Document Filter and RAG.png\n",
            "‚úÖ Hugging Face Token successfully loaded from Colab Secrets.\n",
            "‚úÖ Settings initialized with: Gemini 2.0 Flash\n",
            "‚úÖ SECTION 2A. CORE IMPORTS, SECURITY KEYS LOADED, AND GLOBAL SETTINGS CONFIGURATIONS COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "This section establishes the **Model Agnostic Architecture** of the platform. By decoupling model definition from model execution, the system can seamlessly transition between high-performance cloud APIs (Gemini) and specialized local models (Mistral, Phi-2) without code duplication. This flexibility allows for rapid prototyping and cost-effective scaling within restricted hardware environments.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow follows a **\"Modular Factory Pattern\"**:\n",
        "1. **Encapsulated Initializers** Individual helper functions serve as \"factories\" for specific LLM types. This isolates technical complexities‚Äîsuch as **4-bit quantization configs**, device maps, and specific tokenizer paths‚Äîaway from the core application logic. This modularity ensures that adding a new model only requires creating a new factory function rather than rewriting the orchestration layer.\n",
        "\n",
        "2. **T4 GPU Optimization (Quantization):** Local models are configured using `BitsAndBytesConfig`. This ensures large models like Mistral 7B are compressed into 4-bit precision, significantly reducing their VRAM footprint from ~15GB to ~5.5GB. This optimization is what makes multi-model experimentation possible on a standard T4 instance without sacrificing significant reasoning capability.\n",
        "\n",
        "3. **Stateless Definition:** These functions define how to build the \"brain\" of the AI but do not activate it immediately. This \"on-demand\" approach is a critical memory management strategy; it prevents the system from attempting to load all models into VRAM simultaneously, which would cause an immediate `CUDA out of memory` error."
      ],
      "metadata": {
        "id": "1MYP_DZAw6QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION -------\n",
        "\n",
        "# --- 1. MODEL INITIALIZERS ---\n",
        "\n",
        "### üß† Helper function to set up Gemini (External API) ###\n",
        "def setup_gemini_llm():\n",
        "    \"\"\"Initializes the latest Google GenAI model for the platform.\"\"\"\n",
        "    # Ensure you use the correct class name GoogleGenAI\n",
        "    llm = GoogleGenAI(\n",
        "        model=\"models/gemini-2.0-flash\",\n",
        "        api_key=API_KEY\n",
        "    )\n",
        "    # Pydantic-safe name pinning\n",
        "    object.__setattr__(llm, 'model_name', \"Gemini 2.0 Flash\")\n",
        "\n",
        "    Print(\"‚úÖ Gemini API Key Loaded & Configured.\")\n",
        "\n",
        "    return llm\n",
        "\n",
        "\n",
        "### üß† Helper function to set up Mistral (External API) ###\n",
        "def setup_mistral_7b_llm():\n",
        "    \"\"\"Downloads and loads Mistral-7B via GGUF to fit ~4.1GB VRAM.\"\"\"\n",
        "\n",
        "    model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "    model_path = \"/content/mistral-7b-v0.2.Q4_K_M.gguf\"\n",
        "\n",
        "    # Automated check to ensure the file exists\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"üì• Downloading Mistral (4.1GB)...\")\n",
        "        os.system(f'wget -q {model_url} -O {model_path}')\n",
        "        print(\"‚úÖ Download Complete.\")\n",
        "\n",
        "    llm = LlamaCPP(\n",
        "        model_path=model_path,\n",
        "        temperature=0.1,\n",
        "        max_new_tokens=1024,\n",
        "        context_window=16384, # increase from 8192 to 16384 due to CPU limitations; model was trained to \"remember\" up to 32,768 tokens\n",
        "        model_kwargs={\"n_gpu_layers\": 33}, # Offload 33 layers (full model) to T4 GPU\n",
        "        messages_to_prompt=lambda messages: \"\\n\".join([f\"{m.role}: {m.content}\" for m in messages]),\n",
        "        completion_to_prompt=lambda completion: f\"AI: {completion}\",\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Pydantic-safe name pinning\n",
        "    object.__setattr__(llm, 'model_name', \"Mistral 7B\")\n",
        "\n",
        "\n",
        "    print(\"‚úÖ Mistral 7B API Key Loaded & Configured.\")\n",
        "\n",
        "    return llm\n",
        "\n",
        "\n",
        "\n",
        "### üß† Helper function to set up Microsoft Phi-2(External API) ###\n",
        "def setup_phi2_llm():\n",
        "    \"\"\"Loads Microsoft Phi-2 (2.7B) - Very fast on T4.\"\"\"\n",
        "    llm = HuggingFaceLLM(\n",
        "        model_name=\"microsoft/phi-2\",\n",
        "        model_kwargs={\"quantization_config\": bnb_config, \"trust_remote_code\": True},\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Pydantic-safe name pinning\n",
        "    object.__setattr__(llm, 'model_name', \"Microsoft Phi-2\")\n",
        "\n",
        "    print(\"‚úÖ Phi-2 API Key Loaded & Configured.\")\n",
        "\n",
        "    return llm\n",
        "\n",
        "\n",
        "\n",
        "### üß† Helper function to set up TinyLlama (External API) ###\n",
        "def setup_tinyllama_llm():\n",
        "    \"\"\"Loads TinyLlama (1.1B) - Lowest memory footprint (~2GB).\"\"\"\n",
        "    llm = HuggingFaceLLM(\n",
        "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        model_kwargs={\"quantization_config\": bnb_config},\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    # Pydantic-safe name pinning\n",
        "    object.__setattr__(llm, 'model_name', \"TinyLlama 1.1B\")\n",
        "\n",
        "    print(\"‚úÖ TinyLlama API Key Loaded & Configured.\")\n",
        "\n",
        "    return llm\n",
        "\n",
        "print(\"‚úÖ SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION COMPLETE.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ljYtjJ54OLd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e6f39e-0b01-4928-af0f-bd81cfceadbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 2B. LLM FACTORY & RESOURCE CONFIGURATION COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT**\n",
        "\n",
        "Logic and Flow Analysis\n",
        "This section defines the **Data Blueprint** for the entire platform. While many RAG systems treat text as \"flat strings,\" this MVP uses Python dataclasses to create a hierarchical representation of a document.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flows through three granular levels:\n",
        "1. **Page Level (**`PageInfo`**):** The smallest physical unit. It captures raw text while tagging it with a page number, ensuring that when the AI answers a question, it can cite the exact location in the original PDF.\n",
        "\n",
        "2. **Logical Level (**`LogicalDocument`**):** The business unit. Often, a single 100-page PDF contains multiple distinct documents (e.g., an Invoice followed by a Contract). This structure allows the system to \"boundary-detect\" where one document ends and another begins, preventing data leakage between unrelated sections.\n",
        "\n",
        "3. **Search Level (**`ChunkMetadata`**):** The retrieval unit. This stores the text along with \"Rich Metadata\" (IDs, types, and page ranges). By including the embedding field directly in the object, the platform can easily pass these structures between the FAISS vector store and the LLM generator."
      ],
      "metadata": {
        "id": "RLrZv_r30NBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT -------\n",
        "@dataclass\n",
        "class PageInfo:\n",
        "    \"\"\"\n",
        "    PHYSICAL LAYER: Represents one page of the input file.\n",
        "    Used for OCR tracking and initial classification. Stores information about a single page.\n",
        "    \"\"\"\n",
        "    page_num: int\n",
        "    text: str\n",
        "    doc_type: Optional[str] = None\n",
        "    page_in_doc: int = 0   # Position relative to the logical start\n",
        "\n",
        "@dataclass\n",
        "class LogicalDocument:\n",
        "    \"\"\"\n",
        "    BUSINESS LAYER: Groups pages into a single 'semantic' entity.\n",
        "    Logic: If a PDF has 10 pages, pages 1-3 might be a 'Purchase Order'\n",
        "    and 4-10 a 'Master Service Agreement'.\n",
        "    Represents a logical document within a PDF.\n",
        "    \"\"\"\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    chunks: List[Dict] = None\n",
        "\n",
        "@dataclass\n",
        "class ChunkMetadata:\n",
        "    \"\"\"\n",
        "    RETRIEVAL LAYER: The actual object indexed in the Vector Database.\n",
        "    Rich metadata here allows for 'Siloed Retrieval' (filtering by doc_type).\n",
        "    Rich metadata for each chunk.\n",
        "    \"\"\"\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    chunk_index: int\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "\n",
        "print(\"‚úÖ DATA STRUCTURES INITIALIZED.\")\n",
        "\n",
        "print(\"‚úÖ SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT COMPLETE.\")\n"
      ],
      "metadata": {
        "id": "wvc4XUc1AvTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0a77a2-6a73-4b42-b4c6-060acfb5f221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DATA STRUCTURES INITIALIZED.\n",
            "‚úÖ SECTION 3. DATA STRUCTURES FOR ENHANCED DOCUMENT MANAGEMENT COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section acts as the \"Triage Layer\" of the platform. It moves beyond simple text extraction and applies semantic intelligence to understand what the document is and where it ends.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow consists of two primary cognitive tasks:\n",
        "1. **Semantic Classification (**`classify_document_type` **):** Instead of using brittle keyword matching, this function utilizes the LLM to understand the context of the page. It maps raw text to a predefined taxonomy (Invoices, Land Deeds, etc.). This categorization is vital for \"Siloed Retrieval\" later in the pipeline, ensuring that a query about a \"loan fee\" doesn't accidentally pull data from a \"Medical Report.\"\n",
        "2. **Logical Boundary Detection\n",
        "(** `detect_document_boundary` **):** This is the solution to the \"Combined PDF\" problem. In business workflows, users often scan multiple documents into a single file. This function performs a \"bridge analysis\" between the end of one page and the start of the next. It checks for continuity in formatting, topic, and sentence structure to decide if the system should start a new `LogicalDocument` object."
      ],
      "metadata": {
        "id": "xQNJUFwB2gMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS -------\n",
        "def classify_document_type(text: str, max_length: int = 1500) -> str:\n",
        "    \"\"\"\n",
        "    Identifies the document category using semantic analysis.\n",
        "    Essential for routing queries to the correct document 'silo'.\n",
        "    \"\"\"\n",
        "    # Truncate text if too long to avoid token limits\n",
        "    # Safety Check: Use a sample to stay within LLM context limits and reduce latency\n",
        "    text_sample = text[:max_length] if len(text) > max_length else text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze this document and classify it into ONE of these categories:\n",
        "    - Resume: CV, professional profile, work history\n",
        "    - Contract: Legal agreement, terms and conditions, service agreement\n",
        "    - Mortgage Contract: Home loan agreement, mortgage terms, property financing\n",
        "    - Invoice: Bill, payment request, financial statement\n",
        "    - Pay Slip: Salary statement, wage slip, earnings statement\n",
        "    - Lender Fee Sheet: Loan fees, lender charges, closing costs\n",
        "    - Land Deed: Property deed, title document, ownership certificate\n",
        "    - Bank Statement: Account statement, transaction history\n",
        "    - Tax Document: W2, 1099, tax return, tax form\n",
        "    - Insurance: Insurance policy, coverage document\n",
        "    - Report: Analysis, research document, findings\n",
        "    - Letter: Correspondence, memo, communication\n",
        "    - Form: Application, questionnaire, data entry form\n",
        "    - ID Document: Driver's license, passport, identification\n",
        "    - Medical: Medical report, prescription, health record\n",
        "    - Other: Doesn't fit other categories\n",
        "\n",
        "    Document sample:\n",
        "    {text_sample}\n",
        "\n",
        "    Respond with ONLY the category name, nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use the global LlamaIndex LLM setting\n",
        "        response = Settings.llm.complete(prompt)\n",
        "        doc_type = response.text.strip()\n",
        "\n",
        "        # Normalize the response\n",
        "        valid_types = [\n",
        "            'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',\n",
        "            'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',\n",
        "            'Insurance', 'Report', 'Letter', 'Form', 'ID Document',\n",
        "            'Medical', 'Other'\n",
        "        ]\n",
        "\n",
        "        # Find best match (case-insensitive)\n",
        "        for valid_type in valid_types:\n",
        "            if doc_type.lower() == valid_type.lower():\n",
        "                return valid_type\n",
        "\n",
        "        return 'Other'\n",
        "    except Exception as e:\n",
        "        print(f\"Classification error: {e}\")\n",
        "        return 'Other'\n",
        "\n",
        "def detect_document_boundary(prev_text: str, curr_text: str,\n",
        "                            current_doc_type: str = None) -> bool:\n",
        "    \"\"\"\n",
        "    Detect if two consecutive pages belong to the same document.\n",
        "    Returns True if they're from the same document.\n",
        "    \"\"\"\n",
        "    # Quick heuristic checks first\n",
        "    if not prev_text or not curr_text:\n",
        "        return False\n",
        "\n",
        "    # Sample the texts for L\\LM analysis\n",
        "    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text\n",
        "    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Determine if these two pages are from the SAME document or different documents.\n",
        "\n",
        "    Current document type: {current_doc_type or 'Unknown'}\n",
        "\n",
        "    End of Previous Page:\n",
        "    ...{prev_sample}\n",
        "\n",
        "    Start of Current Page:\n",
        "    {curr_sample}...\n",
        "\n",
        "    Consider:\n",
        "    - Continuity of content\n",
        "    - Formatting consistency\n",
        "    - Topic coherence\n",
        "    - Page numbers or headers\n",
        "\n",
        "    Default to 'Yes' unless you see a clear signal of a different entity\n",
        "    (e.g., a new person's name on a resume, a different bank logo, or a new header 'Exhibit A').\n",
        "    Answer ONLY 'Yes' or 'No'.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use the global LlamaIndex LLM setting\n",
        "        response = Settings.llm.complete(prompt)\n",
        "\n",
        "        return response.text.strip().lower().startswith('yes')\n",
        "    except Exception as e:\n",
        "        print(f\"Boundary detection error: {e}\")\n",
        "        # Default to keeping pages together if uncertain\n",
        "        return True\n",
        "\n",
        "print(\"‚úÖ SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS COMPLETE.\")\n"
      ],
      "metadata": {
        "id": "g6u6uV1RJab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf8b8d3-d9cc-4049-db95-f241bee911a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 4. DOCUMENT INTELLIGENCE FUNCTIONS COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 5. ADVANCED PDF PROCESSING PIPELINE**\n",
        "\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section serves as the **Data Gateway** of the platform. It is designed to handle \"Real-World\" documents which are often messy, scanned, or combined.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow implements a **Smart Routing and Hybrid OCR** strategy:\n",
        "1. **Aware Router (** `extract_and_analyze_file`**):** Acts as a traffic controller, detecting file extensions and directing the payload to either the PDF or Image processor.\n",
        "\n",
        "2. **Hybrid OCR Engine (** `extract_and_analyze_pdf` **):** Solves the \"Blank Page\" problem. If standard text extraction (PyMuPDF) returns an empty string‚Äîcommon in scanned mortgage or legal docs‚Äîthe system automatically triggers a high-resolution render and passes it to **Tesseract OCR**.\n",
        "\n",
        "3. **Boundary Integration (** `analyze_pages` **):** This is the heart of the logical segmentation. It iterates through extracted pages, calling the intelligence functions from Section 4 to group them into cohesive LogicalDocument objects. It ensures that a 10-page \"Mixed PDF\" is correctly split into its constituent business components.\n",
        "\n",
        "4. **UI Visualization Layer (** `load_pdf_into_viewer & flip_page` **):** Converts heavy PDF objects into a crisp image-based state for the Gradio UI. By using a high-density Matrix (3x3), it ensures that fine print on legal documents remains legible for the human reviewer."
      ],
      "metadata": {
        "id": "yGrQwXDp4gh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 5. ADVANCED PDF PROCESSING PIPELINE -------\n",
        "\n",
        "# --- 1. CORE SEGMENTATION LOGIC ---\n",
        "def analyze_pages(pages_info):  # Shared Analysis Logic\n",
        "    \"\"\"\n",
        "    Groups individual pages into logical business units.\n",
        "    Flow: Page Ingestion -> Boundary Detection -> Logical Document Creation.\n",
        "    \"\"\"\n",
        "\n",
        "    logical_docs = []\n",
        "    current_pages = []\n",
        "    doc_counter = 0\n",
        "\n",
        "    for i, page in enumerate(pages_info):\n",
        "        if i == 0:\n",
        "            # Initialize the first document type\n",
        "            doc_type = classify_document_type(page.text)\n",
        "            current_pages = [page]\n",
        "        else:\n",
        "            # Check if current page is a continuation of the previous one\n",
        "            if detect_document_boundary(pages_info[i-1].text, page.text, doc_type):\n",
        "                current_pages.append(page)\n",
        "            else:\n",
        "              # Boundary detected: Finalize the current logical document\n",
        "                logical_docs.append(\n",
        "                    LogicalDocument(\n",
        "                        doc_id=f\"doc_{doc_counter}\",\n",
        "                        doc_type=doc_type,\n",
        "                        page_start=current_pages[0].page_num,\n",
        "                        page_end=current_pages[-1].page_num,\n",
        "                        text=\"\\n\\n\".join(p.text for p in current_pages),\n",
        "                    )\n",
        "                )\n",
        "                doc_counter += 1\n",
        "                doc_type = classify_document_type(page.text)\n",
        "                current_pages = [page]\n",
        "\n",
        "    # Handle the final trailing document in the sequence\n",
        "    if current_pages:\n",
        "        logical_docs.append(\n",
        "            LogicalDocument(\n",
        "                doc_id=f\"doc_{doc_counter}\",\n",
        "                doc_type=doc_type,\n",
        "                page_start=current_pages[0].page_num,\n",
        "                page_end=current_pages[-1].page_num,\n",
        "                text=\"\\n\\n\".join(p.text for p in current_pages),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return pages_info, logical_docs\n",
        "\n",
        "\n",
        "# --- 2. MULTI-MODAL INGESTION ROUTERS ---\n",
        "def extract_and_analyze_file(file): # Aware Router\n",
        "    \"\"\"\n",
        "    AWARE ROUTER: Detects file type and selects the appropriate ingestion path.\n",
        "    \"\"\"\n",
        "\n",
        "    ext = os.path.splitext(file.name)[1].lower()\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        return extract_and_analyze_pdf(file)\n",
        "    elif ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
        "        return extract_and_analyze_image(file)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
        "\n",
        "\n",
        "def extract_and_analyze_pdf(pdf_file) -> Tuple[List[PageInfo], List[LogicalDocument]]:\n",
        "    \"\"\"\n",
        "    HYBRID OCR PIPELINE: Extracts digital text or triggers OCR for scanned pages.\n",
        "    \"\"\"\n",
        "\n",
        "    # Capture the actual name from the Gradio file object\n",
        "    original_filename = os.path.basename(pdf_file.name)\n",
        "\n",
        "\n",
        "    print(\"üìñ Starting PDF extraction and analysis for: {original_filename}\")\n",
        "\n",
        "    doc = fitz.open(pdf_file.name) # open file\n",
        "\n",
        "    pages_info = []\n",
        "    for i, page in enumerate(doc):\n",
        "        text = page.get_text().strip()\n",
        "\n",
        "        # Hybrid OCR: If no text found, render page to image and use Tesseract\n",
        "        if not text:\n",
        "            pix = page.get_pixmap()\n",
        "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "            text = pytesseract.image_to_string(img)\n",
        "\n",
        "        pages_info.append(PageInfo(page_num=i, text=text))\n",
        "\n",
        "    doc.close()\n",
        "    return analyze_pages(pages_info)\n",
        "\n",
        "\n",
        "def extract_and_analyze_image(image_file): # Image Ingestion\n",
        "    \"\"\"Processes standalone image files via OCR.\"\"\"\n",
        "\n",
        "    print(\"üñºÔ∏è Processing Image:\", image_file.name)\n",
        "\n",
        "    img = Image.open(image_file.name)\n",
        "    text = pytesseract.image_to_string(img)\n",
        "\n",
        "    pages_info = [PageInfo(page_num=0, text=text)]\n",
        "    return analyze_pages(pages_info)\n",
        "\n",
        "\n",
        "\n",
        " # --- 3. UI RENDERING LOGIC ---\n",
        "\n",
        "# For Document Viewer in UI (Convert Uploaded PDF file into an image to be viewed in Gradio UI)\n",
        "def load_pdf_into_viewer(selected_file):\n",
        "    \"\"\"\n",
        "    Loads all pages into state, but only displays the first one.\n",
        "    Renders PDF pages to crisp images for the Gradio interface.\n",
        "    \"\"\"\n",
        "\n",
        "    if not selected_file or not os.path.exists(str(selected_file)):\n",
        "        return None, {\"current_page\": 0, \"images\": []}, \"**Page 0 of 0**\"\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(selected_file)\n",
        "        images = []\n",
        "        # Matrix(3, 3) + csRGB ensures crisp black text (fixes faded look)\n",
        "        for page in doc:\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(3, 3), colorspace=fitz.csRGB)\n",
        "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "            images.append(img)\n",
        "        doc.close()\n",
        "\n",
        "        if not images:\n",
        "            return None, {\"current_page\": 0, \"images\": []}, \"**Empty PDF**\"\n",
        "\n",
        "        return images[0], {\"current_page\": 0, \"images\": images}, f\"<center>**Page 1 of {len(images)}**</center>\"\n",
        "    except Exception as e:\n",
        "        print(f\"Viewer Error: {e}\")\n",
        "        return None, {\"current_page\": 0, \"images\": []}, \"**Error loading**\"\n",
        "\n",
        "\n",
        "def flip_page(direction, state):\n",
        "    \"\"\"Navigates through the images stored in state.\"\"\"\n",
        "    images = state.get(\"images\", [])\n",
        "    current = state.get(\"current_page\", 0)\n",
        "\n",
        "    if not images:\n",
        "        return None, state, \"**Page 0 of 0**\"\n",
        "\n",
        "    if direction == \"next\":\n",
        "        current = min(current + 1, len(images) - 1)\n",
        "    else:\n",
        "        current = max(current - 1, 0)\n",
        "\n",
        "    state[\"current_page\"] = current\n",
        "    indicator = f\"**Page {current + 1} of {len(images)}**\"\n",
        "    return images[current], state, f\"<center>**Page {current + 1} of {len(images)}**</center>\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 5. ADVANCED PDF PROCESSING PIPELINE COMPLETE.\")\n"
      ],
      "metadata": {
        "id": "dPphWV5WK273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c4ae1f-a5f4-46e6-93f9-cf153be19793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 5. ADVANCED PDF PROCESSING PIPELINE COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section defines the Granular Transformation Layer. After a document has been logically segmented (e.g., separating an Invoice from a Contract), the text must be broken down into \"chunks\" that fit the context window of an LLM while ensuring that the \"provenance\" (where the data came from) is never lost.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow provides two distinct pathways:\n",
        "1. **Semantic Sliding Window (**`chunk_document_with_metadata`**):** A custom-built algorithm that ensures no information is lost at the boundaries of chunks by creating an \"overlap.\" It uses a calculated stride to maintain context across segments.\n",
        "\n",
        "2. **LlamaIndex Orchestration (**`chunk_with_llama_index` **):** An alternative high-level path using the `SentenceSplitter`. This is superior for complex documents as it respects paragraph and sentence boundaries, preventing a chunk from being cut off in the middle of a critical legal clause.\n",
        "\n",
        "3. **Metadata Injection:** This is the \"Secret Sauce\" of the platform. Every chunk‚Äîno matter how small‚Äîis stamped with its `doc_type`, `doc_id`, and `page_range`. This ensures that in the retrieval phase (Section 7), the system can filter out irrelevant document types with 100% precision."
      ],
      "metadata": {
        "id": "qYroZgDG9FxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION -------\n",
        "\n",
        "# --- 1. CUSTOM SLIDING WINDOW CHUNKING ---\n",
        "def chunk_document_with_metadata(logical_doc: LogicalDocument,\n",
        "                                chunk_size: int = 500,\n",
        "                                overlap: int = 100) -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Chunk a logical document while preserving rich metadata.\n",
        "    Uses sliding window with overlap for better context.\n",
        "\n",
        "    Ensures 'overlap' to prevent loss of context at chunk boundaries.\n",
        "    \"\"\"\n",
        "    chunks_metadata = []\n",
        "    words = logical_doc.text.split()\n",
        "\n",
        "    # Case A: Document is smaller than the threshold\n",
        "    if len(words) <= chunk_size:\n",
        "        # Document is small enough to be a single chunk\n",
        "        chunk_meta = ChunkMetadata(\n",
        "            chunk_id=f\"{logical_doc.doc_id}_chunk_0\",\n",
        "            doc_id=logical_doc.doc_id,\n",
        "            doc_type=logical_doc.doc_type,\n",
        "            chunk_index=0,\n",
        "            page_start=logical_doc.page_start,\n",
        "            page_end=logical_doc.page_end,\n",
        "            text=logical_doc.text\n",
        "        )\n",
        "        chunks_metadata.append(chunk_meta)\n",
        "\n",
        "    # Case B: Multi-chunk split with sliding window\n",
        "    else:\n",
        "        # Create overlapping chunks\n",
        "        stride = chunk_size - overlap\n",
        "        for i, start_idx in enumerate(range(0, len(words), stride)):\n",
        "            end_idx = min(start_idx + chunk_size, len(words))\n",
        "            chunk_text = ' '.join(words[start_idx:end_idx])\n",
        "\n",
        "            # Calculate which pages this chunk spans\n",
        "            # (simplified - in production, track more precisely)\n",
        "            chunk_position = start_idx / len(words)\n",
        "            page_range = logical_doc.page_end - logical_doc.page_start\n",
        "            relative_page = int(chunk_position * page_range)\n",
        "            chunk_page_start = logical_doc.page_start + relative_page\n",
        "            chunk_page_end = min(chunk_page_start + 1, logical_doc.page_end)\n",
        "\n",
        "            chunk_meta = ChunkMetadata(\n",
        "                chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
        "                doc_id=logical_doc.doc_id,\n",
        "                doc_type=logical_doc.doc_type,\n",
        "                chunk_index=i,\n",
        "                page_start=chunk_page_start,\n",
        "                page_end=chunk_page_end,\n",
        "                text=chunk_text\n",
        "            )\n",
        "            chunks_metadata.append(chunk_meta)\n",
        "\n",
        "            if end_idx >= len(words):\n",
        "                break\n",
        "\n",
        "    return chunks_metadata\n",
        "\n",
        "\n",
        "# --- 2. LLAMA-INDEX ADVANCED CHUNKING ---\n",
        "def chunk_with_llama_index(logical_doc: LogicalDocument,\n",
        "                           chunk_size: int = 500,\n",
        "                           chunk_overlap: int = 100) -> List[Document]: # Chunk Metadata\n",
        "    \"\"\"\n",
        "    Alternative: Use LlamaIndex's advanced chunking with metadata.\n",
        "    \"\"\"\n",
        "    # Create LlamaIndex document with metadata\n",
        "    doc = Document(\n",
        "        text=logical_doc.text,\n",
        "        metadata={\n",
        "            \"doc_id\": logical_doc.doc_id,\n",
        "            \"doc_type\": logical_doc.doc_type,\n",
        "            \"page_start\": logical_doc.page_start,\n",
        "            \"page_end\": logical_doc.page_end,\n",
        "            \"source\": f\"{logical_doc.doc_type}_document\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Use LlamaIndex's sentence splitter for better chunking\n",
        "    # Sentence-aware splitter prevents cutting mid-sentence\n",
        "    splitter = SentenceSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        paragraph_separator=\"\\n\\n\",\n",
        "        separator=\" \",\n",
        "    )\n",
        "\n",
        "    # Create nodes (chunks) from document\n",
        "    nodes = splitter.get_nodes_from_documents([doc])\n",
        "\n",
        "    # Convert to our ChunkMetadata format for consistency\n",
        "    chunks_metadata = []\n",
        "    for i, node in enumerate(nodes):\n",
        "        chunk_meta = ChunkMetadata(\n",
        "            chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
        "            doc_id=logical_doc.doc_id,\n",
        "            doc_type=logical_doc.doc_type,\n",
        "            chunk_index=i,\n",
        "            page_start=node.metadata.get(\"page_start\", logical_doc.page_start),\n",
        "            page_end=node.metadata.get(\"page_end\", logical_doc.page_end),\n",
        "            text=node.text\n",
        "        )\n",
        "        chunks_metadata.append(chunk_meta)\n",
        "\n",
        "    return chunks_metadata\n",
        "\n",
        "\n",
        "# --- 3. BATCH PROCESSOR ---\n",
        "def process_all_documents(logical_docs: List[LogicalDocument],\n",
        "                         use_llama_index: bool = False) -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Orchestrates (Process) all logical documents into chunks with metadata.\n",
        "    Can use either custom or LlamaIndex chunking.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    for logical_doc in logical_docs:\n",
        "        if use_llama_index:\n",
        "            chunks = chunk_with_llama_index(logical_doc)\n",
        "        else:\n",
        "            chunks = chunk_document_with_metadata(logical_doc)\n",
        "\n",
        "        logical_doc.chunks = chunks  # Store reference\n",
        "        all_chunks.extend(chunks)\n",
        "        print(f\"üìÑ {logical_doc.doc_type}: Created {len(chunks)} chunks\")\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "print(\"‚úÖ SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION COMPLETE.\")\n"
      ],
      "metadata": {
        "id": "Mvk9h4e1LUCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77fa70d-d126-4294-c1ef-b4378067dca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 6. INTELLIGENT CHUNKING WITH METADATA PRESERVATION COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL**\n",
        "\n",
        " **Logic and Flow Analysis**\n",
        "\n",
        "This section serves as the **Precision Layer** of the system. Its primary goal is to solve \"Context Contamination\" which is a common failure in standard RAG where a query about an \"Invoice date\" might accidentally pull data from a \"Bank Statement\" just because they share similar keywords.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow implements Siloed Vector Search:\n",
        "1. **Semantic Query Routing (** `predict_query_document_type` **):** Before searching the database, the system uses the LLM as a \"Traffic Controller.\" It analyzes the user's intent to predict which specific document category (e.g., Mortgage Contract) holds the answer. The use of Regex parsing ensures that even if the LLM is verbose, the system extracts only the valid JSON routing instruction.\n",
        "\n",
        "2. **Segregated Indexing (** `build_indices` **):** Unlike basic RAG which creates one giant bucket of data, this system builds a primary FAISS index and several sub-indices (silos) based on document types. This allows for lightning-fast, high-precision searches within a specific document class.\n",
        "\n",
        "3. **Adaptive Retrieval (** `retrieve` **):** This function makes a real-time decision. If the Router has high confidence (>0.7), the search is restricted to the specific \"silo.\" If confidence is low, it falls back to a global search. This \"Hybrid Approach\" maximizes accuracy for specific questions while maintaining flexibility for general queries."
      ],
      "metadata": {
        "id": "d_MsgfAaAGsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL -------\n",
        "\n",
        "\n",
        "# --- 1. THE ROUTER (INTENT ANALYSIS) ---\n",
        "def predict_query_document_type(query: str, llm=None) -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Predict which document type is most likely to contain the answer.\n",
        "    Uses the currently active LLM from the Orchestrator.\n",
        "    Dynamically applies Mistral [INST] tags only when necessary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the passed LLM, or fall back to the global Settings.llm\n",
        "    active_llm = llm or Settings.llm\n",
        "\n",
        "    # Extract model name for logging (handles LlamaIndex LLM objects)\n",
        "    model_name = getattr(active_llm,\n",
        "                 \"model_name\",\n",
        "                 \"AI Engine\")\n",
        "\n",
        "    print(f\"üß† Routing via {model_name}...\")\n",
        "\n",
        "    # --- STEP 1: Detect Mistral ---\n",
        "    # This prevents formatting errors when switching between Gemini and Mistral\n",
        "    is_mistral = \"Mistral\" in model_name\n",
        "\n",
        "    # --- LOGGING THE CHOSEN LLM ---\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n",
        "        \"event\": \"ROUTING_ATTEMPT\",\n",
        "        \"model_used\": model_name,\n",
        "        \"query_preview\": query[:30] + \"...\"\n",
        "    }\n",
        "    audit_logs.append(log_entry)\n",
        "    print(f\"üß† Routing via {model_name}...\")\n",
        "\n",
        "    raw_prompt = f\"\"\"\n",
        "    Analyze this query and predict which document type would most likely contain the answer.\n",
        "\n",
        "    Query: \"{query}\"\n",
        "\n",
        "    Choose the MOST LIKELY type from:\n",
        "    - Resume: Career, experience, education, skills, employment history\n",
        "    - Contract: Terms, agreements, obligations, parties, legal terms\n",
        "    - Mortgage Contract: Home loan, property financing, mortgage terms, interest rates\n",
        "    - Invoice: Payments, amounts due, billing, charges, invoiced items\n",
        "    - Pay Slip: Salary, wages, deductions, earnings, pay period\n",
        "    - Lender Fee Sheet: Loan fees, closing costs, origination fees, lender charges\n",
        "    - Land Deed: Property ownership, deed information, property description, title\n",
        "    - Bank Statement: Account balance, transactions, deposits, withdrawals\n",
        "    - Tax Document: Tax information, W2, 1099, tax returns, tax amounts\n",
        "    - Insurance: Coverage, policy details, premiums, claims\n",
        "    - Report: Analysis, findings, conclusions, research data\n",
        "    - Letter: Communications, requests, notifications, correspondence\n",
        "    - Form: Applications, submitted data, form fields\n",
        "    - ID Document: Personal identification, ID numbers, identity verification\n",
        "    - Medical: Health information, medical conditions, prescriptions\n",
        "    - Other: General or unclear\n",
        "\n",
        "    Respond in JSON format:\n",
        "    {{\"type\": \"DocumentType\", \"confidence\": 0.85}}\n",
        "\n",
        "    Confidence should be between 0.0 and 1.0\n",
        "    \"\"\"\n",
        "\n",
        "    # --- STEP 2: Wrap Prompt for Mistral ---\n",
        "    # Gemini ignores these tags, but Mistral requires them to follow instructions\n",
        "    if is_mistral:\n",
        "        final_prompt = f\"[INST] {raw_prompt} [/INST]\"\n",
        "    else:\n",
        "        final_prompt = raw_prompt\n",
        "\n",
        "\n",
        "    try:\n",
        "        # 1. Use the wrapped promptthe generic .complete() method which works for Gemini, Phi-2, TinyLlama\n",
        "        response = active_llm.complete(final_prompt).text.strip()\n",
        "\n",
        "        # 2. Use Regex Parsing to find the JSON block.\n",
        "        # This prevents the \"line 1 column 1\" error if the LLM adds conversational text.\n",
        "        # Essential to strip 'conversational' text if the LLM adds it\n",
        "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            # 3. EXTRACT the string found by regex\n",
        "            raw_json_str = json_match.group()\n",
        "\n",
        "            # 4. REPAIR the string (Fixes missing quotes/commas from 4-bit Mistral)\n",
        "            repaired_json = repair_json(raw_json_str)\n",
        "\n",
        "            # 5. LOAD the cleaned JSON\n",
        "            result = json.loads(repaired_json)\n",
        "\n",
        "            doc_type = result.get(\"type\", \"Other\")\n",
        "            confidence = result.get(\"confidence\", 0.5)\n",
        "\n",
        "            print(f\"‚úÖ Router assigned: {doc_type} ({confidence*100:.1f}%)\")\n",
        "            return doc_type, confidence\n",
        "        else:\n",
        "            raise ValueError(\"No JSON pattern found in response text\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üéØ Routing fallback used. Error: {e}\")\n",
        "        return \"Other\", 0.0\n",
        "\n",
        "\n",
        "# --- 2. THE INTELLIGENT RETRIEVER (VECTOR ENGINE) ---\n",
        "class IntelligentRetriever:\n",
        "    \"\"\"\n",
        "    Advanced retrieval system with metadata filtering and query routing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.chunks_metadata = [] # Master list of all chunks from all files\n",
        "        self.doc_type_indices = {} # Map of indices per document type\n",
        "\n",
        "    def build_indices(self, new_chunks: List[ChunkMetadata]):\n",
        "        \"\"\"\n",
        "        Builds or UPDATES FAISS indices.\n",
        "        Ensures new chunks are added to the existing database.\n",
        "        \"\"\"\n",
        "        print(f\"üî® Processing {len(new_chunks)} new chunks for the vector index...\")\n",
        "\n",
        "        # 1. Create embeddings only for the NEW chunks\n",
        "        texts = [chunk.text for chunk in new_chunks]\n",
        "        embeddings_list = Settings.embed_model.get_text_embedding_batch(texts, show_progress=True)\n",
        "        new_embeddings = np.array(embeddings_list).astype('float32')\n",
        "        dim = new_embeddings.shape[1]\n",
        "\n",
        "        # Store embeddings in metadata for these new chunks\n",
        "        for i, chunk in enumerate(new_chunks):\n",
        "            chunk.embedding = new_embeddings[i]\n",
        "\n",
        "        # --- TIER 1: GLOBAL INDEX (APPEND MODE) ---\n",
        "        if self.index is None:\n",
        "            self.index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "        self.index.add(new_embeddings)\n",
        "\n",
        "        # IMPORTANT: Append new chunks to the master metadata list\n",
        "        # Prevents previous files from disappearing\n",
        "        self.chunks_metadata.extend(new_chunks)\n",
        "\n",
        "        # --- TIER 2: SEGREGATED INDICES (SILOS) ---\n",
        "        # Updates the silos to include the new data\n",
        "        doc_types = set(chunk.doc_type for chunk in new_chunks)\n",
        "\n",
        "        for doc_type in doc_types:\n",
        "            # Find indices of the new chunks that match this type\n",
        "            # Reference the full self.chunks_metadata to rebuild the mapping correctly\n",
        "            all_type_indices = [idx for idx, chunk in enumerate(self.chunks_metadata)\n",
        "                                if chunk.doc_type == doc_type]\n",
        "\n",
        "            if all_type_indices:\n",
        "                # Rebuild the specific silo index for this type\n",
        "                # (FAISS IndexFlatL2 is fast enough to rebuild for specific silos)\n",
        "                type_embeddings = np.array([self.chunks_metadata[i].embedding for i in all_type_indices]).astype('float32')\n",
        "\n",
        "                type_index = faiss.IndexFlatL2(dim)\n",
        "                type_index.add(type_embeddings)\n",
        "\n",
        "                self.doc_type_indices[doc_type] = {\n",
        "                    'index': type_index,\n",
        "                    'mapping': all_type_indices  # Maps back to the updated master list\n",
        "                }\n",
        "\n",
        "        print(f\"‚úÖ Database updated. Total Chunks: {len(self.chunks_metadata)}\")\n",
        "\n",
        "\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 4,\n",
        "                filter_doc_type: Optional[str] = None,\n",
        "                auto_route: bool = True) -> List[Tuple[ChunkMetadata, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve relevant chunks with optional filtering and routing.\n",
        "        Smart Retrieval Logic: Routes to silo if confidence > 0.7, else searches global index.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # 1. GENERATE QUERY EMBEDDING - Use Settings.embed_model.get_query_embedding\n",
        "        # FAISS expects a 2D numpy array (float32)\n",
        "        # Wrap the single embedding in a list\n",
        "        query_vec = Settings.embed_model.get_query_embedding(query)\n",
        "        query_embedding = np.array([query_vec]).astype('float32')\n",
        "\n",
        "        # Variables to store search results\n",
        "        chunk_indices = []\n",
        "        distances = []\n",
        "\n",
        "\n",
        "        # 2. SELECTION (ROUTING) LOGIC (Which index to search?)\n",
        "        # CASE A: User manually selected a specific filter (and it's not \"All\")\n",
        "        if filter_doc_type and filter_doc_type.lower() != \"all\" and filter_doc_type in self.doc_type_indices:\n",
        "            print(f\"üîç Searching specific silo: {filter_doc_type}\")\n",
        "            type_data = self.doc_type_indices[filter_doc_type]\n",
        "            D, I = type_data['index'].search(query_embedding, k)\n",
        "\n",
        "            # Map the silo-specific index back to the master self.chunks_metadata list\n",
        "            chunk_indices = [type_data['mapping'][i] for i in I[0] if i != -1]\n",
        "            distances = D[0][:len(chunk_indices)]\n",
        "\n",
        "        # CASE B: Auto-Route is enabled (AI guesses the document type)\n",
        "        elif auto_route:\n",
        "            predicted_type, confidence = predict_query_document_type(query)\n",
        "\n",
        "            # If AI is confident and the silo exists, search the silo\n",
        "            if confidence > 0.7 and predicted_type in self.doc_type_indices:\n",
        "                print(f\"üéØ Auto-routed to: {predicted_type} ({confidence:.2%})\")\n",
        "                type_data = self.doc_type_indices[predicted_type]\n",
        "                D, I = type_data['index'].search(query_embedding, k)\n",
        "                chunk_indices = [type_data['mapping'][i] for i in I[0] if i != -1]\n",
        "                distances = D[0][:len(chunk_indices)]\n",
        "            else:\n",
        "                # Fallback to global search if AI is unsure\n",
        "                print(f\"üåê Low routing confidence ({confidence:.2%}). Searching all documents...\")\n",
        "                D, I = self.index.search(query_embedding, k)\n",
        "                chunk_indices = [i for i in I[0] if i != -1]\n",
        "                distances = D[0][:len(chunk_indices)]\n",
        "\n",
        "        # CASE C: Search Everything (Filter is \"All\" or no filter provided)\n",
        "        else:\n",
        "            print(\"üåê Searching global index (all files)...\")\n",
        "            D, I = self.index.search(query_embedding, k)\n",
        "            chunk_indices = [i for i in I[0] if i != -1]\n",
        "            distances = D[0][:len(chunk_indices)]\n",
        "\n",
        "        # 3. CONVERT RESULTS TO SCORED CHUNKS\n",
        "        valid_results = []\n",
        "        for idx, i in enumerate(chunk_indices):\n",
        "            # i is the index in the master self.chunks_metadata list\n",
        "            # D[idx] is the distance (lower is better)\n",
        "            dist = distances[idx]\n",
        "\n",
        "            # Convert distance to a similarity score (0.0 to 1.0)\n",
        "            score = 1.0 / (1.0 + dist)\n",
        "\n",
        "            # Retrieve the full metadata object (preserves page numbers!)\n",
        "            chunk_obj = self.chunks_metadata[i]\n",
        "\n",
        "\n",
        "            #  Wrap in a Mock Object to prevent 'AttributeError' ---\n",
        "            # Create a simple class-like object that has a .node and a .score\n",
        "            class Retainer:\n",
        "                def __init__(self, node, score):\n",
        "                    self.node = node\n",
        "                    self.score = score\n",
        "\n",
        "            # Ensures the 'node' has metadata for the generator\n",
        "            class MockNode:\n",
        "                def __init__(self, text, metadata):\n",
        "                    self.text = text\n",
        "                    self.metadata = metadata\n",
        "                def get_content(self): return self.text\n",
        "\n",
        "            mock_node = MockNode(chunk_obj.text, {\n",
        "                \"page_start\": chunk_obj.page_start,\n",
        "                \"page_end\": chunk_obj.page_end,\n",
        "                \"doc_type\": chunk_obj.doc_type,\n",
        "                \"doc_id\": chunk_obj.doc_id\n",
        "            })\n",
        "\n",
        "            valid_results.append(Retainer(mock_node, score))\n",
        "\n",
        "        return valid_results\n",
        "\n",
        "print(\"‚úÖ SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL COMPLETE.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WvWhBx9-L8iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18de4463-75bf-42d9-f76b-b494ee73f083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 7. QUERY ROUTING AND INTELLIGENT RETRIEVAL COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section represents the **Output & Validation Layer.** Its goal is to transform raw data into a trustworthy business insight while providing a mechanism to measure the system's \"truthfulness.\"\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow follows a strict **Evidence-Based Generation** cycle:\n",
        "1. **Context Synthesis (**  `generate_answer_with_sources` **):** Instead of just passing text to the LLM, the system builds a \"Structured Context.\" Every chunk is prefixed with its metadata (Doc Type and Page Numbers). This forces the LLM to provide \"In-Text Citations,\" allowing a human reviewer to verify the answer against the source document.\n",
        "\n",
        "2. **Strict Constraint Enforcement:** The prompt is engineered with a \"Closed-Domain\" instruction (Answer based ONLY on provided context). This is the primary defense against hallucinations.\n",
        "\n",
        "3. **The RAG Triad Auditor (** `evaluate_rag_performance` **):** This implements an automated quality gate. By using a \"Judge LLM,\" the system evaluates itself on three critical metrics:\n",
        "    - **Faithfulness:** Does the answer stay true to the facts in the document?\n",
        "    - **Context Relevance:** Did the retriever find the right information?\n",
        "    - **Answer Relevance:** Did the response actually help the user?\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "VrLmw0oyEGfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION -------\n",
        "\n",
        "\n",
        "# --- 1. THE GENERATOR (CONTEXT-AWARE SYNTHESIS) ---\n",
        "def generate_answer_with_sources(query: str,\n",
        "                                retrieved_chunks: list) -> dict:\n",
        "    \"\"\"\n",
        "    Generate answer with detailed source attribution using the active LLM.\n",
        "    \"\"\"\n",
        "    if not retrieved_chunks:\n",
        "        return {\n",
        "            'answer': \"I couldn't find relevant information to answer your question.\",\n",
        "            'sources': [],\n",
        "            'confidence': 0.0,\n",
        "            'context_text': \"\"\n",
        "        }\n",
        "\n",
        "    # 1.1 Context Preparation\n",
        "    # Prefix every chunk with its 'Physical Provenance' (Type + Page)\n",
        "    context_parts = []\n",
        "    sources = []\n",
        "\n",
        "    for item in retrieved_chunks:\n",
        "        # LlamaIndex returns 'NodeWithScore' objects.\n",
        "        # We extract the node (text) and the score.\n",
        "        node = item.node if hasattr(item, 'node') else item\n",
        "        score = item.score if hasattr(item, 'score') else 0.0\n",
        "\n",
        "        # Extract metadata safely\n",
        "        meta = node.metadata if hasattr(node, 'metadata') else {}\n",
        "        doc_type = meta.get('doc_type', 'Document')\n",
        "        p_start = meta.get('page_start', '?')\n",
        "        p_end = meta.get('page_end', '?')\n",
        "        text_content = node.get_content() if hasattr(node, 'get_content') else str(node)\n",
        "\n",
        "        header = f\"[Source: {doc_type}, Pages {p_start}-{p_end}]\"\n",
        "        context_parts.append(f\"{header}\\n{text_content}\\n\")\n",
        "\n",
        "        sources.append({\n",
        "            'doc_type': doc_type,\n",
        "            'pages': f\"{p_start}-{p_end}\",\n",
        "            'relevance': f\"{score:.2%}\"\n",
        "        })\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "\n",
        "\n",
        "    # 1.2 Prompt Engineering\n",
        "    # Generate answer\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful AI Docyment Assistant. Use the provided context to answer the question.\n",
        "    Be specific and cite which document type and pages support your answer.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Instructions:\n",
        "    1. Answer based ONLY on the provided context\n",
        "    2. Mention which document type(s) contain the information\n",
        "    3. Be concise but complete\n",
        "    4. If the context doesn't contain enough information, say so\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use the universal LlamaIndex LLM object\n",
        "        response = Settings.llm.complete(prompt)\n",
        "\n",
        "        # Explicitly calculate avg_score using 'item' to avoid 'chunk not defined'\n",
        "        total_score = 0.0\n",
        "        for item in retrieved_chunks:\n",
        "            total_score += getattr(item, 'score', 0.0)\n",
        "        avg_score = total_score / len(retrieved_chunks)\n",
        "\n",
        "        return {\n",
        "            'answer': response.text.strip(),\n",
        "            'sources': sources,\n",
        "            'context_text': context,\n",
        "            'confidence': avg_score,\n",
        "            'chunks_used': len(retrieved_chunks)\n",
        "        }\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Answer generation error: {e}\")\n",
        "        return {\n",
        "            'answer': f\"Error generating answer: {str(e)}\",\n",
        "            'sources': sources,\n",
        "            'confidence': 0.0,\n",
        "            'context_text': context\n",
        "        }\n",
        "\n",
        "\n",
        "# --- 2. THE AUDITOR (PERFORMANCE METRICS) ---\n",
        "def evaluate_rag_performance(query, context, answer):\n",
        "    \"\"\"\n",
        "    The 'Judge LLM' logic: Evaluates the RAG Triad in JSON format.\n",
        "    Ensures high-fidelity output and detects potential hallucinations.\n",
        "\n",
        "    RAG Triad:\n",
        "    Faithfulness, Answer Relevance, and Context Relevance.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Act as an AI Quality Auditor. Rate this RAG response (1-5 scale).\n",
        "\n",
        "    Query: {query}\n",
        "    Context: {context}\n",
        "    Answer: {answer}\n",
        "\n",
        "    Rate the following from 1-5 (5 is best) in JSON format:\n",
        "    1. Faithfulness (Is the answer supported ONLY by the context?)\n",
        "    2. Context Relevance (Is the retrieved context useful for the query?)\n",
        "    3. Answer Relevance (Does the answer actually address the user's question?)\n",
        "\n",
        "    Respond ONLY in JSON: {{\"faithfulness\": 5, \"relevance\": 4, \"answer_relevance\": 5}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the universal LlamaIndex LLM object\n",
        "        response = Settings.llm.complete(prompt).text.strip()\n",
        "\n",
        "        # JSON extraction\n",
        "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            return json.loads(json_match.group())\n",
        "        else:\n",
        "            raise ValueError(\"No valid JSON found in Auditor response\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Audit Evaluation Error: {e}\")\n",
        "        # Default fallback to 0 if the LLM fails to judge itself\n",
        "        return {\"faithfulness\": 0, \"relevance\": 0, \"answer_relevance\": 0}\n",
        "\n",
        "print(\"‚úÖ SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION COMPLETE.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "d4U5Q3aOMjzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e875e49-98ae-4e49-c68d-587d4e2c978b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 8. ENHANCED ANSWER GENERATION WITH SOURCE ATTRIBUTION COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 9. ENHANCED DOCUMENT STORE**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section defines the **Central Intelligence Hub** of the platform. The `EnhancedDocumentStore` class acts as the \"Manager\" that encapsulates all previously defined logic (OCR, Classification, Chunking, and Indexing) into a single, unified object.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow follows a **State-Machine Architecture:**\n",
        "1. **Unified Ingestion Pipeline (** `process_file`**):** This is the primary entry point. It triggers a \"Hard Reset\" on every new upload to prevent data leakage between sessions. It then orchestrates the sequential flow:\n",
        "\n",
        "    *Extraction -> Boundary Detection -> Metadata-Rich Chunking -> Vector Index Building.*\n",
        "\n",
        "2. **Telemetry & Dashboarding:** Upon completion, the store generates `processing_stats`. This metadata is used to populate the UI dashboard, giving the user immediate feedback on how many \"Logical Documents\" (e.g., Invoices vs. Contracts) the AI found within their upload.\n",
        "\n",
        "3. **Semantic Query Routing (** `query` **):** This function serves as the bridge between the UI and the Retriever. It handles the \"Auto-Route\" logic, deciding whether to search the entire vector space or target a specific document silo based on the user's intent.\n",
        "\n",
        "4. **UI Data Serialization (** `get_document_structure` **):** Provides a structured summary for the Gradio interface, mapping internal dataclasses to human-readable labels and page numbers."
      ],
      "metadata": {
        "id": "mJTVS2CcHBq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 9. ENHANCED DOCUMENT STORE -------\n",
        "class EnhancedDocumentStore:\n",
        "    \"\"\"\n",
        "    Manages the complete document processing and retrieval pipeline.\n",
        "\n",
        "    Stateful manager for document processing, storage, and retrieval.\n",
        "    Orchestrates the lifecycle from raw bytes to searchable intelligence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # State Variables\n",
        "        self.pages_info = []\n",
        "        self.logical_docs = []\n",
        "        self.chunks_metadata = []\n",
        "        self.retriever = IntelligentRetriever()\n",
        "\n",
        "        # Metadata & Analytics\n",
        "        self.is_ready = False\n",
        "        self.processing_stats = {}\n",
        "        self.filename = None\n",
        "\n",
        "\n",
        "    # --- PRIMARY INGESTION PIPELINE ---\n",
        "    def process_pdf(self, pdf_file, filename: str = \"document.pdf\"):\n",
        "        \"\"\"\n",
        "        Unified processing pipeline: Orchestrates Section 4 through Section 7.\n",
        "        Handles both digital PDFs and scanned Images.\n",
        "        \"\"\"\n",
        "        self.filename = filename\n",
        "        self.is_ready = False\n",
        "        start_time = datetime.now()\n",
        "\n",
        "\n",
        "        # Step 1: File Type Routing. Get file extension\n",
        "        ext = filename.split('.')[-1].lower()\n",
        "\n",
        "        try:\n",
        "          # --- THE ROUTER LOGIC (Decision Gate) ---\n",
        "          # Step 1: Append instead of Overwrite ---\n",
        "          # Extract new info but add it to our existing lists\n",
        "          new_pages, new_logical_docs = extract_and_analyze_pdf(pdf_file)\n",
        "\n",
        "          self.pages_info.extend(new_pages)\n",
        "          self.logical_docs.extend(new_logical_docs)\n",
        "\n",
        "          # Step 2: Chunking\n",
        "          new_chunks = process_all_documents(new_logical_docs)\n",
        "          self.chunks_metadata.extend(new_chunks) # Add new chunks to the master list\n",
        "\n",
        "          # --- Update Index - Not recreate) ---\n",
        "          # Ensure build_indices function is capable of adding nodes\n",
        "          # or if using VectorStoreIndex: self.vector_index.insert_nodes(new_nodes)\n",
        "          self.retriever.build_indices(new_chunks)\n",
        "\n",
        "          # Step 4: Telemetry (Update stats for total database)\n",
        "          process_time = (datetime.now() - start_time).total_seconds()\n",
        "          self.processing_stats = {\n",
        "              'filename': filename,\n",
        "              'total_pages': len(self.pages_info), # Total pages in entire system\n",
        "              'documents_found': len(self.logical_docs),\n",
        "              'total_chunks': len(self.chunks_metadata),\n",
        "              'document_types': list(set(doc.doc_type for doc in self.logical_docs)),\n",
        "              'processing_time': f\"{process_time:.1f}s\"\n",
        "          }\n",
        "\n",
        "          self.is_ready = True\n",
        "          return True, self.processing_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, {'error': str(e)}\n",
        "\n",
        "\n",
        "\n",
        "    # --- 1. THE INGESTION ENGINE ---\n",
        "    def process_file(self, file):\n",
        "        \"\"\"\n",
        "        Executes the full pipeline: Extract -> Segment -> Chunk -> Index.\n",
        "        Ensures a hard reset to maintain data privacy between uploads.\n",
        "        \"\"\"\n",
        "\n",
        "        self.filename = os.path.basename(file.name)\n",
        "        start = time.time()\n",
        "\n",
        "        print(f\"‚öôÔ∏è Orchestrator: Starting pipeline for {self.filename}\")\n",
        "\n",
        "        # Step 1: Extract NEW content\n",
        "        new_pages, new_docs = extract_and_analyze_file(file)\n",
        "\n",
        "        # Step 2: Chunk and Append\n",
        "        new_chunks = process_all_documents(new_docs)\n",
        "\n",
        "        self.pages_info.extend(new_pages)\n",
        "        self.logical_docs.extend(new_docs)\n",
        "        self.chunks_metadata.extend(new_chunks)\n",
        "\n",
        "\n",
        "        # Step 3: Index (Append Mode)\n",
        "        self.retriever.build_indices(new_chunks)\n",
        "\n",
        "        # Step 4: Stats\n",
        "        self.processing_stats = {\n",
        "            \"filename\": self.filename,\n",
        "            \"total_pages\": len(self.pages_info),\n",
        "            \"total_chunks\": len(self.chunks_metadata),\n",
        "            \"document_types\": list(set(doc.doc_type for doc in self.logical_docs)),\n",
        "            \"processing_time\": f\"{time.time() - start:.2f}s\",\n",
        "        }\n",
        "\n",
        "        self.is_ready = True\n",
        "        return True, self.processing_stats\n",
        "\n",
        "    # --- 2. THE QUERY ENGINE ---\n",
        "    def query(self, question: str, filter_type: Optional[str] = None,\n",
        "             auto_route: bool = True, k: int = 4) -> Dict:\n",
        "        \"\"\"\n",
        "        Query the document store with automatic global fallback.\n",
        "        \"\"\"\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"Please upload and process a PDF first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        # Sanitize the filter: Convert \"All\" strings to None for global search\n",
        "        search_filter = None\n",
        "        if filter_type and str(filter_type).strip().lower() != \"all\":\n",
        "            search_filter = filter_type\n",
        "\n",
        "        # FIRST ATTEMPT: Targeted Retrieval. Retrieve relevant chunks (Section 7 - Segregated Retrieval)\n",
        "        retrieved = self.retriever.retrieve(\n",
        "            question, k=k,\n",
        "            filter_doc_type=search_filter,\n",
        "            auto_route=auto_route\n",
        "        )\n",
        "\n",
        "        # FALLBACK: If 0 results found and we used a filter, try searching EVERYTHING\n",
        "        if not retrieved and search_filter is not None:\n",
        "            print(f\"‚ö†Ô∏è No results found for silo '{search_filter}'. Falling back to Global Search...\")\n",
        "            retrieved = self.retriever.retrieve(\n",
        "                question, k=k,\n",
        "                filter_doc_type=None, # Remove the filter\n",
        "                auto_route=False      # Disable routing for the fallback\n",
        "            )\n",
        "            filter_type = \"All (Fallback)\"\n",
        "\n",
        "        # GENERATE RESPONSE - (Section 8 - Evidence-based Response)\n",
        "        # This function should take the list of nodes and return a dict with 'answer' and 'context_text'\n",
        "        result = generate_answer_with_sources(question, retrieved)\n",
        "\n",
        "        # Ensure 'retrieved_chunks' is in the dictionary so chat_with_status can see it\n",
        "        result['retrieved_chunks'] = retrieved\n",
        "        result['filter_used'] = filter_type or ('auto' if auto_route else 'none')\n",
        "\n",
        "        # Calculate a simple confidence score for the logs\n",
        "        result['confidence'] = sum([n.score for n in retrieved]) / len(retrieved) if retrieved else 0.0\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    # --- 3. UI HELPER METHODS ---\n",
        "    def get_document_structure(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get the document structure for UI display.\n",
        "        \"\"\"\n",
        "        if not self.logical_docs:\n",
        "            return []\n",
        "\n",
        "        structure = []\n",
        "        for doc in self.logical_docs:\n",
        "            structure.append({\n",
        "                'id': doc.doc_id,\n",
        "                'type': doc.doc_type,\n",
        "                'pages': f\"{doc.page_start + 1}-{doc.page_end + 1}\",  # 1-indexed for UI\n",
        "                'chunks': len(doc.chunks) if doc.chunks else 0,\n",
        "                'preview': doc.text[:200] + \"...\" if len(doc.text) > 200 else doc.text\n",
        "            })\n",
        "\n",
        "        return structure\n",
        "\n",
        "\n",
        "# --- UI FILTERING LOGIC ---\n",
        "\n",
        "def get_filtered_structure(selected_filters):\n",
        "    \"\"\"\n",
        "    selected_filters: List of strings from the Multiselect (e.g., [\"Type: Report\", \"File: my_doc.pdf\"])\n",
        "    \"\"\"\n",
        "    # 1. Get all logical documents from your store\n",
        "    # (Using the LogicalDocument dataclass from your Section 3)\n",
        "    all_docs = doc_store.logical_documents\n",
        "\n",
        "    if not selected_filters or \"All\" in selected_filters:\n",
        "        filtered = all_docs\n",
        "    else:\n",
        "        # Extract the actual values from the labels\n",
        "        type_filters = [f.replace(\"Type: \", \"\") for f in selected_filters if f.startswith(\"Type: \")]\n",
        "        file_filters = [f.replace(\"File: \", \"\") for f in selected_filters if f.startswith(\"File: \")]\n",
        "\n",
        "        filtered = [\n",
        "            d for d in all_docs\n",
        "            if d.doc_type in type_filters or os.path.basename(d.source) in file_filters\n",
        "        ]\n",
        "\n",
        "    # 2. Build the display string\n",
        "    structure_lines = [\"üß¨ FILTERED DOCUMENT STRUCTURE:\"]\n",
        "    current_file = \"\"\n",
        "    for doc in filtered:\n",
        "        fname = os.path.basename(doc.source)\n",
        "        if fname != current_file:\n",
        "            structure_lines.append(f\"\\nüìÇ FILE: {fname}\")\n",
        "            current_file = fname\n",
        "        structure_lines.append(f\"   ‚îî‚îÄ üè∑Ô∏è {doc.doc_type.upper()} | üìë Pgs: {doc.page_start + 1}-{doc.page_end + 1}\")\n",
        "\n",
        "    return \"\\n\".join(structure_lines)\n",
        "\n",
        "print(\"‚úÖ SECTION 9. ENHANCED DOCUMENT STORE COMPLETE.\")\n"
      ],
      "metadata": {
        "id": "fKXiXfaVNOIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01921641-d90f-4ec5-b331-8923f0472b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 9. ENHANCED DOCUMENT STORE COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 10. BACKEND CHAT  & AUDIT  LOGIC**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section acts as the **\"Operational Nerve Center\"** of the platform. It bridges the gap between the core AI logic and the Gradio user interface. The logic focuses on three key pillars:\n",
        "**Ground-Truth Validation, Performance Analytics, and UI State Management.**\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow consists of:\n",
        "1. **Golden Dataset Evaluation:** Defines \"Ground Truth\" question-answer pairs for specific domains (Legal, Healthcare, Real Estate). This allows the system to be benchmarked against human-verified answers rather than just \"guessing\" if a response is good.\n",
        "\n",
        "2. **Performance Auditing (** `run_performance_audit` **):** This is the \"Diagnostic Engine.\" It doesn't just display answers; it measures **LLM Speed** (tokens/sec), **Latency**, and **Context Precision**. It visualizes these metrics using Seaborn/Matplotlib to help users identify bottlenecks (e.g., is the delay in the Retriever or the LLM?).\n",
        "\n",
        "3. **Advanced Export Engine:** Implements the logic for generating professional-grade PDF reports for both Chat History and Performance Audits using `ReportLab`. This ensures that the AI's findings are \"portable\" for business stakeholders.\n",
        "\n",
        "4. **Batch Processing Handler (** `process_pdf_handler` **):** Manages the \"Multi-File\" experience. It ensures that when a user drops 5 files into the UI, they are processed in parallel, categorized into \"Smart Filters,\" and mapped to the PDF viewer without losing track of which text belongs to which file."
      ],
      "metadata": {
        "id": "txaCA0n9MNYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 10. BACKEND CHAT  & AUDIT  LOGIC -------\n",
        "\n",
        "\n",
        "# 1. GLOBAL STORE INSTANCE (Initialize The Engine)\n",
        "doc_store = EnhancedDocumentStore()\n",
        "\n",
        "\n",
        "# 2. - GOLDEN DATASETS (Define Ground-Truth) -\n",
        "# GOLDEN DATASETS to test RAG Pipleine responses with source of truth\n",
        "GOLDEN_DATASETS = {\n",
        "    \"Healthcare\": [\n",
        "        {\"question\": \"What is the primary diagnosis?\", \"golden_answer\": \"Diagnosis of Type 2 Diabetes with neuropathy.\"},\n",
        "        {\"question\": \"What are the latest lab results for Glucose?\", \"golden_answer\": \"Fasting glucose was 145 mg/dL.\"\n",
        "}\n",
        "    ],\n",
        "    \"Legal\": [\n",
        "        {\"question\": \"What is the termination notice period?\", \"golden_answer\": \"The agreement requires a 30-day written notice for termination.\"},\n",
        "        {\"question\": \"Who are the parties involved?\", \"golden_answer\": \"Between Acme Corp and John Smith.\"}\n",
        "    ],\n",
        "    \"Real Estate\": [\n",
        "        {\"question\": \"What is the total cash to close?\", \"golden_answer\": \"The estimated cash to close is $95,802.\"},\n",
        "        {\"question\": \"What is the loan amount and the interest rate?\", \"golden_answer\": \"The loan amount is $380,000 and the interest rate is 4.25%.\"},\n",
        "        {\"question\": \"Who are the applicants and what is the property address?\", \"golden_answer\": \"The applicants are John Q. Smith and Mary A. Smith. The property is 1254 Main Street, San Diego, CA 92110.\"\n",
        "}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. - AUDIT EVALUATOR FOR MISTRAL LLM -\n",
        "def evaluate_response_audit(query: str, response: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluates response quality using the current LLM.\n",
        "    Uses is_mistral logic to prevent JSON parsing errors.\n",
        "    \"\"\"\n",
        "    active_llm = Settings.llm\n",
        "    model_name = getattr(active_llm, \"model_name\", \"AI Engine\")\n",
        "    is_mistral = \"Mistral\" in model_name\n",
        "\n",
        "    raw_audit_prompt = f\"\"\"\n",
        "    Evaluate this Q&A pair:\n",
        "    Query: {query}\n",
        "    AI Response: {response}\n",
        "\n",
        "    Return ONLY JSON:\n",
        "    {{\"score\": 0.9, \"reasoning\": \"1-sentence explanation\"}}\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply Mistral tags if needed\n",
        "    final_audit_prompt = f\"[INST] {raw_audit_prompt} [/INST]\" if is_mistral else raw_audit_prompt\n",
        "\n",
        "    try:\n",
        "        raw_output = active_llm.complete(final_audit_prompt).text.strip()\n",
        "\n",
        "        # Robust JSON search\n",
        "        json_match = re.search(r'\\{.*\\}', raw_output, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            repaired = repair_json(json_match.group())\n",
        "            result = json.loads(repaired)\n",
        "            return result\n",
        "        else:\n",
        "            raise ValueError(\"Auditor output was not structured JSON\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Audit Error: {e}\")\n",
        "        return {\"score\": 0.0, \"reasoning\": \"Audit engine parsing failed.\"}\n",
        "\n",
        "\n",
        " #-- 4. PERFORMANCE AUDIT & VISUALIZATION LOGIC ---\n",
        " # Generates The Accuracy Metrics & Plots\n",
        "def run_performance_audit(doc_filter, audit_num_chunks):\n",
        "    \"\"\"\n",
        "    Calculates Speed, Chunk Metrics, and RAG Triad scores for the Audit Dashboard.\n",
        "    \"\"\"\n",
        "    if not audit_logs:\n",
        "        return \"**Avg Latency:** N/A\", {}, \"N/A\", None, [[\"No Data\", \"-\", \"-\", \"-\"]]\n",
        "\n",
        "    # Convert logs to DataFrame for filtering. Filter logs based on active UI selection\n",
        "    full_df = pd.DataFrame(audit_logs)\n",
        "\n",
        "    # 1. DEFINE SECTOR MAPPINGS\n",
        "    # This ties the UI selection to the AI's classification types\n",
        "    sector_map = {\n",
        "        \"Real Estate\": [\n",
        "            \"Mortgage Contract\", \"Land Deed\", \"Lender Fee Sheet\",\n",
        "            \"Pay Slip\", \"Tax Document\", \"W2\", \"Tax Return\" # Added financial types\n",
        "        ],\n",
        "        \"Healthcare\": [\n",
        "            \"Medical\", \"Medical Report\", \"Insurance\", \"Health Form\"\n",
        "        ],\n",
        "        \"Legal\": [\n",
        "            \"Contract\", \"Land Deed\", \"Legal Letter\", \"Form\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 2. APPLY FILTERING LOGIC\n",
        "    if doc_filter == \"All\":\n",
        "        filtered_df = full_df\n",
        "    elif doc_filter in sector_map:\n",
        "        # Filter logs where the 'Filter_Used' matches any type in the sector list\n",
        "        relevant_types = sector_map[doc_filter]\n",
        "        filtered_df = full_df[full_df['Filter_Used'].isin(relevant_types)]\n",
        "    else:\n",
        "        # Fallback for direct document type filtering (e.g., if user selects 'Invoice' directly)\n",
        "        filtered_df = full_df[full_df['Filter_Used'] == doc_filter]\n",
        "\n",
        "    # Check if we have data after filtering\n",
        "    if filtered_df.empty:\n",
        "        return (\n",
        "            f\"**No audit data found for sector: {doc_filter}**\",\n",
        "            {}, \"0%\", None, [[\"No Data\", \"-\", \"-\", \"-\"]]\n",
        "        )\n",
        "\n",
        "    # 3. CALCULATE METRICS (Using your existing logic)\n",
        "    avg_latency = filtered_df['latency'].mean() if 'latency' in filtered_df else 0.0\n",
        "\n",
        "    # Calculate Success Rate (where score > 0.7)\n",
        "    if 'audit_score' in filtered_df:\n",
        "        success_count = (filtered_df['audit_score'] > 0.7).sum()\n",
        "        success_rate = (success_count / len(filtered_df)) * 100\n",
        "    else:\n",
        "        success_rate = 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Speed: Tokens per Second (Simulated based on generation time)\n",
        "    avg_tokens = 150 # Placeholder for avg response length\n",
        "    tokens_per_sec = avg_tokens / avg_latency if avg_latency > 0 else 0\n",
        "\n",
        "\n",
        "    # METRIC 2: Context Precision (How relevant was the retrieved data?)\n",
        "    # Chunk Density: Percentage of 'audit_num_chunks' that were highly relevant\n",
        "    # We use the 'Relevance' score (0-5) to determine chunk quality\n",
        "    avg_relevance = filtered_df['Relevance'].mean()\n",
        "    context_density = (avg_relevance / 5) * 100\n",
        "\n",
        "    # VISUALIZATION: Efficiency Bottleneck Chart (with Speed Metric)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.barplot(x=['Retriever', 'LLM Speed'],\n",
        "                y=[avg_latency * 0.3, tokens_per_sec / 10], # Normalized for scale\n",
        "                hue=['Retriever', 'LLM Speed'],\n",
        "                palette=\"viridis\",\n",
        "                legend=False)\n",
        "    plt.title(f\"Efficiency Metrics (Chunks: {audit_num_chunks})\")\n",
        "    plt.savefig(\"bottlenecks.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # DATA FOR UI COMPONENT (COMPARISON TABLE)\n",
        "    audit_table_data = [\n",
        "        [\"Generation Speed\", f\"{tokens_per_sec:.1f} t/s\", \"12.5 t/s\", \"Industrial\"],\n",
        "        [\"Context Precision\", f\"{context_density:.0f}%\", \"85%\", \"Target\"],\n",
        "        [\"Avg Latency\", f\"{avg_latency:.1f}s\", \"3.0s\", \"Target\"],\n",
        "        [\"Chunk Retrieval\", f\"{audit_num_chunks}\", \"N/A\", \"Config\"]\n",
        "    ]\n",
        "\n",
        "    return (\n",
        "        f\"**Avg Latency:** {avg_latency:.2f}s | **Speed:** {tokens_per_sec:.1f} tokens/sec\",\n",
        "        {\"Faithfulness\": avg_relevance/5, \"Context Density\": context_density/100},\n",
        "        f\"{success_rate:.1f}%\",\n",
        "        f\"{context_density:.0f}%\",\n",
        "        \"bottlenecks.png\",\n",
        "        audit_table_data\n",
        "    )\n",
        "\n",
        "\n",
        "# --- 5. BATCH UPLOAD & UI STATE HANDLER  TO UPLOAD & PROCESS PDF\n",
        "def process_pdf_handler(file_list):\n",
        "    \"\"\"\n",
        "    Orchestrates the ingestion of multiple files and updates UI components.\n",
        "    Returns: (Status Message, Structure JSON, Structure Display, Filter Update, View Selector)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "      if not file_list:\n",
        "\n",
        "              # Return empty defaults for all 6 outputs\n",
        "              return (\n",
        "                \"‚ö†Ô∏è No files uploaded.\",\n",
        "                \"[]\",\n",
        "                \"\",\n",
        "                gr.update(choices=[\"All\"], value=[\"All\"]),\n",
        "                gr.update(choices=[], value=None),\n",
        "                \"No file uploaded\"\n",
        "              )\n",
        "\n",
        "      file_reports = []\n",
        "      all_doc_types = set()\n",
        "      all_filenames = []\n",
        "      view_selector_choices = []\n",
        "\n",
        "      total_pages = 0\n",
        "      total_chunks = 0\n",
        "      start_batch_time = datetime.now()\n",
        "\n",
        "      for file in file_list:\n",
        "          # full_path is the /tmp/gradio/... path needed for the PDF viewer\n",
        "          full_path = file.name\n",
        "          # fname is the clean name for the UI\n",
        "          fname = os.path.basename(full_path)\n",
        "\n",
        "          # 2. PROCESS FILE - Call the Orchestrator from Section 9\n",
        "          # Pass full_path to the engine so it can actually read the bits\n",
        "          success, stats = doc_store.process_pdf(file, filename=fname)\n",
        "\n",
        "          if success:\n",
        "              all_doc_types.update(stats.get('document_types', []))\n",
        "              all_filenames.append(fname)\n",
        "\n",
        "              # Create the (Label, Value) tuple for the dropdown\n",
        "              view_selector_choices.append((fname, full_path))\n",
        "\n",
        "              total_pages += stats.get('total_pages', 0)\n",
        "              total_chunks += stats.get('total_chunks', 0)\n",
        "\n",
        "              # Build a clean plain-text report for this specific file\n",
        "              report = (\n",
        "                f\"üíæ  {fname}\\n\"\n",
        "                f\"   ‚îî‚îÄ üìÑ Pages: {stats['total_pages']} | üß© Chunks: {stats['total_chunks']}\\n\"\n",
        "                f\"   ‚îî‚îÄ üè∑Ô∏è Types: {', '.join(stats['document_types'])}\\n\"\n",
        "                f\"   ‚îî‚îÄ ‚è±Ô∏è Time: {stats['processing_time']}\"\n",
        "              )\n",
        "\n",
        "              # Build individual file report line\n",
        "              file_reports.append(report)\n",
        "\n",
        "          else:\n",
        "            file_reports.append(f\"‚ùå {fname} | FAILED: {stats.get('error', 'Unknown Error')}\")\n",
        "\n",
        "\n",
        "      # --- DATA AGGREGATION for STRUCTURE VIEW LOGIC---\n",
        "      # 3. STRUCTURE DATA AGGREGATION - Generate Structure Visuals\n",
        "      structure_json = doc_store.get_document_structure()\n",
        "      structure_lines = [\"üß¨ GLOBAL DOCUMENT STRUCTURE:\"]\n",
        "      current_file = \"\"\n",
        "\n",
        "      for doc in structure_json:\n",
        "          doc_source = doc.get('source') or doc.get('filename') or doc.get('file_name') or \"Unknown File\"\n",
        "\n",
        "          if doc_source != current_file:\n",
        "              # Clean up path if it's a full /tmp/ path\n",
        "            display_name = os.path.basename(doc_source)\n",
        "            structure_lines.append(f\"\\nüìÇ FILE: {display_name}\")\n",
        "            current_file = doc_source\n",
        "\n",
        "          structure_lines.append(f\"   ‚îî‚îÄ üè∑Ô∏è {doc['type'].upper()} | üìë Pgs: {doc['pages']} | üß© {doc['chunks']} chunks\")\n",
        "\n",
        "      structure_display = \"\\n\".join(structure_lines)\n",
        "\n",
        "      # CONSTRUCT THE MAIN STATUS LOG\n",
        "      batch_time = (datetime.now() - start_batch_time).total_seconds()\n",
        "      joined_reports = \"\\n\\n\".join(file_reports)\n",
        "\n",
        "      status_msg = f\"\"\"\n",
        "  ================================================================\n",
        "  üìÇ BATCH PROCESSING COMPLETE ({batch_time:.1f}s)\n",
        "\n",
        "  {joined_reports}\n",
        "\n",
        "  -----------------------------------------------------------------\n",
        "  üìä TOTAL BATCH STATS:\n",
        "  Files: {len(all_filenames)} | Pages: {total_pages} | Chunks: {total_chunks}\n",
        "  =================================================================\"\"\"\n",
        "\n",
        "\n",
        "      # JSON String for the Code box\n",
        "      # Convert the list (structure_json) to a JSON string\n",
        "      # indent=2 makes it look like a pretty-printed JSON object in the UI\n",
        "      structure_json_string = json.dumps(structure_json, indent=2)\n",
        "\n",
        "      # 4. PREPARE SMART FILTERS (Types + Files)\n",
        "      # Create labels that distinguish between Document Types and Specific Files\n",
        "      unique_types = sorted(list(all_doc_types))\n",
        "      type_options = [f\"Type: {t}\" for t in unique_types]\n",
        "      file_options = [f\"File: {f}\" for f in sorted(all_filenames)]\n",
        "\n",
        "      # Dynamic UI Filter logic. Combine them into one list for the multiselect dropdown\n",
        "      smart_filter_choices = [\"All\"] + type_options + file_options\n",
        "\n",
        "\n",
        "      # Update the the Search Document Filter Dropdown\n",
        "      doc_type_filter_update = gr.update(choices=smart_filter_choices, value=[\"All\"])\n",
        "\n",
        "\n",
        "      # Update the \"Select File to View\" Dropdown\n",
        "      # choices = paths, value = the first path in the list\n",
        "      view_selector_update = gr.update(\n",
        "          choices=view_selector_choices,\n",
        "          value=view_selector_choices[0][1] if view_selector_choices else None\n",
        "      )\n",
        "\n",
        "      # 5. WIRING THE RETURN\n",
        "      # Ensure outputs match the click event:\n",
        "      # (status, json_code, textbox_display, doc_filter, view_selector, status_bar)\n",
        "      return (\n",
        "          \"\\n\\n\".join(file_reports),                         # 1. status_msg (Textbox)\n",
        "          json.dumps(structure_json, indent=2),               # 2. structure_json (Code)\n",
        "          \"\\n\".join(structure_lines),                         # 3. structure_display (Textbox)\n",
        "          gr.update(choices=smart_filter_choices, value=[\"All\"]), # 4. doc_type_filter (Multiselect)\n",
        "          gr.update(                                          # 5. view_selector (Dropdown)\n",
        "              choices=view_selector_choices,\n",
        "              value=view_selector_choices[0][1] if view_selector_choices else None\n",
        "          ),\n",
        "          f\"‚úÖ Successfully indexed {len(all_filenames)} files.\" # 6. op_status_bar (Status Label)\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Process Error: {e}\")\n",
        "        return f\"Error: {str(e)}\", \"[]\", \"‚ùå Failed\", gr.update(), gr.update(), \"Error\"\n",
        "\n",
        "\n",
        "# ------- 6. EXPORT LOGIC (ReportLab) - PERFORMANCE AUDIT REPORT EXPORT (Logic for File Generation) ------- #\n",
        "def handle_audit_export(audit_data):\n",
        "    \"\"\"\n",
        "    Logic to convert your dataframe/audit results into a PDF.\n",
        "    This is similar to your chat export but for the audit tab.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure audit_data is a DataFrame and not empty\n",
        "    if audit_data is None or (isinstance(audit_data, pd.DataFrame) and audit_data.empty):\n",
        "        return gr.update(visible=False, value=None), \"‚ö†Ô∏è No audit data available to export.\"\n",
        "\n",
        "    # Create a temporary file\n",
        "    fd, path = tempfile.mkstemp(suffix=\".pdf\")\n",
        "    os.close(fd) # Close immediately to allow ReportLab to write to it\n",
        "\n",
        "    try:\n",
        "        doc = SimpleDocTemplate(path, pagesize=letter)\n",
        "        styles = getSampleStyleSheet()\n",
        "        elements = []\n",
        "\n",
        "        # 2. Add Header\n",
        "        title_style = ParagraphStyle(\n",
        "            'Title',\n",
        "            parent=styles['Heading1'],\n",
        "            fontSize=16,\n",
        "            spaceAfter=20,\n",
        "            alignment=1  # Center alignment\n",
        "        )\n",
        "        elements.append(Paragraph(\"AI-Powered Document Intelligence - Performance Audit Report\", title_style))\n",
        "\n",
        "        # Date and Time\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "        elements.append(Paragraph(f\"Generated on: {current_time}\", styles['Normal']))\n",
        "        elements.append(Spacer(1, 20))\n",
        "\n",
        "        # 3. Process Table Data\n",
        "        # Convert DataFrame to list of lists (Header + Rows)\n",
        "        # Ensure all values are strings for ReportLab\n",
        "        data = [audit_data.columns.to_list()] + audit_data.values.tolist()\n",
        "\n",
        "        # Create the Table object\n",
        "        audit_table = Table(data, hAlign='CENTER')\n",
        "\n",
        "        # Apply Industry-Standard Styling\n",
        "        audit_table.setStyle(TableStyle([\n",
        "            ('BACKGROUND', (0, 0), (-1, 0), colors.darkslategray), # Header Background\n",
        "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),     # Header Text\n",
        "            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
        "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
        "            ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
        "            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
        "            ('BACKGROUND', (0, 1), (-1, -1), colors.whitesmoke),   # Body Background\n",
        "            ('GRID', (0, 0), (-1, -1), 1, colors.black),           # Table Grid\n",
        "            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]) # Striped rows\n",
        "        ]))\n",
        "\n",
        "        elements.append(audit_table)\n",
        "        elements.append(Spacer(1, 30))\n",
        "        elements.append(Paragraph(\"<b>End of Performance Audit Report</b>\", styles['Italic']))\n",
        "\n",
        "        # 4. Build PDF\n",
        "        doc.build(elements)\n",
        "\n",
        "        # 5. Final check and return\n",
        "        if os.path.exists(path):\n",
        "            # We return two things: the file update and the status message\n",
        "            return gr.update(value=path, visible=True, label=\"üì• Download Performance Audit Report\"), \"‚úÖ Audit report generated successfully!\"\n",
        "        else:\n",
        "            return gr.update(visible=False), \"‚ùå Error: PDF file was not created.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Process Error: {e}\")\n",
        "        # Return 6 items to match the expected Gradio outputs\n",
        "        return (\n",
        "            f\"‚ùå Error: {str(e)}\", # 1. Status Message\n",
        "            \"[]\",                  # 2. JSON Code\n",
        "            \"‚ùå Processing Failed\", # 3. Display Text\n",
        "            gr.update(),           # 4. Filter Update\n",
        "            gr.update(),           # 5. View Selector\n",
        "            \"‚ö†Ô∏è System Error\"      # 6. Status Bar\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# --- 7. REPORT GENERATION UTILITIES ---\n",
        "\n",
        "# Wrapper to combine to generate PDF, export, and download Performance Audit. Works with 'handle_audit_export' function\n",
        "def handle_audit_export_ui(audit_data):\n",
        "    \"\"\"\n",
        "    UI Wrapper: Connects the Audit Dashboard state to the PDF downloader.\n",
        "    Categorized as: UI-Backend Bridge.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Call your existing PDF generator\n",
        "    # handle_audit_export usually returns (gr.update(value=path), status_msg)\n",
        "    file_update, status_msg = handle_audit_export(audit_data)\n",
        "\n",
        "    # 2. Extract the actual string path from the dictionary\n",
        "    file_path = file_update.get(\"value\") if isinstance(file_update, dict) else file_update\n",
        "\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        # We return the STRING path for the DownloadButton\n",
        "        # and the status message for the status bar\n",
        "        return file_path, status_msg\n",
        "\n",
        "    return None, \"‚ùå Export failed: No data found.\"\n",
        "\n",
        "\n",
        "# CHAT HISTORY EXPORT (Logic for PDF File Generation) & DOWNLOAD CHAT HISTORY\n",
        "def export_chat_history_to_pdf(history):\n",
        "    \"\"\"\n",
        "    Transforms the live chat session into a formatted PDF document.\n",
        "    Categorized as: Data Serialization logic.\n",
        "    \"\"\"\n",
        "\n",
        "    if not history or len(history) == 0:\n",
        "        return None  # No file to download\n",
        "\n",
        "    try:\n",
        "        # 1. Create temporary file path\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        file_path = f\"/content/AI-Powered_Document_Intelligence_Platform_Chat_History_{timestamp}.pdf\"\n",
        "\n",
        "        # 2. Setup ReportLab PDF\n",
        "        doc = SimpleDocTemplate(file_path, pagesize=letter)\n",
        "        styles = getSampleStyleSheet()\n",
        "        elements = []\n",
        "\n",
        "        # --- NEW: CUSTOM STYLES ---\n",
        "        # Defining a specific style for the Title\n",
        "        title_style = ParagraphStyle(\n",
        "            'MainTitle',\n",
        "            parent=styles['Heading1'],\n",
        "            fontSize=20,\n",
        "            textColor=HexColor(\"#1A5276\"), # Professional Navy Blue\n",
        "            alignment=1, # 0=Left, 1=Center, 2=Right\n",
        "            spaceAfter=14\n",
        "        )\n",
        "\n",
        "        # Defining a style for the Header (Top Right Info)\n",
        "        header_info_style = ParagraphStyle(\n",
        "            'HeaderInfo',\n",
        "            parent=styles['Normal'],\n",
        "            fontSize=9,\n",
        "            textColor=colors.grey,\n",
        "            alignment=2 # Right Aligned\n",
        "        )\n",
        "\n",
        "        # ADDED THIS: Define body_style to fix your error\n",
        "        body_style = ParagraphStyle(\n",
        "            'BodyStyle',\n",
        "            parent=styles['Normal'],\n",
        "            fontSize=10,\n",
        "            leading=14)\n",
        "\n",
        "\n",
        "\n",
        "        # --- HEADER & TITLE TO ELEMENTS ---\n",
        "        # timestamp/header info at the top right\n",
        "        gen_time = datetime.now().strftime(\"%B %d, %Y | %H:%M\")\n",
        "        elements.append(Paragraph(f\"Generated on: {gen_time}\", header_info_style))\n",
        "        elements.append(Paragraph(\"AI Document Intelligence Automation Platform\", header_info_style))\n",
        "        elements.append(Spacer(1, 15))\n",
        "\n",
        "        # Main Title\n",
        "        elements.append(Paragraph(\"AI-Powered Document Intelligence Automation Platform Chat History\", title_style))\n",
        "\n",
        "        # horizontal line (visual break)\n",
        "        # Using a table with a bottom border to make a line in Platypus\n",
        "        line_table = Table([[\"\"]], colWidths=[450])\n",
        "        line_table.setStyle(TableStyle([('LINEBELOW', (0,0), (-1,-1), 1, colors.black)]))\n",
        "        elements.append(line_table)\n",
        "        elements.append(Spacer(1, 20))\n",
        "\n",
        "        # 3. BUILD CHAT CONTENT\n",
        "        for entry in history:\n",
        "            role = \"USER\" if entry['role'] == 'user' else \"AI ASSISTANT\"\n",
        "            raw_text = str(entry['content'])\n",
        "\n",
        "            # --- CLEANING LOGIC ---\n",
        "            # A. Remove the Metadata Footer (Everything from '---' onwards)\n",
        "            clean_text = raw_text.split('---')[0]\n",
        "\n",
        "            # B. Remove specific symbols and markdown markers\n",
        "            clean_text = clean_text.replace(\"**\", \"\").replace(\"‚ñ†\", \"\").replace(\"üë§\", \"\").replace(\"ü§ñ\", \"\")\n",
        "\n",
        "            # C. Clean up line breaks and leading/trailing whitespace\n",
        "            clean_text = clean_text.strip()\n",
        "\n",
        "            # D. Handle the \"Sources\" section (Optionally keep it but clean symbols)\n",
        "            clean_text = clean_text.replace(\"üîç Sources:\", \"\\n<b>Sources:</b>\")\n",
        "\n",
        "            # 3. Add to Elements\n",
        "            elements.append(Paragraph(f\"<b>{role}:</b>\", styles['Normal']))\n",
        "            elements.append(Paragraph(clean_text, body_style))\n",
        "            elements.append(Spacer(1, 5))\n",
        "\n",
        "        doc.build(elements)\n",
        "        return file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Export Error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ SECTION 10. BACKEND CHAT  & AUDIT  LOGIC COMPLETE.\")\n"
      ],
      "metadata": {
        "id": "KAtFrYkcvP_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f850a0b6-4a07-4bab-e63d-96e8a27824b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 10. BACKEND CHAT  & AUDIT  LOGIC COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 11. CHATBOT LOGIC & ORCHESTRATION**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "This section defines the **User Experience (UX) Engine** and serves as the primary controller for the platform. It is a high-concurrency environment that manages **Asynchronous Communication** and **Hardware State**.\n",
        "\n",
        "<br>\n",
        "\n",
        "The logic flow follows a **Six-Stage Execution Cycle::**\n",
        "1. **Readiness & Initialization:** Ensures the Document Store is populated and sanitizes incoming metadata filters (e.g., converting Gradio lists to LlamaIndex-compatible strings).\n",
        "\n",
        "2. **Agentic Routing & Streaming UX:** Uses Python `generators` (`yield`) to provide immediate feedback. By signaling that \"AI is thinking\" before the computation begins, it reduces \"perceived latency\" by up to 70%.\n",
        "\n",
        "3. **Hardware-Aware Switching (VRAM Purge):** The `switch_llm` function acts as a safety gate. Because the T4 GPU cannot hold two local models simultaneously, it performs a **Deep Purge** (deleting object references and clearing the CUDA cache) before calling the Factory functions from Section 2B.\n",
        "\n",
        "4. **Context-Aware Retrieval:** Executes the RAG query with robust metadata handling, ensuring that even if source documents vary in format, page numbers and document types are correctly extracted for the UI.\n",
        "\n",
        "5. **Quality Audit Gate:** Immediately triggers the \"RAG Triad\" evaluation (Section 8) to calculate Faithfulness and Relevance scores before the user sees the answer.\n",
        "\n",
        "6. **Telemetry & History Management:** Logs the entire interaction into `audit_logs` and manages the session state, allowing users to choose whether to keep or clear context when switching models.\n"
      ],
      "metadata": {
        "id": "80rvI0tlTlEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 11. CHATBOT LOGIC & ORCHESTRATION -------\n",
        "\n",
        "\n",
        "# Chat handler with status bar update. Define how the AI thinks and responds.\n",
        "def chat_with_status(message, history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "        \"\"\"\n",
        "        Combines role definitions with real-time status updates.\n",
        "        Returns: Updated history (list of dicts) and status bar text.\n",
        "\n",
        "        Stateful Chat Handler: Manages the 'Thinking Loop' and streams status updates.\n",
        "        Returns: Updated history (list of dicts) and status bar text for Gradio.\n",
        "        \"\"\"\n",
        "\n",
        "        if history is None:\n",
        "          history = []\n",
        "\n",
        "        log_entry = None\n",
        "        start_total = time.time()\n",
        "\n",
        "        # 1. INITIALIZATION & READINESS\n",
        "        # Ensure filter is a string (hashable) for the Vector Store\n",
        "        # Gradio dropdowns often pass a list like ['Contract']. LlamaIndex filters need the string 'Contract'.\n",
        "        if isinstance(doc_type_filter, list) and len(doc_type_filter) > 0:\n",
        "            clean_filter = str(doc_type_filter[0])\n",
        "        elif doc_type_filter:\n",
        "            clean_filter = str(doc_type_filter)\n",
        "        else:\n",
        "            clean_filter = \"All\"\n",
        "\n",
        "        active_filters = [clean_filter]\n",
        "        filter_label = clean_filter\n",
        "\n",
        "\n",
        "        # Check System Readiness. Check if documents exist\n",
        "        if not doc_store.is_ready:\n",
        "            response = \"üìö Please upload and process a PDF document first.\"\n",
        "            history.append({\"role\": \"user\", \"content\": f\"**üë§ You:** {message}\"})\n",
        "            history.append({\"role\": \"assistant\", \"content\": f\"**ü§ñ AI Docuement Assistant:** {response}\"})\n",
        "            yield history, \"‚ö†Ô∏è System Not Ready\"\n",
        "            return\n",
        "\n",
        "        # 2. START PROCESSING TELEMETRY\n",
        "        # Responsive UI Start. Immediate UI Feedback (Streaming Yield)\n",
        "        # Immediately tell the user the AI is working so the app feels responsive.\n",
        "        start_total = time.time()\n",
        "        routed_type = clean_filter\n",
        "        routing_confidence = 1.0\n",
        "\n",
        "\n",
        "        # 3. AGENTIC ROUTING (UI FEEDBACK LAYER)\n",
        "        if auto_route:\n",
        "            yield history, \"üéØ AI is routing your query...\"\n",
        "            # Pass Settings '.llm' to ensure it uses the engine selected in the UI\n",
        "            routed_type, routing_confidence = predict_query_document_type(message, Settings.llm)\n",
        "            clean_filter = routed_type\n",
        "            active_filters = [routed_type]\n",
        "            filter_label = routed_type\n",
        "            print(f\"‚úÖ Router assigned category: {routed_type} ({routing_confidence:.2%})\")\n",
        "\n",
        "\n",
        "        # 4. STREAMING ANALYTICS (Update UI to show the 'Silo' being searched)\n",
        "        # Responsive UI Start\n",
        "        history.append({\"role\": \"user\", \"content\": f\"**üë§ You:** {message}\"})\n",
        "        history.append({\"role\": \"assistant\", \"content\": f\"**ü§ñ AI Document Assistant is üß† Analyzing {filter_label} documents...**\"})\n",
        "        yield history, f\"‚è≥ Searching (Filter: {filter_label})...\"\n",
        "\n",
        "        # DEBUG PRINT: Verify what we are asking the database\n",
        "        print(f\"DEBUG: Querying for '{message}' with filter '{clean_filter}'\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"üîç DEBUG: Sending to Database...\")\n",
        "        print(f\"   > Query: '{message}'\")\n",
        "        print(f\"   > Applied Filter: '{clean_filter}'\")\n",
        "        print(f\"   > Search Depth (k): {audit_num_chunks}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Sanitize the filter: Force it to None if it's \"All\" or empty\n",
        "            # Ensure search_filter is None if \"All\" is selected to bypass metadata silos\n",
        "            search_filter = None if clean_filter.strip().lower() == \"all\" else clean_filter\n",
        "\n",
        "            # 5. EXECUTE CORE RETRIEVAL & GENERATION (Section 9)\n",
        "            # Limit the search depth 'k' based on the user's Audit Slider\n",
        "            result = doc_store.query(\n",
        "                message,\n",
        "                filter_type=search_filter,\n",
        "                auto_route=False, # Set to False here since we already routed above\n",
        "                k=int(audit_num_chunks) if audit_num_chunks else 5\n",
        "            )\n",
        "\n",
        "            # Extract the chunks correctly for the next step\n",
        "            # Note: your doc_store.query likely returns chunks in a key called 'retrieved_chunks'\n",
        "            chunks_to_process = result.get('retrieved_chunks', [])\n",
        "\n",
        "\n",
        "            # --- SMART FALLBACK ---\n",
        "            # If a filtered search returns 0, immediately try a global search\n",
        "            if len(chunks_to_process) == 0 and search_filter is not None:\n",
        "                print(f\"‚ö†Ô∏è Falling back to global search for: {message}\")\n",
        "                result = doc_store.query(\n",
        "                    message,\n",
        "                    filter_type=None, # Explicitly remove the filter\n",
        "                    k=int(audit_num_chunks) if audit_num_chunks else 5\n",
        "                )\n",
        "                chunks_to_process = result.get('retrieved_chunks', [])\n",
        "                clean_filter = \"All (Fallback)\"\n",
        "\n",
        "\n",
        "            # 6. Generate Answer (Now 'chunks_to_process' is defined)\n",
        "            generation_result = generate_answer_with_sources(message, chunks_to_process)\n",
        "\n",
        "\n",
        "            answer = generation_result.get('answer', \"I'm sorry, I couldn't generate an answer for that.\")\n",
        "            context_text = generation_result.get('context_text', \"\")\n",
        "\n",
        "            # 7. POST-GENERATION AUDIT (The Quality Gate)\n",
        "            # Evaluate the 'RAG Triad' immediately after generation\n",
        "            scores = evaluate_rag_performance(message, context_text, answer)\n",
        "\n",
        "            # 8. LOGGING FOR PERFORMANCE DASHBOARD\n",
        "            latency = time.time() - start_total\n",
        "\n",
        "            # Safe extraction of LLM model name for logs\n",
        "            active_llm = Settings.llm\n",
        "\n",
        "            # Check various attributes where model names might be stored\n",
        "            model_name = getattr(active_llm, \"model_name\",\n",
        "                         getattr(active_llm, \"model\",\n",
        "                         \"Gemini 2.0 Flash\")) # Final fallback\n",
        "\n",
        "            # Simple cleanup for the UI\n",
        "            if \"gemini\" in model_name.lower() and \"Flash\" not in model_name:\n",
        "                model_name = \"Gemini 2.0 Flash\"\n",
        "\n",
        "\n",
        "            # Inside chat_with_status when creating log_entry:\n",
        "            log_entry = {\n",
        "                \"Timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n",
        "                \"Model\": model_name,\n",
        "                \"Query\": message[:50],\n",
        "                \"Latency_s\": round(latency, 3),\n",
        "                \"Routed_Category\": routed_type,\n",
        "                \"Routing_Conf\": round(routing_confidence, 2),\n",
        "                \"Faithfulness\": scores.get(\"faithfulness\", 0),\n",
        "                \"Relevance\": scores.get(\"relevance\", 0),\n",
        "                \"Filter_Used\": clean_filter # Convert list to string\n",
        "            }\n",
        "\n",
        "\n",
        "            audit_logs.append(log_entry) # Ensure audit_logs = [] is defined in Section 1\n",
        "\n",
        "            # 9. FINAL UI RESPONSE CONSTRUCTION (Robust Metadata Handling)\n",
        "            retrieved_chunks = result.get('retrieved_chunks', [])\n",
        "            source_entries = []\n",
        "\n",
        "            for chunk in retrieved_chunks:\n",
        "                # --- DATA EXTRACTION ---\n",
        "                # Handles both LlamaIndex objects AND our custom FAISS tuples\n",
        "                if hasattr(chunk, 'node'):\n",
        "                    # It's a LlamaIndex-style object\n",
        "                    node_text = chunk.node.get_content()\n",
        "                    meta = chunk.node.metadata\n",
        "                elif isinstance(chunk, (list, tuple)):\n",
        "                    # A ChunkMetadata & Score tuple from FAISS\n",
        "                    node_text = chunk[0].text\n",
        "                    meta = {\n",
        "                        \"page_start\": getattr(chunk[0], 'page_start', '?'),\n",
        "                        \"doc_type\": getattr(chunk[0], 'doc_type', 'Document')\n",
        "                    }\n",
        "                else:\n",
        "                    meta = {}\n",
        "\n",
        "                # Try to find the page number from common keys\n",
        "                page = meta.get('page_start', meta.get('page_label', meta.get('page_num', '?')))\n",
        "                doc_type = meta.get('doc_type', 'Document')\n",
        "\n",
        "                source_entries.append(f\"{doc_type} (p.{page})\")\n",
        "\n",
        "            # Unique sources only to avoid clutter\n",
        "            unique_sources = list(set(source_entries))\n",
        "            sources_text = \"\\n\\nüîç **Sources:** \" + \", \".join(unique_sources) if unique_sources else \"\\n\\n‚ö†Ô∏è **No relevant data found.**\"\n",
        "\n",
        "            # --- UPDATE CONFIDENCE CALCULATION ---\n",
        "            # Ensure we calculate confidence correctly based on the object type\n",
        "            def get_score(c):\n",
        "                if hasattr(c, 'score'): return c.score\n",
        "                if isinstance(c, (list, tuple)) and len(c) > 1: return c[1]\n",
        "                return 0.0\n",
        "\n",
        "            raw_confidence = sum([get_score(c) for c in retrieved_chunks]) / len(retrieved_chunks) if retrieved_chunks else 0.0\n",
        "\n",
        "\n",
        "\n",
        "            # Metadata Footer\n",
        "            stats_text = f\"\\n\\n---\\n*‚è±Ô∏è {latency:.2f}s | ü§ñ Engine: {model_name} | ‚úÖ Faithfulness: {scores.get('faithfulness', 0)}/5*\"\n",
        "\n",
        "            # Confidence & Filter Display\n",
        "            # Use the actual filter applied by the doc_store\n",
        "            applied_filter = result.get('filter_used', clean_filter)\n",
        "            metadata = f\"\\n\\n*Confidence: {raw_confidence:.1%} | Filter Used: {applied_filter}*\"\n",
        "\n",
        "            # Clean up double newlines and special characters for a cleaner UI\n",
        "            clean_answer = answer.replace('‚ñ†', '').strip()\n",
        "            full_response = f\"ü§ñ **AI Document Assistant:** {clean_answer}{sources_text}{stats_text}{metadata}\"\n",
        "\n",
        "            # Update the last entry (the placeholder) with the final answer\n",
        "            # We target the last item in the list which is the assistant's placeholder\n",
        "            history[-1] = {\"role\": \"assistant\", \"content\": full_response}\n",
        "\n",
        "            # Final yield to close the loop\n",
        "            yield history, \"‚úÖ Response Generated\"\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"**ü§ñ AI Document Assistant:** ‚ö†Ô∏è Error: {str(e)}\"\n",
        "            print(f\"‚ùå Chat Error: {str(e)}\")\n",
        "            history[-1] = {\"role\": \"assistant\", \"content\": error_msg}\n",
        "            yield history, \"‚ùå Search Failed\"\n",
        "\n",
        "# --- 2. CENTRAL SWITCHING & VRAM MANAGEMENT ---\n",
        "def deep_purge_gpu():\n",
        "    import gc\n",
        "    import torch\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Remove the global reference safely\n",
        "    if 'current_llm' in globals():\n",
        "        # Global not deleted\n",
        "        # Set to None to allow the next load to happen cleanly.\n",
        "        pass\n",
        "    print(\"üßπ GPU Memory Purged.\")\n",
        "\n",
        "def switch_llm(model_choice):\n",
        "    \"\"\"\n",
        "    The technical engine that loads the model.\n",
        "    Orchestrates the transition between LLMs while strictly managing T4 GPU VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    # Declare current_llm global at the very start\n",
        "    global current_llm, current_model_name, audit_logs\n",
        "\n",
        "\n",
        "\n",
        "    # Optimization: Prevent redundant loading of the same model\n",
        "    if model_choice == current_model_name:\n",
        "        return f\"‚úÖ {model_choice} is already active.\"\n",
        "\n",
        "    print(f\"üîÑ Switching to {model_choice}...\")\n",
        "\n",
        "    # --- MEMORY MANAGEMENT (PURGE- Memory Safety) ---\n",
        "    # Essential for T4 GPU reliability in Google Colab\n",
        "    # Clear Memory to prevent OOM (Out of Memory) Errors\n",
        "    if current_llm:\n",
        "        print(\"üßπ Purging previous model from VRAM...\")\n",
        "\n",
        "        try:\n",
        "            del current_llm\n",
        "            deep_purge_gpu()\n",
        "            time.sleep(1) # Allow VRAM to stabilize\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Purge warning: {e}\")\n",
        "    # -------------------------------\n",
        "\n",
        "\n",
        "    # --- DYNAMIC LOADING LOGIC ---\n",
        "    try:\n",
        "        if model_choice == \"Gemini 2.0\":\n",
        "            current_llm = setup_gemini_llm()\n",
        "            # Use object.__setattr__ to bypass Pydantic's \"No field model_name\" error\n",
        "            object.__setattr__(current_llm, 'model_name', \"Gemini 2.0 Flash\")\n",
        "\n",
        "        elif model_choice == \"Mistral 7B\":\n",
        "            current_llm = setup_mistral_7b_llm()\n",
        "            object.__setattr__(current_llm, 'model_name', \"Mistral 7B\")\n",
        "\n",
        "        elif model_choice == \"Phi-2\":\n",
        "            current_llm = setup_phi2_llm()\n",
        "            object.__setattr__(current_llm, 'model_name', \"Microsoft Phi-2\")\n",
        "\n",
        "        elif model_choice == \"TinyLlama\":\n",
        "            current_llm = setup_tinyllama_llm()\n",
        "            object.__setattr__(current_llm, 'model_name', \"TinyLlama 1.1B\")\n",
        "\n",
        "        # 4. Re-Sync Global Settings\n",
        "        current_model_name = model_choice\n",
        "        Settings.llm = current_llm\n",
        "\n",
        "        # 5. Logging & Audit\n",
        "        display_name = getattr(current_llm, 'model_name', model_choice)\n",
        "        audit_logs.append({\n",
        "            \"Timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n",
        "            \"Event\": \"LLM_SWITCH\",\n",
        "            \"Details\": f\"Active Engine: {display_name}\"\n",
        "        })\n",
        "\n",
        "        return f\"üöÄ Active Engine: {model_choice}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_choice}: {e}\")\n",
        "        return f\"‚ùå Failed to load {model_choice}\"\n",
        "\n",
        "\n",
        "def handle_model_transition(new_choice, current_history, should_clear):\n",
        "    \"\"\"\n",
        "    The UI manager that talks to Gradio. Backend connector for the UI dropdown.\n",
        "    Switches the LLM and manages the Chatbot state.\n",
        "    \"\"\"\n",
        "    # Switch the actual model in the backend\n",
        "    switch_status = switch_llm(new_choice)\n",
        "\n",
        "    # Decide what to do with the UI state\n",
        "    if should_clear:\n",
        "        # Clear the history list and show an alert\n",
        "        return [], f\"‚úÖ {switch_status} | System Reset. History Cleared\"\n",
        "\n",
        "    # Keep the history and show a status update\n",
        "    return current_history, f\"‚úÖ {switch_status} | Context Retained. History Kept.\"\n",
        "\n",
        "print(\"‚úÖ SECTION 11. CHATBOT LOGIC & ORCHESTRATION Complete\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BFK3Gn4dNgOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0037d4d7-9528-415b-be15-9d7d9d6604b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SECTION 11. CHATBOT LOGIC & ORCHESTRATION Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 12. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "Section 12 is the **Nervous System** of the application. While previous sections defined how the AI \"thinks\" (Logic) and \"remembers\" (Vector Store), this section defines how the AI **\"interacts\"**. It uses the Gradio library to build a professional-grade web interface entirely in Python.\n",
        "\n",
        "<br>\n",
        "\n",
        "The core of this section is **Component Wiring**. In a complex RAG application, a single button click (like \"Process Document\") must trigger a chain reaction:\n",
        "\n",
        "1. **Frontend:** The button goes into a \"loading\" state.\n",
        "\n",
        "2. **Middle-ware:** The `process_pdf_handler` is called.\n",
        "\n",
        "3. **State Management:** The `doc_store` is updated, the `viewer_state` is populated with images, and the `doc_type_filter` dropdown is refreshed with new document silos.\n",
        "\n",
        "4. **Backend:** Metadata is extracted and formatted into JSON for the developer view.\n",
        "\n",
        "5. **Return:** The UI updates 6+ different components simultaneously."
      ],
      "metadata": {
        "id": "2pYWo7wCa47E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 12. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC -------\n",
        "\n",
        "\n",
        "# --- JavaScript is for behavior (Auto-Scroll) ---\n",
        "scroll_script = \"\"\"\n",
        "function() {\n",
        "    const targetNode = document.querySelector('#chatbot-box');\n",
        "    if (!targetNode) {\n",
        "        console.log(\"Chatbot box not found yet...\");\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    const observer = new MutationObserver(() => {\n",
        "        // In newer Gradio, the scrollable area is usually a 'div' inside the chatbot\n",
        "        const scrollContainer = targetNode.querySelector('.scrollable-auto') || targetNode.querySelector('.wrapper') || targetNode;\n",
        "        if (scrollContainer) {\n",
        "            scrollContainer.scrollTo({\n",
        "                top: scrollContainer.scrollHeight,\n",
        "                behavior: 'smooth'\n",
        "            });\n",
        "        }\n",
        "    });\n",
        "\n",
        "    observer.observe(targetNode, { childList: true, subtree: true });\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------- UI LAYOUT  ------------------------------------------------------------------------------------- #\n",
        "\n",
        "# CSS: We target only the 'header-container' for centering\n",
        "# CSS: Targets only the tab navigation bar to make it look like a black menu\n",
        "# Targets Download & Export buttons for Chat History & Performance Audit Report\n",
        "custom_css = \"\"\"\n",
        "    /* Center the header text */\n",
        "    .welcome-text-header-container {\n",
        "        text-align: center;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "\n",
        "    /* 1. FORCE ALL BUTTONS TO BLACK */\n",
        "    /* This targets primary buttons, secondary buttons, and specific IDs */\n",
        "    button.primary, button.secondary, #dark-btn, #chat-export-btn, #ingest_btn, #send_btn {\n",
        "        background-color: black !important;\n",
        "        background: black !important;\n",
        "        color: white !important;\n",
        "        border: 1px solid #444 !important;\n",
        "        box-shadow: none !important;\n",
        "    }\n",
        "\n",
        "    /* Button Hover Effect */\n",
        "    button.primary:hover, button.secondary:hover {\n",
        "        background-color: #222 !important;\n",
        "        border-color: #00d1b2 !important;\n",
        "    }\n",
        "\n",
        "    /* 2. NAVIGATION BAR (TABS) STYLING */\n",
        "    /* The horizontal strip background */\n",
        "    .tabs > .tab-nav {\n",
        "        background-color: black !important;\n",
        "        border-bottom: 2px solid #333 !important;\n",
        "        padding: 8px 10px 0px 10px !important;\n",
        "        display: flex !important;\n",
        "        gap: 5px !important;\n",
        "        border-radius: 8px 8px 0 0 !important;\n",
        "    }\n",
        "\n",
        "    /* Individual Tab Labels (Inactive) */\n",
        "    .tabs > .tab-nav > button {\n",
        "        background-color: #111 !important; /* Very dark grey for inactive */\n",
        "        color: white !important;           /* White font */\n",
        "        border: none !important;\n",
        "        border-radius: 5px 5px 0 0 !important;\n",
        "        padding: 10px 25px !important;\n",
        "        font-weight: bold !important;\n",
        "    }\n",
        "\n",
        "    /* The Active (Selected) Tab */\n",
        "    .tabs > .tab-nav > button.selected {\n",
        "        background-color: black !important; /* Pure black for active */\n",
        "        color: #00d1b2 !important;           /* Highlight font color */\n",
        "        border-bottom: 3px solid #00d1b2 !important;\n",
        "    }\n",
        "\n",
        "    /* LABELS (The specific fix you requested) */\n",
        "    .gradio-container .label {\n",
        "        background-color: black !important;\n",
        "        color: white !important;\n",
        "        padding: 4px 10px !important;\n",
        "        border-radius: 5px 5px 0 0 !important;\n",
        "        border: 1px solid #444 !important;\n",
        "        box-shadow: none !important;\n",
        "        font-weight: bold !important;\n",
        "    }\n",
        "\n",
        "    .control-frame {\n",
        "    border: 1px solid #e0e0e0;\n",
        "    border-radius: 12px;\n",
        "    padding: 20px;\n",
        "    background-color: #fcfcfc;\n",
        "    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .section-divider {\n",
        "        border-top: 2px solid #3b82f6;\n",
        "        margin: 15px 0;\n",
        "        opacity: 0.5;\n",
        "    }\n",
        "\n",
        "    .vertical-divider {\n",
        "    border-right: 2px solid #e0e0e0;\n",
        "    height: 90vh; /* Fills most of the vertical screen */\n",
        "    margin: 0 20px;\n",
        "    align-self: center;\n",
        "}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def create_interface():\n",
        "    # Load custom CSS for the 'Obsidian' black-and-teal theme\n",
        "    with gr.Blocks(title=\"AI-Powered Document Intelligence Automation Platform\") as demo:\n",
        "      # --- 1. HEADER SECTION(CENTERED) ---\n",
        "      with gr.Column(elem_classes=\"welcome-text-header-container\"):\n",
        "          gr.Markdown(\"# ü§ñ AI-Powered Document Intelligence Automation Platform\")\n",
        "          gr.Markdown(\"### Providing assistance with document search. ‚ú®\")\n",
        "          gr.Markdown(\"üìÇ Upload & Process Multi-page PDF or Scanned Image and then enter search request in chatbot\")\n",
        "          gr.Markdown(\"Accepted fiile formats: .pdf, .png, .jpg, .jpeg\")\n",
        "          gr.HTML(\"<hr style='border: 1px solid #e0e0e0;'>\")\n",
        "\n",
        "      # --- 2. THREE-PILLAR NAVIGATION TABS ---\n",
        "      with gr.Tabs() as tabs:\n",
        "# -------- --- TAB 1: OPERATIONS (CHATTING & VIEWING) ---\n",
        "          with gr.TabItem(\"üí¨ Chat Operations\", id=\"chat_tab\"):\n",
        "             with gr.Row():\n",
        " # -----------     --------   # TAB 1: OPERATIONAL CORE (LEFT TOP COLUMN: UI IMAGE)\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Image( # AI-Powered Document Assistant logo v2.png\n",
        "                        value=LOGO_PATH,\n",
        "                        width=100,\n",
        "                        show_label=False,\n",
        "                        container=False,\n",
        "                        scale=1)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        " # -----------     --------    # TAB 1 - LEFT COLUMN: LARGE LANGUAGE MODEL (LLM) SELECTION\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"# üß† Large Language Models (LLM)\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      # Status indicator\n",
        "                      engine_status = gr.Markdown(\"*Status: Ready*\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      # Create the choice button\n",
        "                      llm_selector = gr.Dropdown(\n",
        "                          choices=[\"Gemini 2.0\", \"Mistral 7B\", \"Phi-2\", \"TinyLlama\"],\n",
        "                          label=\"Select LLM Engine\",\n",
        "                          value=\"Gemini 2.0\",\n",
        "                          scale=1,\n",
        "                          container=False)\n",
        "                      clear_on_switch_checkbox = gr.Checkbox(label=\"Clear History on Switch\")\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "\n",
        "                    # DOCUMENT PROCESSING CENTRAL\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"## üìÇ Document Processing Central\")\n",
        "\n",
        "                    # FILE_UPLOAD, INGEST_BTN (PROCESS DOCUMENT), CLEAR_ALL_BTN, DOC_TYPE_FILTER, STATUS_OUTPUT\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"Upload file(s) and press Process Document button\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      file_upload = gr.File(\n",
        "                        label=\"Upload Multi-page PDF or Scanned Image\",\n",
        "                        file_count=\"multiple\", #Enable muliple\n",
        "                        file_types=[\".pdf\", \".png\", \".jpg\", \".jpeg\"],\n",
        "                        interactive=True,\n",
        "                        type=\"filepath\")\n",
        "\n",
        "\n",
        "                    with gr.Row():\n",
        "                      ingest_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\", interactive=True, scale=1 )\n",
        "                      clear_all_btn = gr.Button(\"üóëÔ∏è Clear All\", variant=\"primary\", interactive=True, scale=1)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # PROCESSING STATUS & METADATA\n",
        "                    with gr.Row():\n",
        "                      # This shows the bullet points of document pages created in 'structure_display' string in def 'process_pdf_handler'.\n",
        "                      status_output = gr.Textbox(\n",
        "                          label=\"Processing Status & Metadata\",\n",
        "                          lines=15, # Increased height\n",
        "                          elem_classes=\"status-window\",\n",
        "                          interactive=False,\n",
        "                          placeholder=\"Technical details will appear here after upload...\",\n",
        "                          visible=True,\n",
        "                          elem_id=\"status-box\")   # ID for custom styling\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # VIEW DOCUMENT\n",
        "                    with gr.Row():\n",
        "                      gr.Markdown(\"# üìÑ Document Preview\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                      # View the PDF pages as images in the UI\n",
        "                      # Fixed height viewer prevents layout shifts\n",
        "                      doc_viewer = gr.Image(\n",
        "                              label=\"Page Viewer\",\n",
        "                              type=\"pil\",\n",
        "                              interactive=False,\n",
        "                              height=550)\n",
        "\n",
        "                    with gr.Row():\n",
        "                      prev_btn = gr.Button(\"‚¨ÖÔ∏è Previous\", scale=1)\n",
        "                      # Indicator shows: Page 1 of 10\n",
        "                      next_btn = gr.Button(\"Next ‚û°Ô∏è\", scale=1)\n",
        "\n",
        "                    with gr.Row():\n",
        "                      page_indicator = gr.Markdown(\"## <center>Page 0 of 0</center>\")\n",
        "\n",
        "                      op_status_bar = gr.Markdown(\"**Status:** Ready\")\n",
        "                      # Hidden State to store the PDF pages and current index\n",
        "                      viewer_state = gr.State({\"current_page\": 0, \"images\": []})\n",
        "                      filename_debug_output = gr.Textbox(label=\"Uploaded Filename (Debug)\", visible=False, lines=1, interactive=False) # ADDED DEBUG TEXTBOX\n",
        "\n",
        "\n",
        "\n",
        "# -----------     --------   # TAB 1 - RIGHT COLUMN: AI-POWERED DOCUMENT INTELLIGENCE CHATBOT INTERFACE\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Markdown(\"## AI-Powered Document Intelligence Chatbot\")\n",
        "\n",
        "                    # Chatbot Design\n",
        "                    chatbot = gr.Chatbot(\n",
        "                      label=\"AI Document Assistant\",\n",
        "                      height=1000,\n",
        "                      show_label=True,\n",
        "                      value=[{\"role\": \"assistant\", \"content\": \"**ü§ñ AI Document Assistant:** üëã Welcome! Upload files in the üìÇ Upload & Process Documents tab to begin. üöÄ\"}],\n",
        "                      elem_id=\"chatbot-box\",\n",
        "                      autoscroll=True,\n",
        "                      render_markdown=True) # Processes the **bold** text\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        msg_input = gr.Textbox(show_label=False, placeholder=\"Ask a question about your docs...\", scale=8, container=True)\n",
        "\n",
        "                    with gr.Row():\n",
        "                        send_btn = gr.Button(\"üöÄSend\", scale=1, variant=\"primary\", interactive=True)\n",
        "                        chat_download_btn = gr.DownloadButton(\n",
        "                               \"üì§ Download Chat History (PDF)\", # Button the user clicks to start the export\n",
        "                               visible=True,\n",
        "                               interactive=True,\n",
        "                               elem_id=\"chat-export-btn\",\n",
        "                               variant=\"primary\",\n",
        "                               scale=1)\n",
        "\n",
        "                        # This component holds the actual file once generated\n",
        "                        # Visible=False until the file is ready\n",
        "                        chat_download_file = gr.File(label=\"Download Ready\", visible=False, scale=1)\n",
        "\n",
        "                    with gr.Row():\n",
        "                        example_btn1 = gr.Button(\"üìù Summary\", variant=\"primary\", interactive=True, scale=1)\n",
        "                        example_btn2 = gr.Button(\"üí∞ Find Amounts\", variant=\"primary\", interactive=True, scale=1)\n",
        "                        clear_chat_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"primary\", interactive=True, scale=1)\n",
        "\n",
        "# -----------  # --- TAB 2: RAG CONFIGURATIONS & DOCUMENT(s) FILTERS ---\n",
        "          with gr.TabItem(\"‚öôÔ∏è Configurations & üìÇ Filters\", id=\"Config_filters_tab\"):\n",
        "             with gr.Row():\n",
        "\n",
        "\n",
        " # -----------     --------   # TAB 2 - LEFT COLUMN: DOCUMENT STRUCTURE\n",
        "                with gr.Column(scale=2):\n",
        "                    # DOCUMENT STRUCTURE\n",
        "                    gr.Markdown(\"# üß¨ Processed Document Breakdown\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                          gr.Markdown(\"\"\"\n",
        "                            These view displays the breakdown of your file. Our system identifies\n",
        "                            identifies distinct sub-documents types (e.g., an Invoice followed by a Lease)\n",
        "                            within a single upload, mapping the specific page ranges and initial content previews\n",
        "                            to ensure the retriever (search) knows exactly where each piece of information originated.\n",
        "                            \"\"\")\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # Human-Readable Text output\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"### Document Structure\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"Identifies distinct sub-documents and page ranges within your file.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        structure_output_textbox = gr.Textbox(label=\"Text Output\", visible=True, scale=3, lines=8)\n",
        "\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "                    # Developer JSON output\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"### Developer Document Structure\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"Machine-ready schema for debugging and database integration.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                         # This shows the actual raw JSON data \"ADDED code\" in def 'process_pdf_handler'.\n",
        "                        structure_output_code = gr.Code(label=\"JSON Output\", language=\"json\", lines=25, interactive=False, elem_id=\"structure-json-box\")\n",
        "\n",
        "\n",
        " # -----------     --------   # TAB 2 - RIGHT COLUMN: FILTERS  & RAG CONFIGURATIONS\n",
        "                with gr.Column(scale=2):\n",
        "                    gr.Image( # Document Filter and RAG.png\n",
        "                        value=CONFIG_FILTER_PATH,\n",
        "                        show_label=False,\n",
        "                        container=False,\n",
        "                        scale=1)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    # FILTER DOCUMENTS\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"# Filters\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"Filter AI Document Assistant responses or document preview by File or Document Type.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        view_selector = gr.Dropdown(label=\"Select File to View\", choices=[],scale=2)\n",
        "                    with gr.Row():\n",
        "                        doc_type_filter = gr.Dropdown(\n",
        "                            choices=[\"All\"],\n",
        "                            label=\"Filter By Document (File) & Type:\",\n",
        "                            value=\"All\",\n",
        "                            interactive=True,\n",
        "                            multiselect=True,\n",
        "                            scale=2)\n",
        "\n",
        "                    # --- DIVIDER ---\n",
        "                    gr.HTML(\"<hr>\")\n",
        "\n",
        "\n",
        "                    with gr.Row():\n",
        "                        # RIGHT COLUMN: RAG CONFIGURATION ROW\n",
        "                        gr.Markdown(\"# ‚öôÔ∏è RAG Configuration\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        gr.Markdown(\"To refine AI Document Assistant response, adjust Recall Chunks.\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        auto_route = gr.Checkbox(value=True, label=\"üéØ Auto-Route Queries\")\n",
        "\n",
        "                    with gr.Row():\n",
        "                        audit_num_chunks = gr.Slider(\n",
        "                                  minimum=1,\n",
        "                                  maximum=10,\n",
        "                                  value=4,\n",
        "                                  step=1,\n",
        "                                  label=\"üìä Recall Chunks\",\n",
        "                                  info=\"Determines how many chunks are analyzed for precision.\")\n",
        "\n",
        "\n",
        "# -----------    # --- TAB 3: AUDIT & GROUND TRUTH ---\n",
        "          with gr.TabItem(\"‚öñÔ∏è Performance Audit\", id=\"audit_tab\"):\n",
        "\n",
        "              #Blank row for Spacing\n",
        "              with gr.Row():\n",
        "\n",
        "\n",
        "                  # LEFT COLUMN: SECTOR FILTER PERFORMANCE AUDIT\n",
        "                  with gr.Column(scale=1):\n",
        "                      gr.Markdown(\"### ‚öôÔ∏è PERFORMANCE AUDIT CONFIGURATIONS\")\n",
        "\n",
        "                      with gr.Row():\n",
        "                        sector_dropdown = gr.Dropdown(\n",
        "                        choices=[\"Real Estate\", \"Healthcare\", \"Legal\", \"All\"], # Can be optimized in future enhancements for other domains\n",
        "                        label=\"Select Performance Audit Sector\",\n",
        "                        value=\"All\",\n",
        "                        interactive=True,\n",
        "                        visible=True\n",
        "                      )\n",
        "\n",
        "                      # Left-Middle-Top: GRAPHS\n",
        "                      with gr.Row():\n",
        "                        run_audit_btn = gr.Button(\"üèÅ Run Performance Audit\", variant=\"primary\", scale=1)\n",
        "                        audit_download_btn = gr.DownloadButton(\"üìÑ Download Performance Audit Report (PDF)\", variant=\"primary\", scale=1)\n",
        "                        audit_download_file = gr.File(label=\"üìÑ Download Performance Audit Report (PDF)\", visible=False, interactive=False, container=True)\n",
        "\n",
        "                  # RIGHT COLUMN: VISUALIZATION PERFORMANCE AUDIT METRICS\n",
        "                  with gr.Column(scale=6):\n",
        "                        gr.Markdown(\"# ‚öñÔ∏è Performance Audit\")\n",
        "\n",
        "                        # HEADING\n",
        "                        with gr.Column(scale=6):\n",
        "                            gr.Markdown(\"### ‚öôÔ∏è MONITORING & PERFORMANCE DASHBOARD\")\n",
        "                            gr.Markdown(\"### üõ†Ô∏è Industry Ground Truth Evaluation\")\n",
        "                            gr.Markdown(\"This dashboard translates raw AI-Judge scores into Permonace Audit status.\")\n",
        "                            gr.Markdown(\"--------------------------------------------------------------------------------\")\n",
        "                            gr.Markdown(\"### üìä Live Performance\")\n",
        "                            gr.Markdown(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "                        # RIGHT COLUMN: VISUALIZATION PERFORMANCE AUDIT METRICS\n",
        "                        with gr.Column(scale=6):\n",
        "\n",
        "                          # Right-Top: LIVE PERFORMANCE\n",
        "                          with gr.Row():\n",
        "                            latency_stat = gr.Markdown(\"**Avg Latency:** -- | **Speed:** --\")\n",
        "                          with gr.Row():\n",
        "                            audit_accuracy_gauge = gr.Label(label=\"RAG Triad & Context Metrics\")\n",
        "                            accuracy_gauge = gr.Label(label=\"Context Density Score\")\n",
        "                            bottleneck_plot = gr.Image(label=\"Latency vs. Token Speed\")\n",
        "\n",
        "                          # Right-Middle-Top: AUDIT TABLE\n",
        "                          with gr.Row():\n",
        "                            audit_table = gr.Dataframe(\n",
        "                              headers=[\"Metric\", \"Current Audit\", \"Industry Benchmark\", \"Status\"],\n",
        "                              value=[]\n",
        "                          )\n",
        "\n",
        "\n",
        "          # Bottom: GLOBAL STATUS BAR (Visible across all tabs)\n",
        "          op_status_bar = gr.Markdown(\n",
        "                value=\"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\",\n",
        "                elem_id=\"op_status_bar\"\n",
        "          )\n",
        "\n",
        "# ----------------------------------------------- COMPONENTS WIRING (Defined with chat_interface) ------------------------------------------------------------------------------------- #\n",
        "\n",
        "      # Chat Event handlers\n",
        "      def update_status_bar():\n",
        "            \"\"\"Update the status bar with current statistics.\"\"\"\n",
        "            if doc_store.is_ready:\n",
        "                stats = doc_store.processing_stats\n",
        "                cache_rate = 0\n",
        "                if hasattr(doc_store.retriever, 'total_queries') and doc_store.retriever.total_queries > 0:\n",
        "                    cache_rate = (doc_store.retriever.cache_hits / doc_store.retriever.total_queries) * 100\n",
        "\n",
        "                return f\"**Status:** ‚úÖ Ready | **Documents:** {stats.get('documents_found', 0)} | **Chunks:** {stats.get('total_chunks', 0)} | **Cache Rate:** {cache_rate:.0f}%\"\n",
        "            return \"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\"\n",
        "\n",
        "\n",
        "\n",
        "      def clear_all():\n",
        "          \"\"\"Clear everything and reset the interface.\"\"\"\n",
        "          global doc_store, audit_logs\n",
        "          doc_store = EnhancedDocumentStore()\n",
        "          audit_logs = []\n",
        "\n",
        "          # Return 14 values to match your specific UI layout\n",
        "          return (\n",
        "              [],                                 # 1. chatbot\n",
        "              None,                               # 2. file_upload\n",
        "              \"\",                                 # 3. chat_input\n",
        "              None,                               # 4. doc_viewer (Now gr.Image, so return None)\n",
        "              \"\",                                 # 5. structure_output_textbox\n",
        "              \"\",                                 # 6. structure_output_code\n",
        "              \"\",                                 # 7. extra status\n",
        "              gr.update(choices=[], value=None),  # 8. doc_type_filter\n",
        "              gr.update(choices=[], value=None),  # 9. view_selector\n",
        "              pd.DataFrame(),                     # 10. audit_table\n",
        "              None,                               # 11. audit_download_btn\n",
        "              \"üîÑ System Reset\",                  # 12. op_status_bar\n",
        "              {\"current_page\": 0, \"images\": []}, # 13. viewer_state (Reset State)\n",
        "              \"**Page 0 of 0**\"                  # 14. page_indicator (Reset Markdown)\n",
        "          )\n",
        "\n",
        "\n",
        "      def process_pdf_with_status(file_list):\n",
        "            \"\"\"Processes uploaded file and ensures UI doesn't hang on error.\"\"\"\n",
        "            try:\n",
        "                # Calls your existing handler from Section 11;\n",
        "                status, structure_json_string, structure_display, doc_type_filter, view_selector, filename_summary = process_pdf_handler(file_list)\n",
        "\n",
        "                # UI Gloabl Status Bar\n",
        "                status_bar_text = update_status_bar()\n",
        "\n",
        "                return status, structure_json_string, structure_display, view_selector, doc_type_filter, status_bar_text, filename_summary\n",
        "\n",
        "                return (\n",
        "                    status,                     # -> status_output\n",
        "                    structure_json_string,      # -> structure_output_code\n",
        "                    structure_display,          # -> structure_output_textbox\n",
        "                    gr.update(choices=filter_choices, value=\"All\"), # For search filter\n",
        "                    gr.update(choices=file_paths, value=file_paths[0] if file_paths else None), # For viewer\n",
        "                    view_selector,\n",
        "                    f\"‚úÖ {len(all_filenames)} Files Ready\"\n",
        "                )\n",
        "\n",
        "            except Exception as e:\n",
        "                # Debugging print to see exactly what happened in Colab logs\n",
        "                print(f\"Error in wrapper: {str(e)}\")\n",
        "                return f\"‚ùå System Error: {str(e)}\",\"[]\", \"‚ö†Ô∏è Error\", gr.update(choices=[\"All\"]), \"Error\", gr.update(choices=[])\n",
        "\n",
        "\n",
        "      # UI Buttons: SUMMARY, FIND AMOUNTS,CLEAR CHAT\n",
        "      # Define Example question handlers\n",
        "      def ask_summary(history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "          \"\"\"Specific wrapper for the Summary button.\"\"\"\n",
        "\n",
        "          msg = \"Can you provide a summary of the main points in this document?\"\n",
        "\n",
        "          if history is None or (len(history) > 0 and not isinstance(history[0], dict)):\n",
        "            history = []\n",
        "\n",
        "          # We loop through the generator to get the final yielded history\n",
        "          final_history = history\n",
        "\n",
        "          # Drains the generator from chat_with_status\n",
        "          for updated_history, status in chat_with_status(msg, history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "              final_history = updated_history\n",
        "\n",
        "          return final_history\n",
        "\n",
        "\n",
        "      def ask_amounts(history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "          \"\"\"Specific wrapper for the Find Amounts button.\"\"\"\n",
        "\n",
        "          msg = \"What are all the monetary amounts or financial figures mentioned?\"\n",
        "\n",
        "          if history is None or (len(history) > 0 and not isinstance(history[0], dict)):\n",
        "            history = []\n",
        "\n",
        "          final_history = history\n",
        "          for updated_history, status in chat_with_status(msg, history, doc_type_filter, auto_route, audit_num_chunks):\n",
        "              final_history = updated_history\n",
        "\n",
        "          return final_history\n",
        "\n",
        "\n",
        "      # --- EVENT WIRING ---\n",
        "\n",
        "      # üîó 1. LLM Selector\n",
        "      # Connect the selector to your handle_model_Transition (UI Manager) function\n",
        "      llm_selector.change(\n",
        "          fn=handle_model_transition,\n",
        "          inputs=[llm_selector, chatbot, clear_on_switch_checkbox],\n",
        "          outputs=[chatbot, engine_status]\n",
        "      )\n",
        "\n",
        "\n",
        "      # üîó 2. Processing Events\n",
        "\n",
        "      # A. File Upload. Ensures the loading state for uploading a file is properly cleared\n",
        "      #    File Preview (2 Outputs)\n",
        "      # When files are picked, show the first one in the viewer and names in the debug box\n",
        "      file_upload.change(\n",
        "            fn=lambda x: (x[0].name if x else None, f\"üìë {len(x)} files selected\" if x else \"No files\"),\n",
        "            inputs=[file_upload],\n",
        "            outputs=[doc_viewer, filename_debug_output]\n",
        "        )\n",
        "\n",
        "      # B. \"View File\" dropdown to switch which PDF is showing in the doc_viewer\n",
        "      view_selector.change(\n",
        "         fn=load_pdf_into_viewer,\n",
        "        inputs=[view_selector],\n",
        "        outputs=[doc_viewer, viewer_state, page_indicator]\n",
        "      )\n",
        "\n",
        "\n",
        "      # C. Document Processing (Backend Ingestion)\n",
        "      ingest_btn.click(\n",
        "            fn=process_pdf_handler,\n",
        "            inputs=[file_upload], # Pull from the actual uploaded file\n",
        "            outputs=[\n",
        "                status_output,              # Receives status_msg\n",
        "                structure_output_code,      # Receives structure_json_string\n",
        "                structure_output_textbox,   # Receives structure_display (Bulleted String)\n",
        "                doc_type_filter,               # Receives filter update - doc_type_filter (Dropdown - Filter Document Type)\n",
        "                view_selector,\n",
        "                op_status_bar]              # Receives status bar update - status_bar_text (String - Global Status Indicator)\n",
        "        )\n",
        "\n",
        "\n",
        "      # D. Document Viewer Navigation: Previous Button\n",
        "      prev_btn.click(\n",
        "          fn=flip_page,\n",
        "          inputs=[gr.State(\"prev\"), viewer_state],\n",
        "          outputs=[doc_viewer, viewer_state, page_indicator]\n",
        "      )\n",
        "\n",
        "      # E. Document Viewer Navigation: Next Button\n",
        "      next_btn.click(\n",
        "          fn=flip_page,\n",
        "          inputs=[gr.State(\"next\"), viewer_state],\n",
        "           outputs=[doc_viewer, viewer_state, page_indicator]\n",
        "      )\n",
        "\n",
        "\n",
        "      # üîó 3. Chat Texbox (Message) Input & Send\n",
        "\n",
        "      # A. Chat Functionality (The \"Conversation\" bridge)\n",
        "      # Use .then() to clear the input box after sending\n",
        "      msg_input.submit(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "            outputs=[chatbot, op_status_bar]\n",
        "      ).then(lambda: \"\", None, [msg_input]) # Only clear the input\n",
        "\n",
        "\n",
        "      # B. Chat Functionality (The \"Conversation\" bridge)\n",
        "      # Use .then() to clear the input box after sending\n",
        "      send_btn.click(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "            outputs=[chatbot, op_status_bar]\n",
        "      ).then(lambda: \"\", None, [msg_input])\n",
        "\n",
        "\n",
        "\n",
        "      # üîó 4. Download Chat History & Performance Audit Events\n",
        "\n",
        "      # A. When the button is clicked:\n",
        "      # -----Take 'chatbot' as input, run 'export_chat_history_to_pdf',\n",
        "      # -----and send the result to 'chat_download_file'\n",
        "      chat_download_btn.click(\n",
        "          fn=export_chat_history_to_pdf,\n",
        "          inputs=[chatbot],\n",
        "          outputs=[chat_download_btn]\n",
        "      )\n",
        "\n",
        "      # B. Run Performance Audit\n",
        "      run_audit_btn.click(\n",
        "          fn=run_performance_audit,\n",
        "          inputs=[sector_dropdown, audit_num_chunks],\n",
        "          outputs=[latency_stat, audit_accuracy_gauge, accuracy_gauge, bottleneck_plot, audit_table]\n",
        "      )\n",
        "\n",
        "\n",
        "      # B. Export & Download Performance Audit\n",
        "      audit_download_btn.click(\n",
        "          fn=handle_audit_export_ui,\n",
        "          inputs=[audit_table],\n",
        "          outputs=[\n",
        "              audit_download_btn,  # Receives the file update\n",
        "              op_status_bar         # Receives the status message (the \"‚úÖ Audit report...\" text)\n",
        "          ]\n",
        "      )\n",
        "\n",
        "\n",
        "      # üîó 5. UI Utility Events\n",
        "\n",
        "      # A. Utility/Reset Buttons: Clear ALL (Start new)\n",
        "      # Clear the entire platform\n",
        "      clear_all_btn.click(\n",
        "            fn=clear_all,\n",
        "            inputs=[],\n",
        "            outputs=[\n",
        "                chatbot,\n",
        "                file_upload,\n",
        "                status_output,\n",
        "                doc_viewer,\n",
        "                filename_debug_output,\n",
        "                structure_output_code,\n",
        "                structure_output_textbox,\n",
        "                doc_type_filter,  # Ensure this matches the name in your UI layout\n",
        "                view_selector,\n",
        "                audit_table,\n",
        "                audit_download_file,\n",
        "                op_status_bar\n",
        "           ]\n",
        "     )\n",
        "\n",
        "      # B. Utility/Reset Buttons: Clear Chat History\n",
        "      clear_chat_btn.click(\n",
        "          fn=lambda: (\n",
        "              [{\"role\": \"assistant\", \"content\": \"**ü§ñ AI Document Assistant:** üëã Chat cleared. How can I help you with your documents? üöÄ\"}],\n",
        "              gr.update(visible=False)\n",
        "          ),\n",
        "          inputs=None,\n",
        "          outputs=[chatbot, chat_download_file]\n",
        "      )\n",
        "\n",
        "      # C. Summary Button Wiring\n",
        "      example_btn1.click(\n",
        "          fn=ask_summary,\n",
        "          inputs=[chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "          outputs=[chatbot]\n",
        "      )\n",
        "\n",
        "      # D. Find Amounts Button Wiring\n",
        "      example_btn2.click(\n",
        "          fn=ask_amounts,\n",
        "          inputs=[chatbot, doc_type_filter, auto_route, audit_num_chunks],\n",
        "          outputs=[chatbot]\n",
        "      )\n",
        "\n",
        "      return demo\n",
        "\n",
        "      # üîó 6. ADDED - Initialize JavaScript for Auto-Scroll\n",
        "      demo.load(js=scroll_script)\n",
        "\n",
        "print(\"‚úÖ SSECTION 12. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC Complete.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ryrTe7SQOG0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f6eafd-0ff8-4f46-826a-3a68f46c8fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SSECTION 12. GRADIO INTERFACE, CHAT HANDLERS, & WIRING LOGIC Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 13. APPLICATION LAUNCHER**\n",
        "\n",
        "**Logic and Flow Analysis**\n",
        "\n",
        "Section 13 is the **Ignition System.** This is the final step that transitions the code from a collection of functions and classes into a live, interactive web service.\n",
        "\n",
        "<br>\n",
        "\n",
        "The launcher performs several critical operational tasks:\n",
        "\n",
        "1. **Port Management:** `gr.close_all()` ensures that any previous instances of the app running in the background are terminated, preventing \"Address already in use\" errors‚Äîa common headache in development environments like Google Colab or Jupyter.\n",
        "\n",
        "2. **Theme Injection:** It applies the `gr.themes.Soft()` base and overlays your `custom_css`. This creates the specific \"Dark/Obsidian\" professional aesthetic you designed in Section 12.\n",
        "\n",
        "3. **Tunneling:** By setting `share=True`, Gradio creates a secure public URL (e.g., `https://xyz123.gradio.live`). This allows you to test the mobile responsiveness of your platform or share the MVP with stakeholders without deploying to a cloud server.\n",
        "\n",
        "4. **Debugging:** `debug=True` is vital for the MVP stage. If the AI fails to process a specific PDF, the error logs will print directly in your notebook cell, allowing for immediate troubleshooting."
      ],
      "metadata": {
        "id": "GgDAO0pTcury"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- SECTION 13. APPLICATION LAUNCHER -------\n",
        "\n",
        "# 1. Cleanup: Close any existing Gradio servers to free up ports\n",
        "gr.close_all()\n",
        "\n",
        "print(\"üöÄ Initializing Platform Components...\")\n",
        "print(\"üìÇ Loading Vector Store...\")\n",
        "print(\"üß† Connecting LLM Engine...\")\n",
        "\n",
        "\n",
        "# 2. Build the Interface\n",
        "# Calls the create_interface() function defined in Section 12\n",
        "demo = create_interface()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ AI-Powered Document Intelligence Automation Platform Launching...\")\n",
        "\n",
        "    demo.launch(\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=custom_css,\n",
        "        debug=True,\n",
        "        share=True\n",
        "    )"
      ],
      "metadata": {
        "id": "M7AtFI83Ouno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "9ca4dfb8-38ec-47c3-db73-48d3ce8078e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing Platform Components...\n",
            "üìÇ Loading Vector Store...\n",
            "üß† Connecting LLM Engine...\n",
            "üöÄ AI-Powered Document Intelligence Automation Platform Launching...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b85cb655aef2dbb370.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b85cb655aef2dbb370.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Switching to Mistral 7B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (16384) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mistral 7B API Key Loaded & Configured.\n",
            "üìñ Starting PDF extraction and analysis for: {original_filename}\n"
          ]
        }
      ]
    }
  ]
}